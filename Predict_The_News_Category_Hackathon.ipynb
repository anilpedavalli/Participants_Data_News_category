{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Predict_The_News_Category_Hackathon.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmQv2vG3wo5c"
      },
      "source": [
        "#!wget 'https://storage.googleapis.com/kaggle-data-sets/8327/11650/compressed/glove.840B.300d.txt.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20201123%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20201123T154636Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=52c674af6f6f9f98ee0399a9f259ad2a67ee7b16d9f52e7178146ad9494c06de4a51bb75ab992f69a77fa26c0b8fd9cebe03c142e630ec135f83ae1eab5c886763ea0f26c2fb61dd8ac8111da3ec42dc5239a093f4fb4fd19c5e787418bf5e27e54bb43a9425495ee424f0c2d3335d5073790150251163c31e96e2252fe05cefc95e417cd91474fb2afa11bb25ed85f478a312a7ae965f36a8c2346881ab043694d6739f29db93c3f20b3926ad5a112beb7dc4f70bd1e2041ea325edbc8764adeeb8a227755d84a30046a1b2efa3e39605bc52fcba281dd629e150df120a40d527c42eeee5c461b7a35fd2c3979413224930f107b0d9d09f1f502001d63369d0'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvapTNKo6eHM",
        "outputId": "a9532676-e794-4486-b1e4-1459ad3328f4"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-26 06:19:36--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-11-26 06:19:36--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-11-26 06:19:37--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.17MB/s    in 6m 28s  \n",
            "\n",
            "2020-11-26 06:26:05 (2.12 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkczlSfuS-Ic"
      },
      "source": [
        "#1. Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMBxQHn3SVwT"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import collections\n",
        "\n",
        "import sys\n",
        "if not sys.warnoptions:\n",
        "    import warnings\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "\n",
        "import shutil\n",
        "shutil.unpack_archive(\"/content/glove.6B.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_SCbTIRTTpc"
      },
      "source": [
        "#2. Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GH-hdumQTS3y",
        "outputId": "860718dd-2a31-4272-9092-8bd8306021bd"
      },
      "source": [
        "train_df=pd.read_excel('/content/Data_Train.xlsx')\n",
        "test_df=pd.read_excel('/content/Data_Test.xlsx')\n",
        "\n",
        "print(train_df.shape)\n",
        "print(test_df.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7628, 2)\n",
            "(2748, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1st587njDKi"
      },
      "source": [
        "#3. Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3-VcNB8jVhR",
        "outputId": "1238557d-549f-48c0-8482-5e77dc6c6707"
      },
      "source": [
        "#Dropping Duplicates\n",
        "train_df=train_df.drop_duplicates()\n",
        "\n",
        "#Combining Test and Train DataFrames\n",
        "combined_df=pd.concat([train_df,test_df],axis=0)\n",
        "print(combined_df.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10299, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeHFI7e1TnMS",
        "outputId": "b8b3f53a-50dc-4a40-8a65-04067c6d2a0c"
      },
      "source": [
        "print(combined_df.columns)\n",
        "combined_df['SECTION'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['STORY', 'SECTION'], dtype='object')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0    2731\n",
              "2.0    1914\n",
              "0.0    1673\n",
              "3.0    1233\n",
              "Name: SECTION, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jWNZphGZG_z",
        "outputId": "8f05d168-895e-4b1b-f829-e14567c2526d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.stem import WordNetLemmatizer,SnowballStemmer\n",
        "stop_words = stopwords.words('english')\n",
        "porter = PorterStemmer()\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "combined_df=pd.concat([train_df,test_df],axis=0)\n",
        "Story_prepocessed_lst=[]\n",
        "corpus_lst=[]\n",
        "#stop_words=[]\n",
        "\n",
        "for sent in combined_df['STORY'].values:\n",
        "  str_temp=\"\"\n",
        "  for word in sent.split():\n",
        "    word=word.lower()\n",
        "    word=re.sub('[^A-Za-z0-9]+', '', word)\n",
        "    #word=porter.stem(word)\n",
        "    #word=wordnet_lemmatizer.lemmatize(word,pos = 'v')\n",
        "    #word=porter.stem(wordnet_lemmatizer.lemmatize(word, pos='v'))\n",
        "    if word not in stop_words and len(word)>2 and len(word)<16:\n",
        "      #if word!='said' and word!='also':\n",
        "      str_temp=str_temp+word+\" \"\n",
        "      corpus_lst.append(word.strip())\n",
        "  str_temp=str_temp.strip()\n",
        "  Story_prepocessed_lst.append(str_temp)\n",
        "combined_df['STORY']=Story_prepocessed_lst"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ChOFUV_OifOJ",
        "outputId": "65156910-b85a-4db1-d92e-ea0cf46d116d"
      },
      "source": [
        "wordnet_lemmatizer.lemmatize('also',pos = 'v')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'also'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "E3SGco9ghTx1",
        "outputId": "6f438a4a-b36c-462a-ea6d-4b2203083e86"
      },
      "source": [
        "#Word Frequency\n",
        "keys=list(dict(collections.Counter(corpus_lst).most_common()).keys())\n",
        "values=list(dict(collections.Counter(corpus_lst).most_common()).values())\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.bar(keys[:10],values[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 10 artists>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "R0nwN3WJU6Ci",
        "outputId": "0d17ea41-04ec-4d82-8f6c-5cbaf0a1e319"
      },
      "source": [
        "corpus_lst=list(np.unique(corpus_lst))\n",
        "tokens=[i for i in range(len(corpus_lst))]\n",
        "tokens_dt=dict(zip(corpus_lst,tokens))\n",
        "\n",
        "combined_df['sent_len']=combined_df['STORY'].apply(lambda x:len(x))\n",
        "\n",
        "combined_df['sent_split']=combined_df['STORY'].apply(lambda x:x.split())\n",
        "\n",
        "combined_df['num_unique_words']=combined_df['sent_split'].apply(lambda x:len(list(dict(collections.Counter(x).most_common()).keys())))\n",
        "\n",
        "\n",
        "combined_df['frequent_word_1']=combined_df['sent_split'].apply(lambda x:list(dict(collections.Counter(x).most_common()).keys())[0])\n",
        "combined_df['frequent_word_2']=combined_df['sent_split'].apply(lambda x:list(dict(collections.Counter(x).most_common()).keys())[1])\n",
        "combined_df['frequent_word_3']=combined_df['sent_split'].apply(lambda x:list(dict(collections.Counter(x).most_common()).keys())[2])\n",
        "\n",
        "combined_df['frequency_1']=combined_df['sent_split'].apply(lambda x:list(dict(collections.Counter(x).most_common()).values())[0])\n",
        "combined_df['frequency_2']=combined_df['sent_split'].apply(lambda x:list(dict(collections.Counter(x).most_common()).values())[1])\n",
        "combined_df['frequency_3']=combined_df['sent_split'].apply(lambda x:list(dict(collections.Counter(x).most_common()).values())[2])\n",
        "\n",
        "combined_df['frequent_word_1_ratio']=combined_df['frequency_1']/combined_df['num_unique_words']\n",
        "combined_df['frequent_word_2_ratio']=combined_df['frequency_2']/combined_df['num_unique_words']\n",
        "combined_df['frequent_word_3_ratio']=combined_df['frequency_3']/combined_df['num_unique_words']\n",
        "\n",
        "combined_df['frequent_word_1_token']=combined_df['frequent_word_1'].apply(lambda x:tokens_dt[x])\n",
        "combined_df['frequent_word_2_token']=combined_df['frequent_word_2'].apply(lambda x:tokens_dt[x])\n",
        "combined_df['frequent_word_3_token']=combined_df['frequent_word_3'].apply(lambda x:tokens_dt[x])\n",
        "\n",
        "combined_df=combined_df.drop('sent_split',axis=1)\n",
        "combined_df.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>STORY</th>\n",
              "      <th>SECTION</th>\n",
              "      <th>sent_len</th>\n",
              "      <th>num_unique_words</th>\n",
              "      <th>frequent_word_1</th>\n",
              "      <th>frequent_word_2</th>\n",
              "      <th>frequent_word_3</th>\n",
              "      <th>frequency_1</th>\n",
              "      <th>frequency_2</th>\n",
              "      <th>frequency_3</th>\n",
              "      <th>frequent_word_1_ratio</th>\n",
              "      <th>frequent_word_2_ratio</th>\n",
              "      <th>frequent_word_3_ratio</th>\n",
              "      <th>frequent_word_1_token</th>\n",
              "      <th>frequent_word_2_token</th>\n",
              "      <th>frequent_word_3_token</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>painful huge reversal fee income unheard among...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>574</td>\n",
              "      <td>73</td>\n",
              "      <td>fee</td>\n",
              "      <td>income</td>\n",
              "      <td>means</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.027397</td>\n",
              "      <td>0.027397</td>\n",
              "      <td>0.027397</td>\n",
              "      <td>16860</td>\n",
              "      <td>21175</td>\n",
              "      <td>26084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>formidable opposition alliance among congress ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>108</td>\n",
              "      <td>11</td>\n",
              "      <td>jharkhand</td>\n",
              "      <td>morcha</td>\n",
              "      <td>formidable</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>22595</td>\n",
              "      <td>27146</td>\n",
              "      <td>17613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>asian currencies trading lower today south kor...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>270</td>\n",
              "      <td>35</td>\n",
              "      <td>currencies</td>\n",
              "      <td>trading</td>\n",
              "      <td>china</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.057143</td>\n",
              "      <td>0.057143</td>\n",
              "      <td>0.057143</td>\n",
              "      <td>12489</td>\n",
              "      <td>40978</td>\n",
              "      <td>10360</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               STORY  ...  frequent_word_3_token\n",
              "0  painful huge reversal fee income unheard among...  ...                  26084\n",
              "1  formidable opposition alliance among congress ...  ...                  17613\n",
              "2  asian currencies trading lower today south kor...  ...                  10360\n",
              "\n",
              "[3 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZNSuTAkoip_"
      },
      "source": [
        "### LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcKspK3ZN3MF"
      },
      "source": [
        "#from sklearn.preprocessing import \n",
        "import gensim\n",
        "dictionary = gensim.corpora.Dictionary(combined_df['STORY'].apply(lambda x:x.split()))\n",
        "\n",
        "bow_corpus = [dictionary.doc2bow(doc) for doc in combined_df['STORY'].apply(lambda x:x.split())]\n",
        "\n",
        "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
        "                                   num_topics = 4, \n",
        "                                   id2word = dictionary,                                    \n",
        "                                   passes = 10,\n",
        "                                   workers = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6NhRI5bQpKD",
        "outputId": "dd344fc4-4e03-4910-a348-d1a729fe7d29"
      },
      "source": [
        "#Topics=['Politics','Business','Entertainment','Technology']\n",
        "Topics_key_words=[]\n",
        "\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
        "    Topics_key_words.append(re.sub('[^A-Za-z]+', ' ', topic).strip())\n",
        "    print(\"\\n\")\n",
        "\n",
        "Politicts_lst=Topics_key_words[0].split()\n",
        "Business_lst=Topics_key_words[1].split()\n",
        "Entertainment_lst=Topics_key_words[2].split()\n",
        "Technology_lst=Topics_key_words[3].split()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic: 0 \n",
            "Words: 0.015*\"film\" + 0.005*\"make\" + 0.005*\"actor\" + 0.004*\"like\" + 0.004*\"movie\" + 0.004*\"work\" + 0.004*\"take\" + 0.003*\"show\" + 0.003*\"character\" + 0.003*\"star\"\n",
            "\n",
            "\n",
            "Topic: 1 \n",
            "Words: 0.006*\"game\" + 0.006*\"like\" + 0.006*\"new\" + 0.005*\"one\" + 0.005*\"make\" + 0.005*\"use\" + 0.004*\"screen\" + 0.004*\"phone\" + 0.004*\"display\" + 0.004*\"come\"\n",
            "\n",
            "\n",
            "Topic: 2 \n",
            "Words: 0.007*\"company\" + 0.006*\"india\" + 0.006*\"market\" + 0.006*\"price\" + 0.005*\"new\" + 0.005*\"data\" + 0.005*\"year\" + 0.004*\"smartphone\" + 0.004*\"use\" + 0.004*\"apple\"\n",
            "\n",
            "\n",
            "Topic: 3 \n",
            "Words: 0.015*\"party\" + 0.012*\"congress\" + 0.011*\"bjp\" + 0.009*\"state\" + 0.008*\"seat\" + 0.007*\"elections\" + 0.007*\"minister\" + 0.007*\"modi\" + 0.006*\"poll\" + 0.006*\"sabha\"\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJo2M7rV-s6N",
        "outputId": "f389ee63-7632-438f-aba1-2b04c70d6cab"
      },
      "source": [
        "Topics_key_words_vocab=[]\n",
        "for i in Topics_key_words:\n",
        "  Topics_key_words_vocab.extend(i.split())\n",
        "Topics_key_words_vocab=list(set(Topics_key_words_vocab))\n",
        "print(Topics_key_words_vocab)\n",
        "print(len(Topics_key_words_vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['party', 'data', 'star', 'elections', 'apple', 'minister', 'phone', 'screen', 'work', 'use', 'movie', 'smartphone', 'like', 'year', 'take', 'one', 'congress', 'come', 'company', 'make', 'new', 'film', 'bjp', 'modi', 'india', 'market', 'actor', 'price', 'poll', 'character', 'game', 'state', 'sabha', 'display', 'seat', 'show']\n",
            "36\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxD7mUMcGSoy"
      },
      "source": [
        "Topics_key_words_vocab=['india', 'bjp', 'people', 'data', 'share', 'company', 'bank', 'investor', 'samsung', 'crore', 'lok', 'state', 'display', 'new', 'party', 'minister', 'smartphone', 'user', 'growth', 'technology', 'device', 'price', 'modi', 'camera', 'feature', 'phone', 'sabha', 'film', 'galaxy', 'like', 'election', 'seat', 'congress', 'market', 'pro', 'one', 'year']\n",
        "val_key_words_vocab=['india', 'billion', 'facebook', 'world', 'google', 'yet', 'feed', 'bajaj', 'power', 'bengaluru', 'sebi', 'agency', 'order', 'political', 'helicopter', 'user', 'medium', 'firm', 'nse', 'demand', 'story', 'deol', 'without', 'inc', 'modi', 'million', 'dorsey', 'sale', 'brand', 'war', 'headline', 'segment', 'election', 'market', 'report', 'may', 'one', 'year']\n",
        "\n",
        "Topics_key_words_vocab=list(set(Topics_key_words_vocab).union(val_key_words_vocab))\n",
        "\n",
        "'''\n",
        "Topics=['Politicts','Technology','Entertainment','Business']\n",
        "target_pred_lst=[]\n",
        "target_word_match_count=[]\n",
        "for i in combined_df['STORY'].apply(lambda x:x.split()):\n",
        "\n",
        "  a=(len(set(i).intersection(Politicts_lst)))\n",
        "  b=(len(set(i).intersection(Technology_lst)))\n",
        "  c=(len(set(i).intersection(Entertainment_lst)))\n",
        "  d=(len(set(i).intersection(Business_lst)))\n",
        "  if a==b==c==d==0:\n",
        "    target_pred_lst.append('unknown')\n",
        "    target_word_match_count.append(0)\n",
        "  else:\n",
        "    target_pred_lst.append(Topics[np.argmax([a,b,c,d])])\n",
        "    target_word_match_count.append(np.max([a,b,c,d]))\n",
        "\n",
        "combined_df['target_pred_lda']=target_pred_lst\n",
        "combined_df['target_word_match_count']=target_word_match_count\n",
        "\n",
        "pd.get_dummies(combined_df['target_pred_lda'])\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "fjoXLoW15NLT",
        "outputId": "e1f020d7-16e1-4aa8-ab41-4ff998f1c7c5"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf=TfidfVectorizer(vocabulary=Topics_key_words_vocab,ngram_range=(1,2))\n",
        "tfidf_df=pd.DataFrame(tfidf.fit_transform(combined_df['STORY'].values).todense(),columns=Topics_key_words_vocab)\n",
        "print(tfidf_df.shape)\n",
        "tfidf_df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10299, 69)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>party</th>\n",
              "      <th>data</th>\n",
              "      <th>star</th>\n",
              "      <th>elections</th>\n",
              "      <th>user</th>\n",
              "      <th>firm</th>\n",
              "      <th>nse</th>\n",
              "      <th>apple</th>\n",
              "      <th>minister</th>\n",
              "      <th>segment</th>\n",
              "      <th>phone</th>\n",
              "      <th>screen</th>\n",
              "      <th>headline</th>\n",
              "      <th>without</th>\n",
              "      <th>work</th>\n",
              "      <th>medium</th>\n",
              "      <th>use</th>\n",
              "      <th>movie</th>\n",
              "      <th>agency</th>\n",
              "      <th>facebook</th>\n",
              "      <th>report</th>\n",
              "      <th>story</th>\n",
              "      <th>political</th>\n",
              "      <th>deol</th>\n",
              "      <th>demand</th>\n",
              "      <th>sebi</th>\n",
              "      <th>smartphone</th>\n",
              "      <th>dorsey</th>\n",
              "      <th>sale</th>\n",
              "      <th>brand</th>\n",
              "      <th>war</th>\n",
              "      <th>election</th>\n",
              "      <th>billion</th>\n",
              "      <th>like</th>\n",
              "      <th>year</th>\n",
              "      <th>order</th>\n",
              "      <th>google</th>\n",
              "      <th>take</th>\n",
              "      <th>one</th>\n",
              "      <th>congress</th>\n",
              "      <th>come</th>\n",
              "      <th>company</th>\n",
              "      <th>inc</th>\n",
              "      <th>feed</th>\n",
              "      <th>make</th>\n",
              "      <th>world</th>\n",
              "      <th>million</th>\n",
              "      <th>new</th>\n",
              "      <th>film</th>\n",
              "      <th>bjp</th>\n",
              "      <th>bengaluru</th>\n",
              "      <th>bajaj</th>\n",
              "      <th>helicopter</th>\n",
              "      <th>modi</th>\n",
              "      <th>power</th>\n",
              "      <th>india</th>\n",
              "      <th>market</th>\n",
              "      <th>actor</th>\n",
              "      <th>price</th>\n",
              "      <th>poll</th>\n",
              "      <th>yet</th>\n",
              "      <th>may</th>\n",
              "      <th>character</th>\n",
              "      <th>game</th>\n",
              "      <th>state</th>\n",
              "      <th>sabha</th>\n",
              "      <th>display</th>\n",
              "      <th>seat</th>\n",
              "      <th>show</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.624048</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.473794</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.454623</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.423559</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.338549</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.889173</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.307824</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   party      data  star  elections  user  ...  state  sabha  display  seat  show\n",
              "0    0.0  0.000000   0.0        0.0   0.0  ...    0.0    0.0      0.0   0.0   0.0\n",
              "1    0.0  0.000000   0.0        0.0   0.0  ...    0.0    0.0      0.0   0.0   0.0\n",
              "2    0.0  0.000000   0.0        0.0   0.0  ...    0.0    0.0      0.0   0.0   0.0\n",
              "3    0.0  0.000000   0.0        0.0   0.0  ...    0.0    0.0      0.0   0.0   0.0\n",
              "4    0.0  0.338549   0.0        0.0   0.0  ...    0.0    0.0      0.0   0.0   0.0\n",
              "\n",
              "[5 rows x 69 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgbNpqgdNtPp"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tf_idf=TfidfVectorizer()\n",
        "tf_idf.fit(combined_df['STORY'].values)\n",
        "\n",
        "idf_dt=dict(zip(tf_idf.get_feature_names(),list(tf_idf.idf_)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNUOZO-u2KmN",
        "outputId": "f2cc18b8-d7b6-436d-c6b2-b77ab68d679e"
      },
      "source": [
        "print('Indexing word vectors.')\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open('/content/glove.6B.300d.txt', encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    #print(word)\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec(sentences=combined_df['STORY'].apply(lambda x:x.split()), window=5, min_count=1, size=300,workers=4)\n",
        "model.save(\"word2vec.model\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Indexing word vectors.\n",
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc9It5AoQtJG"
      },
      "source": [
        "#Word 2 Vec\n",
        "w2v_story_lst=[]\n",
        "for sent in combined_df['STORY'].values:\n",
        "  count=0\n",
        "  vec=np.zeros(300)\n",
        "  for word in sent.split():\n",
        "    try:\n",
        "      embeddings_index[word]\n",
        "      vec+=embeddings_index[word]\n",
        "      count+=1\n",
        "    except:\n",
        "      pass\n",
        "  if count!=0:\n",
        "    w2v_story_lst.append(vec/count)\n",
        "  else:\n",
        "    w2v_story_lst.append(vec)\n",
        "\n",
        "w2v_story_df1=pd.DataFrame(w2v_story_lst,columns=[i for i in range(1000,1300)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "CJlmIt7cn9F1",
        "outputId": "b904a895-ef9c-421e-ebbc-0ab33e5e028f"
      },
      "source": [
        "model = Word2Vec.load(\"word2vec.model\")\n",
        "\n",
        "w2v_story_lst=[]\n",
        "for sent in combined_df['STORY'].values:\n",
        "  count=0\n",
        "  vec=np.zeros(300)\n",
        "  for word in sent.split():\n",
        "    try:\n",
        "      model.wv[word]\n",
        "      vec+=model.wv[word]\n",
        "      count+=1\n",
        "    except:\n",
        "      print('Not available')\n",
        "      pass\n",
        "  if count!=0:\n",
        "    w2v_story_lst.append(vec/count)\n",
        "  else:\n",
        "    w2v_story_lst.append(vec)\n",
        "\n",
        "w2v_story_df2=pd.DataFrame(w2v_story_lst,columns=[i for i in range(2000,2300)])\n",
        "w2v_story_df2.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>2000</th>\n",
              "      <th>2001</th>\n",
              "      <th>2002</th>\n",
              "      <th>2003</th>\n",
              "      <th>2004</th>\n",
              "      <th>2005</th>\n",
              "      <th>2006</th>\n",
              "      <th>2007</th>\n",
              "      <th>2008</th>\n",
              "      <th>2009</th>\n",
              "      <th>2010</th>\n",
              "      <th>2011</th>\n",
              "      <th>2012</th>\n",
              "      <th>2013</th>\n",
              "      <th>2014</th>\n",
              "      <th>2015</th>\n",
              "      <th>2016</th>\n",
              "      <th>2017</th>\n",
              "      <th>2018</th>\n",
              "      <th>2019</th>\n",
              "      <th>2020</th>\n",
              "      <th>2021</th>\n",
              "      <th>2022</th>\n",
              "      <th>2023</th>\n",
              "      <th>2024</th>\n",
              "      <th>2025</th>\n",
              "      <th>2026</th>\n",
              "      <th>2027</th>\n",
              "      <th>2028</th>\n",
              "      <th>2029</th>\n",
              "      <th>2030</th>\n",
              "      <th>2031</th>\n",
              "      <th>2032</th>\n",
              "      <th>2033</th>\n",
              "      <th>2034</th>\n",
              "      <th>2035</th>\n",
              "      <th>2036</th>\n",
              "      <th>2037</th>\n",
              "      <th>2038</th>\n",
              "      <th>2039</th>\n",
              "      <th>...</th>\n",
              "      <th>2260</th>\n",
              "      <th>2261</th>\n",
              "      <th>2262</th>\n",
              "      <th>2263</th>\n",
              "      <th>2264</th>\n",
              "      <th>2265</th>\n",
              "      <th>2266</th>\n",
              "      <th>2267</th>\n",
              "      <th>2268</th>\n",
              "      <th>2269</th>\n",
              "      <th>2270</th>\n",
              "      <th>2271</th>\n",
              "      <th>2272</th>\n",
              "      <th>2273</th>\n",
              "      <th>2274</th>\n",
              "      <th>2275</th>\n",
              "      <th>2276</th>\n",
              "      <th>2277</th>\n",
              "      <th>2278</th>\n",
              "      <th>2279</th>\n",
              "      <th>2280</th>\n",
              "      <th>2281</th>\n",
              "      <th>2282</th>\n",
              "      <th>2283</th>\n",
              "      <th>2284</th>\n",
              "      <th>2285</th>\n",
              "      <th>2286</th>\n",
              "      <th>2287</th>\n",
              "      <th>2288</th>\n",
              "      <th>2289</th>\n",
              "      <th>2290</th>\n",
              "      <th>2291</th>\n",
              "      <th>2292</th>\n",
              "      <th>2293</th>\n",
              "      <th>2294</th>\n",
              "      <th>2295</th>\n",
              "      <th>2296</th>\n",
              "      <th>2297</th>\n",
              "      <th>2298</th>\n",
              "      <th>2299</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.111041</td>\n",
              "      <td>-0.092938</td>\n",
              "      <td>0.196163</td>\n",
              "      <td>-0.078103</td>\n",
              "      <td>-0.234818</td>\n",
              "      <td>0.151522</td>\n",
              "      <td>-0.124912</td>\n",
              "      <td>-0.215851</td>\n",
              "      <td>-0.014673</td>\n",
              "      <td>0.167840</td>\n",
              "      <td>-0.104619</td>\n",
              "      <td>0.237129</td>\n",
              "      <td>0.069990</td>\n",
              "      <td>0.062728</td>\n",
              "      <td>-0.050237</td>\n",
              "      <td>-0.249474</td>\n",
              "      <td>0.053331</td>\n",
              "      <td>0.014123</td>\n",
              "      <td>-0.481654</td>\n",
              "      <td>0.245133</td>\n",
              "      <td>0.016761</td>\n",
              "      <td>0.172124</td>\n",
              "      <td>-0.194576</td>\n",
              "      <td>-0.164195</td>\n",
              "      <td>0.294045</td>\n",
              "      <td>0.009317</td>\n",
              "      <td>0.012111</td>\n",
              "      <td>-0.068949</td>\n",
              "      <td>0.083682</td>\n",
              "      <td>-0.085566</td>\n",
              "      <td>-0.128004</td>\n",
              "      <td>0.071947</td>\n",
              "      <td>-0.293366</td>\n",
              "      <td>-0.010949</td>\n",
              "      <td>-0.114541</td>\n",
              "      <td>0.000057</td>\n",
              "      <td>-0.107320</td>\n",
              "      <td>-0.352366</td>\n",
              "      <td>-0.036324</td>\n",
              "      <td>0.094209</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.077747</td>\n",
              "      <td>0.081796</td>\n",
              "      <td>-0.038419</td>\n",
              "      <td>-0.213019</td>\n",
              "      <td>-0.131413</td>\n",
              "      <td>0.216334</td>\n",
              "      <td>-0.205609</td>\n",
              "      <td>-0.317094</td>\n",
              "      <td>-0.058197</td>\n",
              "      <td>0.099581</td>\n",
              "      <td>0.099385</td>\n",
              "      <td>0.327730</td>\n",
              "      <td>-0.251476</td>\n",
              "      <td>0.144655</td>\n",
              "      <td>-0.121377</td>\n",
              "      <td>0.149038</td>\n",
              "      <td>0.208428</td>\n",
              "      <td>-0.073347</td>\n",
              "      <td>-0.252160</td>\n",
              "      <td>-0.059846</td>\n",
              "      <td>-0.027321</td>\n",
              "      <td>0.131576</td>\n",
              "      <td>-0.204286</td>\n",
              "      <td>-0.067578</td>\n",
              "      <td>-0.038193</td>\n",
              "      <td>0.322343</td>\n",
              "      <td>0.056529</td>\n",
              "      <td>-0.030250</td>\n",
              "      <td>0.303522</td>\n",
              "      <td>0.086491</td>\n",
              "      <td>-0.327234</td>\n",
              "      <td>-0.062483</td>\n",
              "      <td>0.048417</td>\n",
              "      <td>0.080335</td>\n",
              "      <td>-0.160686</td>\n",
              "      <td>-0.042371</td>\n",
              "      <td>0.007143</td>\n",
              "      <td>0.248255</td>\n",
              "      <td>-0.011970</td>\n",
              "      <td>0.204146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.272954</td>\n",
              "      <td>-0.201667</td>\n",
              "      <td>-0.360242</td>\n",
              "      <td>-0.175129</td>\n",
              "      <td>-0.386348</td>\n",
              "      <td>0.319007</td>\n",
              "      <td>-0.184870</td>\n",
              "      <td>-0.085722</td>\n",
              "      <td>-0.153296</td>\n",
              "      <td>0.368699</td>\n",
              "      <td>-0.007380</td>\n",
              "      <td>-0.066392</td>\n",
              "      <td>0.022783</td>\n",
              "      <td>0.208588</td>\n",
              "      <td>-0.016057</td>\n",
              "      <td>-0.278040</td>\n",
              "      <td>0.145190</td>\n",
              "      <td>-0.008671</td>\n",
              "      <td>-0.421584</td>\n",
              "      <td>0.308780</td>\n",
              "      <td>-0.050956</td>\n",
              "      <td>0.345344</td>\n",
              "      <td>0.019962</td>\n",
              "      <td>-0.293385</td>\n",
              "      <td>0.512941</td>\n",
              "      <td>-0.031579</td>\n",
              "      <td>-0.099362</td>\n",
              "      <td>0.070059</td>\n",
              "      <td>0.144572</td>\n",
              "      <td>-0.094945</td>\n",
              "      <td>-0.174929</td>\n",
              "      <td>-0.145735</td>\n",
              "      <td>-0.122960</td>\n",
              "      <td>-0.122661</td>\n",
              "      <td>-0.179362</td>\n",
              "      <td>-0.045956</td>\n",
              "      <td>-0.094746</td>\n",
              "      <td>-0.553752</td>\n",
              "      <td>-0.113812</td>\n",
              "      <td>0.121621</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.218486</td>\n",
              "      <td>0.172774</td>\n",
              "      <td>-0.130566</td>\n",
              "      <td>-0.168753</td>\n",
              "      <td>-0.017251</td>\n",
              "      <td>0.033321</td>\n",
              "      <td>-0.125119</td>\n",
              "      <td>-0.286074</td>\n",
              "      <td>0.061477</td>\n",
              "      <td>0.026181</td>\n",
              "      <td>0.262180</td>\n",
              "      <td>0.557498</td>\n",
              "      <td>-0.337324</td>\n",
              "      <td>0.273360</td>\n",
              "      <td>-0.091401</td>\n",
              "      <td>0.047667</td>\n",
              "      <td>0.151924</td>\n",
              "      <td>-0.002025</td>\n",
              "      <td>-0.061558</td>\n",
              "      <td>-0.184751</td>\n",
              "      <td>-0.184860</td>\n",
              "      <td>0.130805</td>\n",
              "      <td>-0.440188</td>\n",
              "      <td>-0.116222</td>\n",
              "      <td>0.161297</td>\n",
              "      <td>0.341330</td>\n",
              "      <td>0.269048</td>\n",
              "      <td>-0.123524</td>\n",
              "      <td>0.299127</td>\n",
              "      <td>-0.024162</td>\n",
              "      <td>-0.361622</td>\n",
              "      <td>-0.203425</td>\n",
              "      <td>0.231710</td>\n",
              "      <td>0.282227</td>\n",
              "      <td>-0.394091</td>\n",
              "      <td>-0.195151</td>\n",
              "      <td>-0.119935</td>\n",
              "      <td>0.226228</td>\n",
              "      <td>0.146945</td>\n",
              "      <td>0.432084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.223640</td>\n",
              "      <td>-0.150633</td>\n",
              "      <td>0.518778</td>\n",
              "      <td>-0.080157</td>\n",
              "      <td>-0.208991</td>\n",
              "      <td>0.312548</td>\n",
              "      <td>-0.039489</td>\n",
              "      <td>-0.394111</td>\n",
              "      <td>0.004306</td>\n",
              "      <td>0.206438</td>\n",
              "      <td>-0.096237</td>\n",
              "      <td>0.218924</td>\n",
              "      <td>0.109384</td>\n",
              "      <td>-0.098078</td>\n",
              "      <td>0.153305</td>\n",
              "      <td>-0.193059</td>\n",
              "      <td>-0.036344</td>\n",
              "      <td>0.150124</td>\n",
              "      <td>-0.647782</td>\n",
              "      <td>0.270586</td>\n",
              "      <td>-0.087407</td>\n",
              "      <td>0.035965</td>\n",
              "      <td>-0.156716</td>\n",
              "      <td>-0.023990</td>\n",
              "      <td>0.526928</td>\n",
              "      <td>-0.037213</td>\n",
              "      <td>-0.012821</td>\n",
              "      <td>-0.134839</td>\n",
              "      <td>0.253816</td>\n",
              "      <td>0.067241</td>\n",
              "      <td>-0.076030</td>\n",
              "      <td>0.177143</td>\n",
              "      <td>-0.365413</td>\n",
              "      <td>0.100678</td>\n",
              "      <td>-0.127035</td>\n",
              "      <td>0.164995</td>\n",
              "      <td>-0.071859</td>\n",
              "      <td>-0.233332</td>\n",
              "      <td>-0.050874</td>\n",
              "      <td>0.229826</td>\n",
              "      <td>...</td>\n",
              "      <td>0.082003</td>\n",
              "      <td>0.059831</td>\n",
              "      <td>0.155523</td>\n",
              "      <td>-0.343769</td>\n",
              "      <td>-0.227450</td>\n",
              "      <td>0.373910</td>\n",
              "      <td>-0.292301</td>\n",
              "      <td>-0.134755</td>\n",
              "      <td>0.109503</td>\n",
              "      <td>0.221095</td>\n",
              "      <td>0.214254</td>\n",
              "      <td>0.248764</td>\n",
              "      <td>-0.263413</td>\n",
              "      <td>0.137787</td>\n",
              "      <td>-0.076417</td>\n",
              "      <td>0.208419</td>\n",
              "      <td>0.203834</td>\n",
              "      <td>-0.093079</td>\n",
              "      <td>-0.438095</td>\n",
              "      <td>-0.032429</td>\n",
              "      <td>-0.089821</td>\n",
              "      <td>0.224331</td>\n",
              "      <td>-0.218561</td>\n",
              "      <td>0.002149</td>\n",
              "      <td>-0.157527</td>\n",
              "      <td>0.437094</td>\n",
              "      <td>0.127063</td>\n",
              "      <td>-0.058205</td>\n",
              "      <td>0.237083</td>\n",
              "      <td>0.106797</td>\n",
              "      <td>-0.349849</td>\n",
              "      <td>-0.115194</td>\n",
              "      <td>-0.098547</td>\n",
              "      <td>-0.059304</td>\n",
              "      <td>0.026411</td>\n",
              "      <td>-0.085473</td>\n",
              "      <td>0.068728</td>\n",
              "      <td>0.325615</td>\n",
              "      <td>-0.119377</td>\n",
              "      <td>0.127518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.023332</td>\n",
              "      <td>-0.048139</td>\n",
              "      <td>0.106083</td>\n",
              "      <td>-0.079528</td>\n",
              "      <td>-0.266037</td>\n",
              "      <td>0.051164</td>\n",
              "      <td>-0.207306</td>\n",
              "      <td>-0.147486</td>\n",
              "      <td>-0.009532</td>\n",
              "      <td>0.139556</td>\n",
              "      <td>-0.149295</td>\n",
              "      <td>0.375843</td>\n",
              "      <td>0.066295</td>\n",
              "      <td>0.168315</td>\n",
              "      <td>-0.187311</td>\n",
              "      <td>-0.296486</td>\n",
              "      <td>0.114823</td>\n",
              "      <td>-0.099435</td>\n",
              "      <td>-0.474694</td>\n",
              "      <td>0.254349</td>\n",
              "      <td>0.083296</td>\n",
              "      <td>0.274881</td>\n",
              "      <td>-0.298549</td>\n",
              "      <td>-0.256384</td>\n",
              "      <td>0.167389</td>\n",
              "      <td>-0.007572</td>\n",
              "      <td>0.068645</td>\n",
              "      <td>-0.022015</td>\n",
              "      <td>-0.006448</td>\n",
              "      <td>-0.198280</td>\n",
              "      <td>-0.156642</td>\n",
              "      <td>0.054977</td>\n",
              "      <td>-0.303462</td>\n",
              "      <td>-0.088950</td>\n",
              "      <td>-0.111284</td>\n",
              "      <td>-0.081405</td>\n",
              "      <td>-0.198479</td>\n",
              "      <td>-0.470170</td>\n",
              "      <td>-0.029478</td>\n",
              "      <td>-0.009881</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.188950</td>\n",
              "      <td>0.114087</td>\n",
              "      <td>-0.125919</td>\n",
              "      <td>-0.185751</td>\n",
              "      <td>-0.146965</td>\n",
              "      <td>0.189249</td>\n",
              "      <td>-0.198804</td>\n",
              "      <td>-0.508204</td>\n",
              "      <td>-0.227426</td>\n",
              "      <td>0.059248</td>\n",
              "      <td>0.003102</td>\n",
              "      <td>0.437001</td>\n",
              "      <td>-0.293134</td>\n",
              "      <td>0.139659</td>\n",
              "      <td>-0.196030</td>\n",
              "      <td>0.105296</td>\n",
              "      <td>0.258034</td>\n",
              "      <td>-0.071194</td>\n",
              "      <td>-0.223179</td>\n",
              "      <td>-0.083911</td>\n",
              "      <td>-0.015361</td>\n",
              "      <td>0.097344</td>\n",
              "      <td>-0.180964</td>\n",
              "      <td>-0.155578</td>\n",
              "      <td>-0.003132</td>\n",
              "      <td>0.328851</td>\n",
              "      <td>-0.030897</td>\n",
              "      <td>-0.009634</td>\n",
              "      <td>0.439013</td>\n",
              "      <td>0.108931</td>\n",
              "      <td>-0.409520</td>\n",
              "      <td>-0.037141</td>\n",
              "      <td>0.157529</td>\n",
              "      <td>0.180415</td>\n",
              "      <td>-0.274828</td>\n",
              "      <td>0.027683</td>\n",
              "      <td>-0.014159</td>\n",
              "      <td>0.251763</td>\n",
              "      <td>0.096795</td>\n",
              "      <td>0.276718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.163353</td>\n",
              "      <td>-0.148985</td>\n",
              "      <td>0.694300</td>\n",
              "      <td>-0.152867</td>\n",
              "      <td>-0.242488</td>\n",
              "      <td>0.334450</td>\n",
              "      <td>-0.012879</td>\n",
              "      <td>-0.439760</td>\n",
              "      <td>0.027636</td>\n",
              "      <td>0.211573</td>\n",
              "      <td>-0.152019</td>\n",
              "      <td>0.443454</td>\n",
              "      <td>0.189872</td>\n",
              "      <td>-0.087153</td>\n",
              "      <td>0.065904</td>\n",
              "      <td>-0.288331</td>\n",
              "      <td>-0.061381</td>\n",
              "      <td>0.108891</td>\n",
              "      <td>-0.796181</td>\n",
              "      <td>0.339856</td>\n",
              "      <td>-0.093294</td>\n",
              "      <td>0.041548</td>\n",
              "      <td>-0.243894</td>\n",
              "      <td>-0.124462</td>\n",
              "      <td>0.557559</td>\n",
              "      <td>-0.031636</td>\n",
              "      <td>0.070136</td>\n",
              "      <td>-0.164984</td>\n",
              "      <td>0.175314</td>\n",
              "      <td>0.032540</td>\n",
              "      <td>-0.095984</td>\n",
              "      <td>0.310582</td>\n",
              "      <td>-0.523515</td>\n",
              "      <td>0.102867</td>\n",
              "      <td>-0.095237</td>\n",
              "      <td>0.155819</td>\n",
              "      <td>-0.099150</td>\n",
              "      <td>-0.313666</td>\n",
              "      <td>-0.050592</td>\n",
              "      <td>0.274014</td>\n",
              "      <td>...</td>\n",
              "      <td>0.063199</td>\n",
              "      <td>0.111591</td>\n",
              "      <td>0.094315</td>\n",
              "      <td>-0.421820</td>\n",
              "      <td>-0.327503</td>\n",
              "      <td>0.472518</td>\n",
              "      <td>-0.379277</td>\n",
              "      <td>-0.224034</td>\n",
              "      <td>0.049340</td>\n",
              "      <td>0.259576</td>\n",
              "      <td>0.165265</td>\n",
              "      <td>0.250536</td>\n",
              "      <td>-0.287252</td>\n",
              "      <td>0.166743</td>\n",
              "      <td>-0.078082</td>\n",
              "      <td>0.244034</td>\n",
              "      <td>0.320087</td>\n",
              "      <td>-0.175295</td>\n",
              "      <td>-0.528370</td>\n",
              "      <td>0.041442</td>\n",
              "      <td>-0.037029</td>\n",
              "      <td>0.278757</td>\n",
              "      <td>-0.217770</td>\n",
              "      <td>0.025575</td>\n",
              "      <td>-0.130352</td>\n",
              "      <td>0.538218</td>\n",
              "      <td>0.134960</td>\n",
              "      <td>-0.030136</td>\n",
              "      <td>0.308377</td>\n",
              "      <td>0.165492</td>\n",
              "      <td>-0.429761</td>\n",
              "      <td>0.000302</td>\n",
              "      <td>-0.150751</td>\n",
              "      <td>-0.009321</td>\n",
              "      <td>-0.045260</td>\n",
              "      <td>-0.029173</td>\n",
              "      <td>0.082613</td>\n",
              "      <td>0.419390</td>\n",
              "      <td>-0.132150</td>\n",
              "      <td>0.165069</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 300 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       2000      2001      2002  ...      2297      2298      2299\n",
              "0  0.111041 -0.092938  0.196163  ...  0.248255 -0.011970  0.204146\n",
              "1  0.272954 -0.201667 -0.360242  ...  0.226228  0.146945  0.432084\n",
              "2  0.223640 -0.150633  0.518778  ...  0.325615 -0.119377  0.127518\n",
              "3  0.023332 -0.048139  0.106083  ...  0.251763  0.096795  0.276718\n",
              "4  0.163353 -0.148985  0.694300  ...  0.419390 -0.132150  0.165069\n",
              "\n",
              "[5 rows x 300 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqvPFbUUIcCn"
      },
      "source": [
        "#Word 2 Vec\n",
        "w2v_story_lst=[]\n",
        "for sent in combined_df['STORY'].values:\n",
        "  count=0\n",
        "  vec=np.zeros(300)\n",
        "  for word in sent.split():\n",
        "    try:\n",
        "      embeddings_index[word]\n",
        "      #vec+=embeddings_index[word]\n",
        "      #count+=1\n",
        "    except:\n",
        "      vec+=model.wv[word]\n",
        "      count+=1\n",
        "  if count!=0:\n",
        "    w2v_story_lst.append(vec/count)\n",
        "  else:\n",
        "    w2v_story_lst.append(vec)\n",
        "\n",
        "w2v_story_df3=pd.DataFrame(w2v_story_lst,columns=[i for i in range(3000,3300)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47rtJe8hxlcd",
        "outputId": "91629a51-d3eb-454e-c2a3-e6d7d2c97b4f"
      },
      "source": [
        "print(\"Most similar to: \\n\")\n",
        "for i in model.wv.most_similar(positive=[\"bjp\"]):\n",
        "  print(i)\n",
        "\n",
        "print(\"Most disimilar to: \\n\")\n",
        "for i in model.wv.most_similar(negative=[\"samsung\"]):\n",
        "  print(i)\n",
        "\n",
        "print(\"Similarity: \\n\")\n",
        "print(\"Similarity is : \",model.wv.similarity(\"samsung\", 'iphone'))\n",
        "\n",
        "print(\"Odd-One-Out: \\n\")\n",
        "print(\"Odd-One-Out is : \",model.wv.doesnt_match(['bjp', 'trs', 'india']))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most similar to: \n",
            "\n",
            "('alliance', 0.9948949813842773)\n",
            "('bjd', 0.9929726123809814)\n",
            "('ruling', 0.9908612966537476)\n",
            "('jds', 0.9883723258972168)\n",
            "('tdp', 0.9880916476249695)\n",
            "('trs', 0.9878060221672058)\n",
            "('democratic', 0.9871597290039062)\n",
            "('legislators', 0.9868444800376892)\n",
            "('cadre', 0.9866710901260376)\n",
            "('tmc', 0.9858370423316956)\n",
            "Most disimilar to: \n",
            "\n",
            "('selfdeclared', 0.8170148134231567)\n",
            "('ungli', 0.7817651629447937)\n",
            "('statethe', 0.7477179765701294)\n",
            "('deolbjp', 0.725529670715332)\n",
            "('paswanled', 0.698302149772644)\n",
            "('shoutout', 0.6702257394790649)\n",
            "('brotherwinning', 0.6653400659561157)\n",
            "('castecentric', 0.6569734215736389)\n",
            "('sikandrabad', 0.6506018042564392)\n",
            "('nalamada', 0.6465573310852051)\n",
            "Similarity: \n",
            "\n",
            "Similarity is :  0.95816165\n",
            "Odd-One-Out: \n",
            "\n",
            "Odd-One-Out is :  india\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WH5kEHtcA2iV",
        "outputId": "4d6044a5-96e4-41e7-a295-1e1c82f8180a"
      },
      "source": [
        "#combined_df=combined_df.drop(['STORY','frequent_word_1','frequent_word_2','frequent_word_3'],axis=1)\n",
        "#combined_df=combined_df.drop('STORY',axis=1)\n",
        "combined_df.reset_index(inplace=True)\n",
        "combined_df=pd.concat([combined_df,w2v_story_df1,tfidf_df],axis=1)\n",
        "combined_df=combined_df.drop('index',axis=1)\n",
        "combined_df.shape\n",
        "#combined_df=pd.concat([combined_df,w2v_story_df,frequent_word_1_df,frequent_word_2_df,frequent_word_3_df],axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10299, 384)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsngkDKJpQGl",
        "outputId": "c0072dff-0997-4fa0-849c-4398f23bb4bd"
      },
      "source": [
        "combined_df.reset_index(inplace=True)\n",
        "dummy_combined_df=combined_df.copy()\n",
        "combined_df=pd.concat([combined_df,w2v_story_df1,w2v_story_df3],axis=1)\n",
        "combined_df=combined_df.drop('index',axis=1)\n",
        "combined_df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10299, 616)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJjnL2dN-Jld"
      },
      "source": [
        "#Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0jNoPiQ-IoW",
        "outputId": "2a502c5e-11b5-4c24-b8c4-3aa20884b32c"
      },
      "source": [
        "train=combined_df[combined_df['SECTION'].isnull()!=True]\n",
        "test=combined_df[combined_df['SECTION'].isnull()==True]\n",
        "X=train.drop('SECTION',axis=1)\n",
        "X_drop=train.drop(['SECTION','STORY','frequent_word_1','frequent_word_2','frequent_word_3'],axis=1) #'target_pred_lda'\n",
        "y=train['SECTION']\n",
        "\n",
        "X_test=test.drop(['SECTION','STORY','frequent_word_1','frequent_word_2','frequent_word_3'],axis=1) #'target_pred_lda'\n",
        "\n",
        "print(X.shape,y.shape)\n",
        "print(X_drop.shape,y.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7551, 615) (7551,)\n",
            "(7551, 611) (7551,)\n",
            "(2748, 611)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySAkyUY_uADg",
        "outputId": "148ac0cd-7488-4afc-ace3-6d24ec959f37"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train1,x_val1,y_train,y_val=train_test_split(X,y,test_size=0.2,stratify=y,random_state=42)\n",
        "x_train=x_train1.drop(['STORY','frequent_word_1','frequent_word_2','frequent_word_3'],axis=1)\n",
        "x_val=x_val1.drop(['STORY','frequent_word_1','frequent_word_2','frequent_word_3'],axis=1)\n",
        "print(x_train.shape,y_train.shape)\n",
        "print(x_val.shape,y_val.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6040, 611) (6040,)\n",
            "(1511, 611) (1511,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KCf0mNJIOOu"
      },
      "source": [
        "# Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBcIrShJDsIG",
        "outputId": "d35ea3d4-0916-4529-b3fd-f45349a0154f"
      },
      "source": [
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "rf=ExtraTreesRegressor()\n",
        "rf.fit(X_drop,y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n",
              "                    max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
              "                    max_samples=None, min_impurity_decrease=0.0,\n",
              "                    min_impurity_split=None, min_samples_leaf=1,\n",
              "                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
              "                    n_estimators=100, n_jobs=None, oob_score=False,\n",
              "                    random_state=None, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "wjL8tKE4EUA_",
        "outputId": "ad030e13-b8ee-4dd4-a105-429720fb2c6b"
      },
      "source": [
        "feature_imp=rf.feature_importances_\n",
        "features=list(X_drop.columns)\n",
        "\n",
        "for i in range(len(features)):\n",
        "  if type(features[i])==int:\n",
        "    features[i]=str(features[i])\n",
        "\n",
        "fimp_dt=dict(zip(features,feature_imp))\n",
        "features_sorted=[]\n",
        "feature_imp_sorted=[]\n",
        "for w in sorted(fimp_dt, key=fimp_dt.get, reverse=True):\n",
        "  features_sorted.append(w)\n",
        "  feature_imp_sorted.append(fimp_dt[w])\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.bar(features_sorted[:10],feature_imp_sorted[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 10 artists>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABIcAAAEvCAYAAADfBqG/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAc40lEQVR4nO3dfbRlZ10f8O+vMyQoSrDJoDUvTGiCNCBSGEKxvGkKhIVlRJMSpJpqVrOoZNXqcsmwXKY06GriG9oSq6mJifElwSjt0IxGJb7UgjETCMIEo0OIZiKVvDVtioEM+fWPswculzuZczP3zr29z+ez1qzZ59nPPvd31rP2Pud+77OfU90dAAAAAMb0d9a6AAAAAADWjnAIAAAAYGDCIQAAAICBCYcAAAAABiYcAgAAABiYcAgAAABgYJvXuoDFjjvuuN66detalwEAAACwYdxyyy33dveWpfatu3Bo69at2b1791qXAQAAALBhVNVfHmyf28oAAAAABiYcAgAAABiYcAgAAABgYMIhAAAAgIEJhwAAAAAGJhwCAAAAGJhwCAAAAGBgwiEAAACAgQmHAAAAAAYmHAIAAAAYmHAIAAAAYGCb17qAjWzrjuvXuoQN486LX7PWJQAAAMCGZOYQAAAAwMCEQwAAAAADEw4BAAAADEw4BAAAADAw4RAAAADAwIRDAAAAAAMTDgEAAAAMTDgEAAAAMDDhEAAAAMDAhEMAAAAAAxMOAQAAAAxMOAQAAAAwMOEQAAAAwMCEQwAAAAADEw4BAAAADEw4BAAAADAw4RAAAADAwIRDAAAAAAMTDgEAAAAMTDgEAAAAMLC5wqGqOrOqbq+qvVW1Y4n9R1fVtdP+m6pq64J9z6mq91fVnqr6cFU9ceXKBwAAAOBwHDIcqqpNSS5N8uokpyV5Q1WdtqjbeUke6O5TkrwjySXTsZuT/FKSN3X3s5K8PMkjK1Y9AAAAAIdlnplDpyfZ2913dPdnklyTZPuiPtuTXDVtX5fkjKqqJK9M8qfd/aEk6e77uvuzK1M6AAAAAIdrnnDo+CR3LXi8b2pbsk9370/yYJJjkzwjSVfVDVX1gar6gcMvGQAAAICVsvkIPP+Lk7wgyaeSvLeqbunu9y7sVFXnJzk/SU466aRVLgkAAACAA+aZOXR3khMXPD5haluyz7TO0DFJ7stsltEfdve93f2pJLuSPG/xD+juy7p7W3dv27Jly/JfBQAAAACPyzzh0M1JTq2qk6vqqCTnJNm5qM/OJOdO22clubG7O8kNSb62qr50Co1eluS2lSkdAAAAgMN1yNvKunt/VV2QWdCzKckV3b2nqi5Ksru7dya5PMnVVbU3yf2ZBUjp7geq6iczC5g6ya7uvn6VXgsAAAAAyzTXmkPdvSuzW8IWtl24YPvhJGcf5Nhfyuzr7AEAAABYZ+a5rQwAAACADUo4BAAAADAw4RAAAADAwIRDAAAAAAMTDgEAAAAMTDgEAAAAMDDhEAAAAMDAhEMAAAAAAxMOAQAAAAxMOAQAAAAwMOEQAAAAwMCEQwAAAAADEw4BAAAADEw4BAAAADAw4RAAAADAwIRDAAAAAAMTDgEAAAAMTDgEAAAAMDDhEAAAAMDAhEMAAAAAAxMOAQAAAAxMOAQAAAAwMOEQAAAAwMCEQwAAAAADEw4BAAAADEw4BAAAADAw4RAAAADAwIRDAAAAAAMTDgEAAAAMTDgEAAAAMLC5wqGqOrOqbq+qvVW1Y4n9R1fVtdP+m6pq69S+tar+tqpunf797MqWDwAAAMDh2HyoDlW1KcmlSV6RZF+Sm6tqZ3fftqDbeUke6O5TquqcJJckef2072Pd/dwVrhsAAACAFTDPzKHTk+zt7ju6+zNJrkmyfVGf7UmumravS3JGVdXKlQkAAADAapgnHDo+yV0LHu+b2pbs0937kzyY5Nhp38lV9cGq+oOqeslh1gsAAADACjrkbWWH6RNJTuru+6rq+Un+S1U9q7v/98JOVXV+kvOT5KSTTlrlkgAAAAA4YJ6ZQ3cnOXHB4xOmtiX7VNXmJMckua+7P93d9yVJd9+S5GNJnrH4B3T3Zd29rbu3bdmyZfmvAgAAAIDHZZ5w6OYkp1bVyVV1VJJzkuxc1GdnknOn7bOS3NjdXVVbpgWtU1VPT3JqkjtWpnQAAAAADtchbyvr7v1VdUGSG5JsSnJFd++pqouS7O7unUkuT3J1Ve1Ncn9mAVKSvDTJRVX1SJJHk7ypu+9fjRcCAAAAwPLNteZQd+9KsmtR24ULth9OcvYSx/16kl8/zBoBAAAAWCXz3FYGAAAAwAYlHAIAAAAYmHAIAAAAYGDCIQAAAICBCYcAAAAABiYcAgAAABiYcAgAAABgYMIhAAAAgIEJhwAAAAAGJhwCAAAAGJhwCAAAAGBgwiEAAACAgQmHAAAAAAYmHAIAAAAYmHAIAAAAYGDCIQAAAICBCYcAAAAABiYcAgAAABiYcAgAAABgYMIhAAAAgIEJhwAAAAAGJhwCAAAAGJhwCAAAAGBgwiEAAACAgQmHAAAAAAYmHAIAAAAYmHAIAAAAYGDCIQAAAICBCYcAAAAABiYcAgAAABjY5nk6VdWZSX46yaYkP9/dFy/af3SSX0zy/CT3JXl9d9+5YP9JSW5L8rbu/vGVKR0Oz9Yd1691CRvCnRe/Zq1LAAAA4DAccuZQVW1KcmmSVyc5Lckbquq0Rd3OS/JAd5+S5B1JLlm0/yeT/ObhlwsAAADASprntrLTk+zt7ju6+zNJrkmyfVGf7UmumravS3JGVVWSVNU3J/l4kj0rUzIAAAAAK2WecOj4JHcteLxvaluyT3fvT/JgkmOr6suSvCXJvzv8UgEAAABYaau9IPXbkryjux96rE5VdX5V7a6q3ffcc88qlwQAAADAAfMsSH13khMXPD5haluqz76q2pzkmMwWpn5hkrOq6keTPCXJo1X1cHe/c+HB3X1ZksuSZNu2bf14XggAAAAAyzdPOHRzklOr6uTMQqBzknzboj47k5yb5P1JzkpyY3d3kpcc6FBVb0vy0OJgCAAAAIC1c8hwqLv3V9UFSW7I7Kvsr+juPVV1UZLd3b0zyeVJrq6qvUnuzyxAAgAAAGCdm2fmULp7V5Jdi9ouXLD9cJKzD/Ecb3sc9QEAAACwilZ7QWoAAAAA1jHhEAAAAMDAhEMAAAAAAxMOAQAAAAxMOAQAAAAwMOEQAAAAwMCEQwAAAAADEw4BAAAADEw4BAAAADAw4RAAAADAwIRDAAAAAAMTDgEAAAAMTDgEAAAAMDDhEAAAAMDAhEMAAAAAAxMOAQAAAAxMOAQAAAAwMOEQAAAAwMCEQwAAAAADEw4BAAAADEw4BAAAADAw4RAAAADAwIRDAAAAAAMTDgEAAAAMTDgEAAAAMLDNa10AwGJbd1y/1iVsGHde/Jq1LgEAAFjnzBwCAAAAGJhwCAAAAGBgwiEAAACAgQmHAAAAAAY2VzhUVWdW1e1Vtbeqdiyx/+iqunbaf1NVbZ3aT6+qW6d/H6qq161s+QAAAAAcjkOGQ1W1KcmlSV6d5LQkb6iq0xZ1Oy/JA919SpJ3JLlkav9Ikm3d/dwkZyb5uaryDWkAAAAA68Q8M4dOT7K3u+/o7s8kuSbJ9kV9tie5atq+LskZVVXd/anu3j+1PzFJr0TRAAAAAKyMecKh45PcteDxvqltyT5TGPRgkmOTpKpeWFV7knw4yZsWhEUAAAAArLFVX5C6u2/q7mcleUGSt1bVExf3qarzq2p3Ve2+5557VrskAAAAACbzhEN3JzlxweMTprYl+0xrCh2T5L6FHbr7o0keSvLsxT+guy/r7m3dvW3Lli3zVw8AAADAYZknHLo5yalVdXJVHZXknCQ7F/XZmeTcafusJDd2d0/HbE6SqnpakmcmuXNFKgcAAADgsB3ym8O6e39VXZDkhiSbklzR3Xuq6qIku7t7Z5LLk1xdVXuT3J9ZgJQkL06yo6oeSfJoku/u7ntX44UAAAAAsHxzfa18d+9KsmtR24ULth9OcvYSx12d5OrDrBEAAACAVbLqC1IDAAAAsH7NNXMIAA7YuuP6tS5hQ7jz4tesdQkAAJDEzCEAAACAoQmHAAAAAAYmHAIAAAAYmHAIAAAAYGDCIQAAAICB+bYyANggfJPcyvFtcgDASMwcAgAAABiYcAgAAABgYMIhAAAAgIEJhwAAAAAGJhwCAAAAGJhwCAAAAGBgwiEAAACAgQmHAAAAAAYmHAIAAAAYmHAIAAAAYGDCIQAAAICBCYcAAAAABiYcAgAAABiYcAgAAABgYMIhAAAAgIEJhwAAAAAGJhwCAAAAGJhwCAAAAGBgwiEAAACAgQmHAAAAAAYmHAIAAAAYmHAIAAAAYGBzhUNVdWZV3V5Ve6tqxxL7j66qa6f9N1XV1qn9FVV1S1V9ePr/G1e2fAAAAAAOx+ZDdaiqTUkuTfKKJPuS3FxVO7v7tgXdzkvyQHefUlXnJLkkyeuT3Jvkn3b3X1fVs5PckOT4lX4RAADr3dYd1691CRvCnRe/Zq1LAIANZ56ZQ6cn2dvdd3T3Z5Jck2T7oj7bk1w1bV+X5Iyqqu7+YHf/9dS+J8mXVNXRK1E4AAAAAIdvnnDo+CR3LXi8L188++dzfbp7f5IHkxy7qM+3JvlAd3/68ZUKAAAAwEo75G1lK6GqnpXZrWavPMj+85OcnyQnnXTSkSgJAAAAgMw3c+juJCcueHzC1LZkn6ranOSYJPdNj09I8u4k39HdH1vqB3T3Zd29rbu3bdmyZXmvAAAAAIDHbZ6ZQzcnObWqTs4sBDonybct6rMzyblJ3p/krCQ3dndX1VOSXJ9kR3f/j5UrGwAAVobFwleOBcMB/v90yJlD0xpCF2T2TWMfTfKu7t5TVRdV1WunbpcnObaq9ib5viQHvu7+giSnJLmwqm6d/j11xV8FAAAAAI/LXGsOdfeuJLsWtV24YPvhJGcvcdwPJ/nhw6wRAAAAgFVyRBakBgAAeDzc9rdy3PYHHIxwCAAAgGUT3K0cwR1rTTgEAAAAG4zwbuWMEN7N81X2AAAAAGxQwiEAAACAgQmHAAAAAAYmHAIAAAAYmHAIAAAAYGDCIQAAAICBCYcAAAAABiYcAgAAABiYcAgAAABgYMIhAAAAgIEJhwAAAAAGJhwCAAAAGJhwCAAAAGBgwiEAAACAgQmHAAAAAAYmHAIAAAAYmHAIAAAAYGDCIQAAAICBCYcAAAAABiYcAgAAABiYcAgAAABgYMIhAAAAgIEJhwAAAAAGJhwCAAAAGJhwCAAAAGBgwiEAAACAgQmHAAAAAAY2VzhUVWdW1e1Vtbeqdiyx/+iqunbaf1NVbZ3aj62q36uqh6rqnStbOgAAAACH65DhUFVtSnJpklcnOS3JG6rqtEXdzkvyQHefkuQdSS6Z2h9O8kNJvn/FKgYAAABgxcwzc+j0JHu7+47u/kySa5JsX9Rne5Krpu3rkpxRVdXd/7e7/yizkAgAAACAdWaecOj4JHcteLxvaluyT3fvT/JgkmNXokAAAAAAVs+6WJC6qs6vqt1Vtfuee+5Z63IAAAAAhjFPOHR3khMXPD5haluyT1VtTnJMkvvmLaK7L+vubd29bcuWLfMeBgAAAMBhmiccujnJqVV1clUdleScJDsX9dmZ5Nxp+6wkN3Z3r1yZAAAAAKyGzYfq0N37q+qCJDck2ZTkiu7eU1UXJdnd3TuTXJ7k6qram+T+zAKkJElV3ZnkyUmOqqpvTvLK7r5t5V8KAAAAAMt1yHAoSbp7V5Jdi9ouXLD9cJKzD3Ls1sOoDwAAAIBVtC4WpAYAAABgbQiHAAAAAAYmHAIAAAAYmHAIAAAAYGDCIQAAAICBCYcAAAAABiYcAgAAABiYcAgAAABgYMIhAAAAgIEJhwAAAAAGJhwCAAAAGJhwCAAAAGBgwiEAAACAgQmHAAAAAAYmHAIAAAAYmHAIAAAAYGDCIQAAAICBCYcAAAAABiYcAgAAABiYcAgAAABgYMIhAAAAgIEJhwAAAAAGJhwCAAAAGJhwCAAAAGBgwiEAAACAgQmHAAAAAAYmHAIAAAAYmHAIAAAAYGDCIQAAAICBCYcAAAAABjZXOFRVZ1bV7VW1t6p2LLH/6Kq6dtp/U1VtXbDvrVP77VX1qpUrHQAAAIDDdchwqKo2Jbk0yauTnJbkDVV12qJu5yV5oLtPSfKOJJdMx56W5Jwkz0pyZpKfmZ4PAAAAgHVgnplDpyfZ2913dPdnklyTZPuiPtuTXDVtX5fkjKqqqf2a7v50d388yd7p+QAAAABYB+YJh45PcteCx/umtiX7dPf+JA8mOXbOYwEAAABYI5vXuoAkqarzk5w/PXyoqm5fy3oGc1ySe9e6iEOpS9a6gjWz7sdn4LFJjM96ZmzWN+Ozfhmb9c34rF/rfmwS47PWRTwWY7O+baDxedrBdswTDt2d5MQFj0+Y2pbqs6+qNic5Jsl9cx6b7r4syWVz1MIKq6rd3b1tretgacZnfTM+65exWd+Mz/plbNY347N+GZv1zfisX8Zm/ZjntrKbk5xaVSdX1VGZLTC9c1GfnUnOnbbPSnJjd/fUfs70bWYnJzk1yZ+sTOkAAAAAHK5Dzhzq7v1VdUGSG5JsSnJFd++pqouS7O7unUkuT3J1Ve1Ncn9mAVKmfu9KcluS/Une3N2fXaXXAgAAAMAyzbXmUHfvSrJrUduFC7YfTnL2QY79kSQ/chg1srrczre+GZ/1zfisX8ZmfTM+65exWd+Mz/plbNY347N+GZt1omZ3fwEAAAAwonnWHAIAAABggxIObUBVdUVVfbKqPrKg7ceq6s+q6k+r6t1V9ZQF+95aVXur6vaqetWC9jur6sNVdWtV7T7Sr2MjOsjY/N2q+p2q+ovp/6+Y2p9ZVe+vqk9X1fcveh5jswqWOT7HVNV7qupDVbWnqr5z0XM9uar2VdU7j/Tr2IiWc12rqq1V9bfT+XFrVf3sgmOOqqrLqurPp2O/dS1ez0ZzkPE5ezo3Hq2qbQva37hgbG6d9j932vf6aTz3VG2gL41dY8s8f46qql+Y3mM+VFUvX+L5di58Lh6/Zb7vvHEarw9X1fuq6usWPdemqvpgVf23I/06NqplnjunL7iufaiqXrfgmO+drmsfqapfraonrsXr2UiW+b5z0OtaVf1+zX4HOjB2Tz3CL2VDWua1bft0Pt1aVbur6sULjvnRaUw/WlX/oapqLV7PKIRDG9OVSc5c1PY7SZ7d3c9J8udJ3pokVXVaZguIP2s65meqatOC476hu5/r6wVXzJX54rHZkeS93X1qkvdOj5PZ4u7/OsmPH+S5jM3KuzLzj8+bk9zW3V+X5OVJfqJm3+h4wNuT/OGqVjuWKzPndW3ysen8eG53v2lB+w8m+WR3PyPJaUn+YBVrHsmV+eLx+UiSb8mi86C7f/nA2CT59iQf7+5bq+rYJD+W5IzuflaSr6qqM1a/9CFcmfnPn3+ZJN39tUlekdm17XOfF6vqW5I8tNoFD+TKzP++8/EkL5vG5u354nU6vifJR1ev1CFdmfnPnY8k2TZd285M8nNVtbmqjs/s89y27n52Zl/wc86RKH6DuzJzvu/kENe1JG9c8Jnhk6tU72iuzPzXtvcm+brp3PmuJD+fJFX19Un+cZLnJHl2khckedmqVz4w4dAG1N1/mFmwsLDtt7t7//Twj5OcMG1vT3JNd3+6uz+eZG+S049YsYNZamwyG4Orpu2rknzz1PeT3X1zkkeOXIVjW874JOkkXz79BePLpuP2J0lVPT/JVyb57dWueRTLvK49lu9K8u+n4x/t7ntXtNBBHWR8Ptrdtx/i0DckuWbafnqSv+jue6bHv5vEzK4VsMzz57QkN059PpnkfyXZliRV9WVJvi/JDx+BsoewzM8F7+vuB6b2L7jmVdUJSV6T6ZcqVsZyzp3u/tSC9idm9jnhgM1JvqSqNif50iR/vaqFD2CZ7zsHva6xOpZ5bXuoP78Q8pPy+XOnMzuXjkpydJInJPmbVSx7eMKhMX1Xkt+cto9PcteCffumtmR2Qv52Vd1SVecfwfpG85Xd/Ylp+39mFiocirE5cg42Pu9M8g8y+4D34STf092PTn+J+okk3/9Fz8RqWnhdS5KTp9sr/qCqXpIk9fnbad9eVR+oql+rqnnON1bP65P86rS9N8nX1Oy2wM2ZfWg8cc0qG8vC8+dDSV47zXg4Ocnz8/lxeHtm17dPHfkShzLP54Lz8oXXvJ9K8gNJHl3l2vhCX/DeU1UvrKo9mX0ueFN37+/uuzObBf5XST6R5MHu9sejI+uxrmtJ8gvTLU0/5LalVXXQa1tVva6q/izJ9ZmdV+nu9yf5vczOm08kuaG7zY5cRcKhwVTVD2Y2u+GX5+j+4u5+XpJXJ3lzVb10VYsjU2o+z1cIGps1sGh8XpXk1iRfneS5Sd5ZVU9O8t1JdnX3vrWpcjxLXNc+keSk7v6Hmc1y+JVpbDZn9hfe903nz/tz8Ns2WWVV9cIkn+rujyTJNCPiXyW5Nsl/T3Jnks+uWYGDWOL8uSKzPxTtzixweF+Sz9ZsXai/393vXpNCB7XU54Kq+obMwqG3TI+/KbPbZW858hWOa6nP1N1903Rb7AuSvLWqnjitq7I9ycmZfWZ4UlX987WoeWBLXtemfW+cbjd7yfTv29ekwsEsvrZ197u7+5mZ/WHo7UlSVadk9ofYEzKbvPCNB/7gx+oQDg2kqv5Fkm/K7CJ44GS8O1+YnJ8wtWX6S8eB6ZfvjtvNVsvfVNXfS5Lp/0Pe62xsjqiDjc93JvmNntmb2VoQz0zyoiQXVNWdmQUP31FVFx/5ssew1HVtuk32vmn7liQfS/KMJPdlNuPhN6bDfy3J8450zXzOOfn8rKEkSXe/p7tf2N0vSnJ7Zut5sEoOcv7s7+7vndbe2J7kKZmNw4uSbJuubX+U5BlV9ftrUvjGd9DPBVX1nMxuHdt+4DqX2Zocr53G5prMfoH6pSNb8lgO8pn6c6bZDQ9ltk7KP8lsbbV7uvuRzN6Dvv4Ilju8x7iuLfxM/X+S/Ep8pl5Nh/ydZ7od7elVdVyS1yX54+m2s4cym6X3oiNZ8GiEQ4OoqjMzm2782u5eOB18Z5JzquroaZrlqUn+pKqeVFVfPh37pCSvzGyRN1beziTnTtvnJvmvj9XZ2BxxBxufv0pyRpJMtyZ9TZI7uvuN3X1Sd2/N7NayX+zuHWHFHey6VlVbDiysX1VPz+y6dsf0Af49mS0gnszG77YjWjRJkun2y3+Wz683dKD9qdP/X5HZLDzrp6ySxzh/vnR6b0lVvSLJ/u6+rbv/U3d/9XRte3GSP+/ul69B6SNY8n2nqk7KLFj49u7+XHDa3W/t7hOmsTknyY3dbWbKKnmMc+fk6ZbYVNXTMvuD0Z2ZfV74R9O5VZm997g15gg62HVtus3suKn9CZkFfj5Tr56DXdtOOXA7X1U9L7P1he7L7Nx52TROT8hsMWrnziravNYFsPKq6lcz++XnuKral+TfZvZNCkcn+Z3p3Pvj7n5Td++pqndl9gvS/iRv7u7PTr/svnvquznJr3T3bx35V7OxHGRsLk7yrqo6L8lfZvYLU6rqqzKb/vrkJI9W1b/JbEG942JsVsVyxiezKa9XVtWHk1SSt1jcePUs57qW5KVJLqqqRzJbf+NN3X1gUcS3JLm6qn4qyT2ZzQDjMB1kfO5P8h+TbElyfVXd2t2vmg55aZK7uvuORU/10/X5r+e+aOEvwDx+yzx/nprkhqp6NLOZxG6xWEXLfN+5MMmxmX2zbDL7BdeiuqtomefOi5PsWPDe893T54J7q+q6JB/I7LP2B/PF3zTHMi3zfedg17Wjp/YnZPYtcr+b5D8f0ReyQS3z2vatmc20fyTJ3yZ5fXf3dN58Y2ZreHWS3+ru9xzZVzKWWmImJAAAAACDcFsZAAAAwMCEQwAAAAADEw4BAAAADEw4BAAAADAw4RAAAADAwIRDAAAAAAMTDgEAAAAMTDgEAAAAMLD/Bxy0MLkBDEcbAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1440x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "_btJZoSShYvH",
        "outputId": "0f79b14e-354e-4ff1-c955-846d7c921d64"
      },
      "source": [
        "plt.figure(figsize=(20,5))\n",
        "plt.bar(features_sorted[-10:],feature_imp_sorted[-10:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 10 artists>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzdfO0SdSLJm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b24f4fe1-a557-4046-cc57-b22e3f350225"
      },
      "source": [
        "for i in range(100):\n",
        "  print(i,features_sorted[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 1205\n",
            "1 1015\n",
            "2 1084\n",
            "3 1256\n",
            "4 1179\n",
            "5 1294\n",
            "6 1024\n",
            "7 1238\n",
            "8 1195\n",
            "9 1038\n",
            "10 1090\n",
            "11 1208\n",
            "12 1020\n",
            "13 1099\n",
            "14 1252\n",
            "15 1027\n",
            "16 1138\n",
            "17 1161\n",
            "18 1071\n",
            "19 1160\n",
            "20 1094\n",
            "21 3070\n",
            "22 1277\n",
            "23 1003\n",
            "24 1196\n",
            "25 1246\n",
            "26 1046\n",
            "27 1209\n",
            "28 3110\n",
            "29 1291\n",
            "30 1116\n",
            "31 1214\n",
            "32 frequency_3\n",
            "33 1297\n",
            "34 1200\n",
            "35 3203\n",
            "36 1225\n",
            "37 1210\n",
            "38 3019\n",
            "39 1122\n",
            "40 1111\n",
            "41 1224\n",
            "42 3027\n",
            "43 1049\n",
            "44 3286\n",
            "45 3298\n",
            "46 1278\n",
            "47 1262\n",
            "48 1146\n",
            "49 1110\n",
            "50 sent_len\n",
            "51 3195\n",
            "52 1158\n",
            "53 3034\n",
            "54 3251\n",
            "55 1136\n",
            "56 1019\n",
            "57 1292\n",
            "58 1083\n",
            "59 3256\n",
            "60 frequency_2\n",
            "61 1251\n",
            "62 3219\n",
            "63 1276\n",
            "64 3058\n",
            "65 1216\n",
            "66 frequency_1\n",
            "67 1057\n",
            "68 1059\n",
            "69 1206\n",
            "70 1295\n",
            "71 1261\n",
            "72 1108\n",
            "73 1271\n",
            "74 3001\n",
            "75 1247\n",
            "76 3129\n",
            "77 3026\n",
            "78 num_unique_words\n",
            "79 1065\n",
            "80 1010\n",
            "81 1283\n",
            "82 1131\n",
            "83 1086\n",
            "84 1182\n",
            "85 1014\n",
            "86 3033\n",
            "87 1140\n",
            "88 1115\n",
            "89 1089\n",
            "90 3211\n",
            "91 1005\n",
            "92 3279\n",
            "93 3210\n",
            "94 1235\n",
            "95 1121\n",
            "96 1287\n",
            "97 1037\n",
            "98 1009\n",
            "99 3156\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwqIwx1lMaOO"
      },
      "source": [
        "#Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aNDOXmjMeEm"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8Itrqp2FRS4",
        "outputId": "2b6ea015-ae3a-48a0-e436-f65855d7ec61"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "rf=RandomForestClassifier(n_estimators=1000,max_depth=100)\n",
        "rf.fit(x_train,y_train)\n",
        "#rf.fit(X_drop,y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=100, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrO43Bg1POf_",
        "outputId": "26f0d0c8-0a73-4121-bacc-b49871b0d14b"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "y_train_pred=rf.predict(x_train)\n",
        "y_val_pred=rf.predict(x_val)\n",
        "print('Train accuracy_score:', accuracy_score(y_train, y_train_pred))\n",
        "print('Val accuracy_score:', accuracy_score(y_val, y_val_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train accuracy_score: 0.9998344370860928\n",
            "Val accuracy_score: 0.9463931171409663\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEUbwxEGyEcb",
        "outputId": "9ddf5c59-ac84-4bc2-d407-dd96434cb69f"
      },
      "source": [
        "new_count=0\n",
        "for count,i in enumerate(range(len(y_val_pred))):\n",
        "  if y_val_pred[i]-y_val.iloc[i]!=0:\n",
        "    new_count+=1\n",
        "    print(x_val1['STORY'].iloc[i])\n",
        "    print(count,y_val.iloc[i],y_val_pred[i])\n",
        "\n",
        "print(new_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "story published wire agency feed without modification text headline changed\n",
            "521 1.0 3.0\n",
            "story published wire agency feed without modification text headline changed\n",
            "881 0.0 3.0\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3pYoWAyPqkT"
      },
      "source": [
        "y_test_pred=rf.predict(X_test)\n",
        "\n",
        "y_test_pred=[int(i) for i in y_test_pred]\n",
        "Submission_df=pd.DataFrame({'SECTION':y_test_pred})\n",
        "Submission_df.to_csv('Submission_14.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwKAh2-8JQHG"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "rf=RandomForestClassifier()\n",
        "params={'n_estimators':[int(x) for x in np.linspace(100,1500,15)],\n",
        "        'max_depth':[int(x) for x in np.linspace(10,100,10)],\n",
        "}\n",
        "rand_rf=RandomizedSearchCV(rf,params,n_iter=100,verbose=1,cv=3,n_jobs=-1)\n",
        "rand_rf.fit(X,y)\n",
        "print('Best Parameters obtained from RamdomSearchCV is:',rand_rf.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKhZSejVJfCk"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "y_pred=rand_rf.predict(X)\n",
        "y_test_pred=rand_rf.predict(X_test)\n",
        "print('accuracy_score:', accuracy_score(y, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfJWd3aDFe3B",
        "outputId": "9351983e-fd50-4eba-9aa0-0f2dfee1a6d5"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "y_pred=rf.predict(X)\n",
        "y_test_pred=rf.predict(X_test)\n",
        "print('accuracy_score:', accuracy_score(y, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy_score: 0.9976402726796014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPbz-vUnMi0p"
      },
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_0g1hi1G7n6",
        "outputId": "f58b7596-ed3d-4887-a9ea-3e57757cef84"
      },
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "xgb_clf=xgb.XGBClassifier(n_estimators=800,max_depth=80)\n",
        "\n",
        "xgb_clf.fit(x_train,y_train)\n",
        "#xgb_clf.fit(X_drop,y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "              learning_rate=0.1, max_delta_step=0, max_depth=80,\n",
              "              min_child_weight=1, missing=None, n_estimators=800, n_jobs=1,\n",
              "              nthread=None, objective='multi:softprob', random_state=0,\n",
              "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "              silent=None, subsample=1, verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAHPv7TjjY86",
        "outputId": "26186530-d4a8-4065-deeb-4589fbb9c07f"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "y_train_pred=xgb_clf.predict(x_train)\n",
        "y_val_pred=xgb_clf.predict(x_val)\n",
        "print('Train accuracy_score:', accuracy_score(y_train, y_train_pred))\n",
        "print('Val accuracy_score:', accuracy_score(y_val, y_val_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train accuracy_score: 0.9998344370860928\n",
            "Val accuracy_score: 0.9642620780939775\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHP47rwtmEVS",
        "outputId": "0251ef31-533a-4042-f004-8e156d601495"
      },
      "source": [
        "new_count=0\n",
        "new_val_lst=[]\n",
        "for count,i in enumerate(range(len(y_val_pred))):\n",
        "  if y_val_pred[i]-y_val.iloc[i]!=0:\n",
        "    new_count+=1\n",
        "    new_val_lst.append(x_val1['STORY'].iloc[i])\n",
        "    print(x_val1['STORY'].iloc[i])\n",
        "    print(count,y_val.iloc[i],y_val_pred[i])\n",
        "\n",
        "print(new_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "smartphone app allow citizens record polling misconduct malpractice distributing liquor drugs common method vote buying complainant remain anonymous upload photo video app officials obliged respond action taken within 100 minutes tollfree helpline number voter information feedback complaints also operate first time apart citizen monitors million polling staff involved running 2014 election india facebooks biggest market smartphone use exploding since last election thanks worlds cheapest data tariffs candidates declare social media accounts legal filings apply become election candidate aimed monitoring ending misuse social media polls country online misinformation rife\n",
            "83 0.0 1.0\n",
            "mcc comes force soon election schedule announced remains force till election results declared\n",
            "91 2.0 0.0\n",
            "slowdown indias rural economy deepened january reflecting crunch spending agricultural workers inflation halved modi govt averaging consumerprice growth averaged 20092013 slowdown rural economy taken shine key achievement narendra modi govt mumbai subdued inflation india proving dilemma prime minister narendra modi weeks lok sabha elections 2019 politicians heading elections would want cheer efforts taming inflation emerging economies like india price surges oust governmentsmodi restrained achievement\n",
            "119 0.0 3.0\n",
            "new delhi bollywood actor sunny deol joined bjp tuesday presence defence minister nirmala sitharaman railway minister piyush goyal deol met bjp president amit shah pune airport briefly last week way father film star dharmendra attached atalji today come join modiji said deol whatever family bjp iti wont talk show work deol added\n",
            "151 0.0 2.0\n",
            "yeti spent long time walking around himalayas prime minister narendra modi certain heard yeti happened wander bit would come across least heard whispered migothe tibetan word yeti unfortunately theres hindi sanskrit name indian army definitely knows beast trouble one knows looks like thing abominable yeti course cussed refusal seen bit like indian members parliament weeks isnt election season tell voters youve seen yeti dynasty longrunning american prime time television serial theres reason longrunning prime time entertaining anything else telly real lifeand certainly elections ran eight years 1981 1989arguably tiny bit shorter political families government reunion 1991 started showing new reboot series 2017how shameless get curses theres need get offensive curses hone cursing skills find better uses curse enemy political camp vanishes dont bother refusing sign development project papers villages didnt vote curse villages polling booths disappear since village polling booths government schools youd fulfilled dedevelopment pledge anyway see school bed tea promise voters vote naturally bed tea week every week return constituency steaming kettle tea visit every home dawn pour hot mug help keep touch constituent sit spot charcha try say taj everytime pour cup get suspicious kind people mistaking admirer mausoleum love certainly wouldnt anybodys cup tea\n",
            "169 0.0 2.0\n",
            "election funding root much corruption india completely overhauled must rest easy\n",
            "175 0.0 1.0\n",
            "shown income 4803 lakh income tax return 201718 affidavit filed 2014 declared income lakh bjp south delhi ramesh bidhuri movable immovable assets worth crore increase close crore last five years according affidavit bidhuri wife dependent himanshu shown income 1672 lakh 309 lakh 318 lakh respectively returns filed 201718 57yearold leader pitted debutants vijender singh congress aam aadmi partys raghav chadha south delhi seat declared movable assets worth 137 crore wife cash jewellery worth 1321 lakh\n",
            "269 0.0 3.0\n",
            "per data rights issue received bids 11092857339 shares 2000 hours showed status active\n",
            "275 3.0 1.0\n",
            "organization held similar function april 2014 invited modi event chief minister gujarat prime ministerial candidate bjp national democratic alliance nda tried ease lives businesses traders five years tenure said tried ease life business traders want traders work without fear honesty increases transparency rises lead development country trading community always worked people fulfilled demands people modi said\n",
            "304 0.0 3.0\n",
            "first shift primetime televisionwhen people returned home every evening watch favourite programmesto ondemand viewership new delhi television programming seen multiple shifts years according ali hussein chief operating officer eros digital first shift primetime televisionwhen people returned home every evening watch favourite programmesto ondemand viewership\n",
            "335 1.0 2.0\n",
            "indian software service saas market hand expected grow per annum touch 3334 billion 2022 boost expected back lower cost workforce abundant talent availability mature sales ecosystem adoption newer technologies like machine learning build products globally cloud spending growing 165 cagr expected touch 345 billion 2022 accelerate cloud growth momentum india right strategic direction report recommended indian government set tangible timebound targets cloud sharper focus multitude infrastructural operational factors reliable power sustainable land regulations highspeed connectivity\n",
            "370 1.0 3.0\n",
            "numbers crore cant right figure almost every smartphone ownerfrom grandpa maid homeuses whatsapp potential target groups working roundtheclock reach people across agegroups using whatsapp safe say facebookowned platform reaches crore indians almost size facebook users country even bigger tarun pathak associate director counterpoint research told ians reliance jio data gone ultracheap political parties seen livestream rallies press meets debates facebook youtube reach target audiences hinterlands 87000 groups aiming influence voters currently active whatsapp fake statistics related various government policies news promoting regional violence manipulated political news government scams historical myths propaganda patriotism hindu nationalism whatsapp election season informed social media expert anoop mishra\n",
            "385 0.0 1.0\n",
            "sony already rolled streaming service playstation released 2014 streaming technology limited investment data centers held back service users complaining lag times asumi maeda spokeswoman sony interactive entertainment said game industry heating something make players happy nintendo spokesman declined comment technology adapt people way around google chief executive officer sundar pichai said event dead serious making technology accessible everyone google wouldnt say much charge users whether funded advertising like businesses service launch later year company said without announcing partnerships toptier game developers\n",
            "397 3.0 1.0\n",
            "able win seat last time hearts smritiji worked development work done one said referring union minister smriti irani unsuccessfully contested last election amethi happy works done cared tried honestly welfarehave done injustice modi told rally voted oursthis mantra sabka saath sabka vikas modi said\n",
            "440 0.0 2.0\n",
            "phenomenal interest shown audiences across country testament one anticipated movies fans cant wait watch partnering bookmyshow enabled reach cinemalovers across country incisive data analytics gathered millions users avengers infinity war mad titan committing biggest genocide history halving population universe using infinity stones\n",
            "479 2.0 1.0\n",
            "promises one talkedabout youtube originals ever robert kyncl youtubes chief business officer said announcing deal thursday\n",
            "494 2.0 1.0\n",
            "story published wire agency feed without modifications text headline changed\n",
            "521 1.0 3.0\n",
            "marvels 616 documentary series exploring intersection marvel stories characters real world chef food competition show families compete winners dish served walt disney world\n",
            "571 1.0 2.0\n",
            "nse introduced colocation server premises 2009 opg subscribed facility according sat order use secondary server permitted case failure primary server nse already issued directions sat order said conducted forensic investigation deliotte touche tohmatsu llp forensic report brought adverse findings opg following exchange passed order september 2017\n",
            "583 3.0 1.0\n",
            "earnings call analysts declaring results late tuesday cook said company made adjustments india seen preliminary better results think india important market longterm challenging market shortterm learning lot started manufacturing important able serve market reasonable way growing capability apple ceo informed\n",
            "626 1.0 3.0\n",
            "brand built time point view issues important bow trolling important brands sensitive peoples emotions wrong understanding cultural nuances doesnt mean brands play safe stand convictions said dabas pointed red label kumbh accompanying tweet highlighted people abandon elderly parents kumbh perhaps went wrong correctly representing holy festival stands surf excel campaign shining example brands take exemplary stances stick nikekaepernick episode shows even polarised society stick brand conviction likely win brand loyalty respect stronger support consumers added reference hul standing holi\n",
            "663 0.0 2.0\n",
            "ratings provide idea degree violence game pegi age rating system classifies games five categories3 every number reflective age group suit\n",
            "728 1.0 2.0\n",
            "much common crypto exchanges employ internal trading desks buy sell market traditional financial world new york attorney general report crypto exchanges released last year listed several markets bitfinex bitflyer poloniex reported allowed practices exchanges thats dirty little secret crypto exchange world said richard johnson analyst greenwich associates specializes blockchain technology referring internal trading desks story published wire agency feed without modifications text headline changed\n",
            "729 3.0 1.0\n",
            "photo bengalurus electronic city citys residents may techsavvy vocal social media far armchair activists jithendra mmint bengalurus residents may techsavvy vocal social media far armchair activists\n",
            "752 0.0 1.0\n",
            "court observed inappropriate content including pornography platform easily accessible children also observed app encouraged children play pranks people around creating issues regarding violation privacy\n",
            "759 1.0 2.0\n",
            "news termination amarons agreement johnson controls peeved investors naveen kumar sainimint analysts concerned whether amara rajajohnson split lead entry another strategic buyer without open offer also questions whether amara raja seize opportunity presented lithiumion batteries exide done\n",
            "779 3.0 2.0\n",
            "none citing uschina tensions reason hard ignore geopolitical backdrop even 90day ceasefire tariffs set expire march trump struck conciliatory tone sunday american trade negotiators prepared meet counterparts beijing china imposed import taxes hundreds billions dollars others goods trump threatened come including tariffs could make devices like iphones expensive also exploring new export restrictions would target industries china hoping get ahead artificial intelligence robotics justice department brought charges last month two chinese citizens accused stealing american trade secrets sensitive information behalf beijings main intelligence agency course plenty chinese entrepreneurs still eager show innovations hosts 52yearold trade event sought downplay tensions noting theyve weathered previous trade tensions roiled usjapan relations 1980s japanese presence used big similar panic said gary shapiro ceo consumer technology association organizes ces ended japanese bubble economy burst 1991 tech industry began long slow decline japanese innovation companies shrunk companies consolidated shapiro said things impermanent dont lose much sleep\n",
            "785 1.0 3.0\n",
            "lok sabha elections tamil nadu first ever demise political stalwarts jayalalithaa aiadmk karunanidhi dmk\n",
            "795 2.0 0.0\n",
            "production passenger vehicles including alto swift dzire vitara brezza declined 147550 units compared 161116 units february 2018\n",
            "826 3.0 1.0\n",
            "story published wire agency feed without modifications text headline changed\n",
            "881 0.0 3.0\n",
            "new delhi schedule next big project chhapaak along deepika padukone sat chat vikrant massey actor talked recently released hotstar series criminal justice impact roles mental emotional health costar pankaj tripathi watch bbcs criminal justice night signing criminal justice board watched referencei seen criminal justice wonderful coincidence remember sitting couple writer friends years ago discussing night one shows made india however signed criminal justice wanted fresh perspective tend get influenced quickly especially audiovisual medium good recall value original criminal justice since official adaptation knew direction heading\n",
            "963 2.0 1.0\n",
            "chandigarh stateowned insurance behemoth lic requires extensive data solutions make operations digital process lakh transactions 60000 claims day said sarita garg cio lic speaking c4io 2018 conference chandigarh friday garg said insurance giant one first indian companies tap potential introducing data warehouse india consumer end lic allows policyholders operate portfolios mobile phones laptops pay premiums online\n",
            "1007 1.0 3.0\n",
            "report titled cloud next wave growth india said cloud spending 2018 estimated total spending indian iaas spending estimated billion 2018 forecast grow reach 2324 billion 2022 new delhi growing adoption big data analytics artificial intelligence internet things expected push cloud market india grow 3fold billion 2022 report nasscom said tuesday\n",
            "1043 1.0 3.0\n",
            "first omo purchase worth 12500 crore take place may date second announced later\n",
            "1090 3.0 1.0\n",
            "offtourist season hardly sell time year due elections state getting buyers said prabhakar barala artist covered intricate pattachitra tribal motifs baralas house canvas progress visual treat visitors ask explain patiently takes one art work even gives live demo skills drawing portrait lady minutes asked expectations demands creative set voters pat comes baralas reply give market show work world\n",
            "1091 0.0 2.0\n",
            "landing taking military helicopters often looks like gleaming halos sometimes even two three halos michael yon rings sparks one sees helicopter action caused tiny shards pyrophoric metal igniting spontaneously fly air koppetchells effect reminder wrenching tragedy war sometimes stumble touch beauty election campaign season dominated talk military operations might appropriate topic mathematics column like one ive pondering question dont want subject yet another electionrelated screed even turns mathematical want remind reality war conveniently used political gain kills people yet sometimes also produces great beauty column via phenomenon first read several years ago sure know something helicopters least much rotors top wars used ferry troops supplies probably pretty routine stuff photographer war zone doubt youd going way photograph helicopters yet theres least one war photographer know michael yon taken plenty images military helicopters action\n",
            "1122 1.0 2.0\n",
            "monsters work series inspired pixar hit monsters inc billy crystal john goodman return voices mike sulley diary female president comedy series 12yearold cubanamerican girl journey become president united states\n",
            "1160 1.0 2.0\n",
            "nuro raised billion investors including greylock partners gaorong capital participated series funding round story published wire agency feed without modifications text headline changed\n",
            "1218 1.0 3.0\n",
            "way father film star dharmendra attached atalji today come join modiji said sunny deol bjp likely field sunny deol either gurdaspur chandigarh ongoing lok sabha elections\n",
            "1266 0.0 2.0\n",
            "according official press release particles generate meteor shower composed nontoxic materials last years ale conducted several material tests simulations ensure safe multicolour meteor shower first meteor shower event expected take place spring 2020 japan vicinity hiroshima seto inland sea press release adds six million people area 200km diameter would able see visual spectacle ale also plans launch second final stage assemblyby summer\n",
            "1286 1.0 2.0\n",
            "feature first introduced 2018 however made available google home google available stories range tales blaze monster machines classic bedtime stories like sleeping beauty little red riding hood\n",
            "1292 1.0 2.0\n",
            "moneylosing uber one swarm techrelated companies gone public year considering alongside pinterest zoom video communications inc made trading debut month highprofile startups looking public include slack technologies inc postmates inc palantir technologies inc airbnb inc initial ipo filing uber revealed operating loss billion 2018 bringing total operating losses past three years billion san franciscobased company planning pitch possible investors global transportation platform people familiar matter said building supplying new demand everything scooters bicycles freight food delivery story published wire agency feed without modifications text headline changed\n",
            "1296 3.0 1.0\n",
            "level autonomous driving second highest levela top level vehicle able navigate roads without driver input conditions\n",
            "1300 1.0 0.0\n",
            "spending online advertising growing attempts manipulate advertisers frauds reports suggest billion stolen advertisers every year frauds report released analytics firm techarc tuesday claims india market size 163 billion accounted worldwide digital fraud 2018 online commerce india biggest breeding ground fraud contributed total fraud country followed leisure travel platforms account frauds according report 2019 digital fraud india projected increase\n",
            "1307 1.0 0.0\n",
            "lakh cases pending high courts currently functioning sanctioned strength difference opinion supreme court centre delaying finalisation memorandum procedure mop judicial appointments mop judicial appointments yet finalised revised mop make judicial appointments transparent prasad added\n",
            "1321 1.0 0.0\n",
            "modi know develop economy always marketing bad products marketing manager says chandrababu naidu\n",
            "1324 0.0 1.0\n",
            "keep hiring based resume continue paradigm everything\n",
            "1327 2.0 1.0\n",
            "facebook ceo mark zuckerberg facebook aims constant engagement eci understand ensure elections 2019 safe abuse misinformation platform nyt facebook planning operations centre delhi monitor content relating elections 2019 platform india coordinate facebooks offices menlo park dublin singapore keep tabs electionrelated fake news new delhi bid prevent spread fake news platform ahead elections 2019 facebook inc plans start operations centre delhi coordinate social media giants offices menlo park california dublin singapore 24x7 basis monitor election content said shivnath thukral director public policy india south asia facebook\n",
            "1330 0.0 1.0\n",
            "response tweet head instagram adam mosseri apologised confusion said supposed small test went broad accident\n",
            "1359 1.0 2.0\n",
            "new delhi video clip shot sparse rooftop looks like lowrise apartment block shows young indian man swaying lipsyncing song praising prime minister narendra modi describing proud indian online identity garrytomar wearing earstuds shows beaded necklace partly unbuttoned shirt 15second clip modi singlehandedly trounced everyone modi storm know goes hindi song posted chinese video mobile application tiktok latest digital platform grip indias small towns villages ahead general election due may created beijing bytedance technology one worlds valuable startups potentially worth billion tiktok allows users create share short videos various special effects becoming hugely popular rural india home countrys billion people social media platforms facebook unit whatsapp twitter extensively used indian politicians campaigning ahead election facebooks 300 million users whatsapps 200 million made india largest market world twitter millions users tiktok fast catching downloaded 240 million times india far according app analytics firm sensor tower million users india installed last month times january 2018\n",
            "1388 0.0 1.0\n",
            "never able understand people supported much narendra modi said interview bollywood actor akshay kumar new delhi bollywoods love affair prime minister narendra modi got tiny bit tender days election commission banned screening biopic arguing could skew playing field modi sprang bollywoodlaced strategic surprise nation came form 60minutelong interview bollywood actor akshay kumar designed showcase softer personal side prime ministers personality modi called apolitical video released media news agency ani modi said never dreamt occupying top job isnt disciplinarian made\n",
            "1427 0.0 2.0\n",
            "mohan babu considered political heavyweight known close ntr even acted telugu movies past\n",
            "1434 0.0 2.0\n",
            "people names voter list also known electoral roll could cast vote says voters also find information polling booths contesting candidates election dates timings identity cards electronic voting machines evm voting 543 lok sabha seats would held seven phases april april april april may may may counting would take place may ninetyone lok sabha constituencies spread across states two union territories voting first phase thursday andhra pradesh arunachal pradesh meghalaya uttarakhand mizoram nagaland sikkim lakshadweep andaman nicobar islands telangana vote single phase\n",
            "1444 1.0 0.0\n",
            "game industrys business model creating hardware platform sonys playstation nintendos switch charging publishers right access come pressure recent years thats happened many casual gamers turn freetoplay mobile titles doubt service makes life even difficult established platforms amir anvarzadeh market strategist asymmetric advisors pte said note clients google help fragment gaming market already coming pressure big games adopted mobile gaming business model giving titles away free hope generating ingame content sales japanese companies responded creating subscription services offering content games googles push 180 billion industry threatens longstanding hardware model search giant however provide smooth lagfree experience must also convince publishers bring marquee titles\n",
            "1447 3.0 1.0\n",
            "54\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkgmpbZaDdcD",
        "outputId": "d99a7e29-727e-4d6a-cc1c-8bd129e0c008"
      },
      "source": [
        "#from sklearn.preprocessing import\n",
        "new_val_df=pd.DataFrame({'STORY':new_val_lst})\n",
        "\n",
        "#Topics=['Politics','Business','Entertainment','Technology']\n",
        "\n",
        "import gensim\n",
        "dictionary = gensim.corpora.Dictionary(new_val_df['STORY'].apply(lambda x:x.split()))\n",
        "\n",
        "bow_corpus = [dictionary.doc2bow(doc) for doc in new_val_df['STORY'].apply(lambda x:x.split())]\n",
        "\n",
        "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
        "                                   num_topics = 4, \n",
        "                                   id2word = dictionary,                                    \n",
        "                                   passes = 10,\n",
        "                                   workers = 2)\n",
        "val_key_words=[]\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
        "    val_key_words.append(re.sub('[^A-Za-z]+', ' ', topic).strip())\n",
        "    print(\"\\n\")\n",
        "\n",
        "val_key_words_vocab=[]\n",
        "for i in val_key_words:\n",
        "  val_key_words_vocab.extend(i.split())\n",
        "val_key_words_vocab=list(set(val_key_words_vocab))\n",
        "print(val_key_words_vocab)\n",
        "print(len(val_key_words_vocab))                          "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic: 0 \n",
            "Words: 0.006*\"corporation\" + 0.006*\"political\" + 0.006*\"available\" + 0.005*\"make\" + 0.005*\"users\" + 0.005*\"part\" + 0.005*\"indian\" + 0.005*\"one\" + 0.005*\"know\" + 0.005*\"war\"\n",
            "\n",
            "\n",
            "Topic: 1 \n",
            "Words: 0.006*\"inc\" + 0.006*\"brand\" + 0.005*\"agency\" + 0.005*\"india\" + 0.005*\"modifications\" + 0.005*\"fee\" + 0.005*\"story\" + 0.005*\"headline\" + 0.005*\"change\" + 0.005*\"without\"\n",
            "\n",
            "\n",
            "Topic: 2 \n",
            "Words: 0.007*\"vote\" + 0.007*\"election\" + 0.007*\"deol\" + 0.006*\"poll\" + 0.006*\"bengaluru\" + 0.006*\"technology\" + 0.005*\"may\" + 0.005*\"april\" + 0.005*\"exchange\" + 0.005*\"lok\"\n",
            "\n",
            "\n",
            "Topic: 3 \n",
            "Words: 0.006*\"come\" + 0.006*\"order\" + 0.005*\"make\" + 0.005*\"market\" + 0.005*\"game\" + 0.005*\"new\" + 0.005*\"modi\" + 0.004*\"instagram\" + 0.004*\"bharat\" + 0.004*\"billion\"\n",
            "\n",
            "\n",
            "['indian', 'bharat', 'available', 'headline', 'without', 'part', 'agency', 'vote', 'war', 'story', 'deol', 'brand', 'fee', 'know', 'election', 'users', 'billion', 'exchange', 'order', 'modifications', 'one', 'inc', 'april', 'technology', 'come', 'make', 'instagram', 'new', 'lok', 'bengaluru', 'modi', 'india', 'poll', 'market', 'may', 'change', 'corporation', 'game', 'political']\n",
            "39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6dGoMJZkz_f",
        "outputId": "1e108354-0336-4131-e82f-37351d3d5fda"
      },
      "source": [
        "xgb_clf.fit(X_drop,y)\n",
        "\n",
        "y_pred=xgb_clf.predict(X_drop)\n",
        "y_test_pred=xgb_clf.predict(X_test)\n",
        "print('Train accuracy_score:', accuracy_score(y, y_pred))\n",
        "\n",
        "y_test_pred=[int(i) for i in y_test_pred]\n",
        "Submission_df=pd.DataFrame({'SECTION':y_test_pred})\n",
        "Submission_df.to_csv('Submission_18.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train accuracy_score: 0.9996027016289233\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNosLlcjfmd_"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "y_train_pred=xgb_clf.predict(x_train)\n",
        "y_val_pred=xgb_clf.predict(x_val)\n",
        "print('Train accuracy_score:', accuracy_score(y_train, y_train_pred))\n",
        "print('Val accuracy_score:', accuracy_score(y_val, y_val_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naaNkU8LfDSP",
        "outputId": "6f76ef14-85f6-43d8-8ae8-b1af6ce50590"
      },
      "source": [
        "from sklearn.metrics import accuracy_score,auc\n",
        "y_pred=xgb_clf.predict(X_drop)\n",
        "y_proba_pred=xgb_clf.predict_proba(X_drop)\n",
        "y_test_pred=xgb_clf.predict(X_test)\n",
        "print('accuracy_score:', accuracy_score(y, y_pred))\n",
        "#print('AUC_score:', auc(y, y_proba_pred[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy_score: 0.9996027016289233\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ur2B5W-1XU56"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4_N6QllXLLS"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc=StandardScaler()\n",
        "X_pre=sc.fit_transform(X_drop)\n",
        "\n",
        "sc_2=StandardScaler()\n",
        "x_train_pre=sc_2.fit_transform(x_train)\n",
        "x_val_pre=sc_2.transform(x_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mf1WYskSjapR",
        "outputId": "3be04310-c46d-4c1b-a797-00b08e89ec83"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "embeddings_index = {}\n",
        "with open('/content/glove.6B.300d.txt') as f:\n",
        "  for line in f:\n",
        "    word, coefs = line.split(maxsplit=1)\n",
        "    coefs = np.fromstring(coefs, 'f', sep=' ')\n",
        "    embeddings_index[word] = coefs\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7LBULmmjvyy",
        "outputId": "99d3a2d0-a0ef-4431-cf87-1a63057f202b"
      },
      "source": [
        "#Tokenizer for STORY\n",
        "tokenizer_STORY = Tokenizer(num_words=10000,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n') #including \"_\" in tokenizer\n",
        "tokenizer_STORY.fit_on_texts(x_train1['STORY'])\n",
        "\n",
        "sequences_train_STORY = tokenizer_STORY.texts_to_sequences(x_train1['STORY'])\n",
        "sequences_val_STORY = tokenizer_STORY.texts_to_sequences(x_val1['STORY'])\n",
        "word_index_STORY = tokenizer_STORY.word_index #unique words\n",
        "print('Found %s unique tokens.' % len(word_index_STORY))\n",
        "#Padding\n",
        "MAX_SEQUENCE_LENGTH_STORY=max([len(x) for x in sequences_train_STORY])\n",
        "print(\"MAX_SEQUENCE_LENGTH_STORY is: \",MAX_SEQUENCE_LENGTH_STORY)\n",
        "\n",
        "x_train_STORY = pad_sequences(sequences_train_STORY, maxlen=MAX_SEQUENCE_LENGTH_STORY)\n",
        "x_val_STORY = pad_sequences(sequences_val_STORY, maxlen=MAX_SEQUENCE_LENGTH_STORY)\n",
        "\n",
        "y_train_oh = to_categorical(np.asarray(y_train))\n",
        "y_val_oh = to_categorical(np.asarray(y_val))\n",
        "\n",
        "print('Shape of x_train_essay :', x_train_STORY.shape)\n",
        "print('Shape of x_test_essay :', x_val_STORY.shape)\n",
        "#print('Shape of y_train_essay :', y_train_essay.shape)\n",
        "#print('Shape of y_test_essay :', y_test_essay.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 34839 unique tokens.\n",
            "MAX_SEQUENCE_LENGTH_STORY is:  474\n",
            "Shape of x_train_essay : (6040, 474)\n",
            "Shape of x_test_essay : (1511, 474)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a_YlhCVmEoa",
        "outputId": "b8ae1298-4ce0-4e74-d7f2-7a212bcd56eb"
      },
      "source": [
        "#embedding matrix\n",
        "num_words_STORY = min(10000, len(word_index_STORY) + 1)\n",
        "embedding_matrix = np.zeros((num_words_STORY, 300))\n",
        "for word, i in word_index_STORY.items():\n",
        "  if i >= 10000:\n",
        "    print(i,word)\n",
        "    continue\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    # words not found in embedding index will be all-zeros.\n",
        "    embedding_matrix[i] = embedding_vector\n",
        "print('embedding_matrix shape is :',embedding_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "14794 pcg\n",
            "14795 mahavir\n",
            "14796 jayanti\n",
            "14797 musicsmart\n",
            "14798 cameraamazon\n",
            "14799 plustechnology\n",
            "14800 sharekhan\n",
            "14801 accuse\n",
            "14802 intrigue\n",
            "14803 fended\n",
            "14804 suspicion\n",
            "14805 upperclass\n",
            "14806 7381\n",
            "14807 862\n",
            "14808 republicans\n",
            "14809 wicker\n",
            "14810 kansas\n",
            "14811 bernstein\n",
            "14812 iea\n",
            "14813 onshore\n",
            "14814 hug\n",
            "14815 chants\n",
            "14816 bhave\n",
            "14817 heroine\n",
            "14818 impromptu\n",
            "14819 permanence\n",
            "14820 7415\n",
            "14821 60740\n",
            "14822 pacemaker\n",
            "14823 purdue\n",
            "14824 scintillating\n",
            "14825 viswasam\n",
            "14826 thanksgiving\n",
            "14827 621inch\n",
            "14828 olufsen\n",
            "14829 multispeaker\n",
            "14830 autumn\n",
            "14831 nellis\n",
            "14832 1171225\n",
            "14833 nfhs\n",
            "14834 ghar\n",
            "14835 nongovernment\n",
            "14836 ceew\n",
            "14837 hypothetical\n",
            "14838 mygate\n",
            "14839 serviced\n",
            "14840 cereals\n",
            "14841 droughts\n",
            "14842 aghadi\n",
            "14843 containers\n",
            "14844 repos\n",
            "14845 admitting\n",
            "14846 bbrated\n",
            "14847 rightfully\n",
            "14848 sanguine\n",
            "14849 estimating\n",
            "14850 dubaibased\n",
            "14851 arqaam\n",
            "14852 neighbors\n",
            "14853 qatar\n",
            "14854 upsidedown\n",
            "14855 dharam\n",
            "14856 microfinance\n",
            "14857 dangling\n",
            "14858 temples\n",
            "14859 novice\n",
            "14860 ordeal\n",
            "14861 empires\n",
            "14862 manishi\n",
            "14863 leadup\n",
            "14864 narayanan\n",
            "14865 peripheral\n",
            "14866 dattamint\n",
            "14867 onepoint\n",
            "14868 upbringing\n",
            "14869 sealing\n",
            "14870 mueller\n",
            "14871 defection\n",
            "14872 kundgol\n",
            "14873 shivalli\n",
            "14874 fortunately\n",
            "14875 inline\n",
            "14876 6903\n",
            "14877 hardman\n",
            "14878 kakaotalk\n",
            "14879 kanpurbased\n",
            "14880 christ\n",
            "14881 anomalies\n",
            "14882 vigil\n",
            "14883 cisf\n",
            "14884 inspections\n",
            "14885 uncontrolled\n",
            "14886 thrill\n",
            "14887 advancements\n",
            "14888 lin\n",
            "14889 stuttered\n",
            "14890 transitioning\n",
            "14891 wadhawan\n",
            "14892 unconditional\n",
            "14893 plausible\n",
            "14894 discreetly\n",
            "14895 injustice\n",
            "14896 anyay\n",
            "14897 pessimistic\n",
            "14898 illustrate\n",
            "14899 incidental\n",
            "14900 divisiveness\n",
            "14901 arises\n",
            "14902 watchmen\n",
            "14903 contentious\n",
            "14904 afghanistan\n",
            "14905 selfconscious\n",
            "14906 cringe\n",
            "14907 cultivated\n",
            "14908 vine\n",
            "14909 behaviors\n",
            "14910 brutal\n",
            "14911 cynical\n",
            "14912 inference\n",
            "14913 bolted\n",
            "14914 bhola\n",
            "14915 activation\n",
            "14916 bilingual\n",
            "14917 039\n",
            "14918 dealbreaker\n",
            "14919 webos\n",
            "14920 frustrating\n",
            "14921 asim\n",
            "14922 687\n",
            "14923 specials\n",
            "14924 olivier\n",
            "14925 terminal\n",
            "14926 bosses\n",
            "14927 inflection\n",
            "14928 preemptive\n",
            "14929 preparedness\n",
            "14930 transitioned\n",
            "14931 erp\n",
            "14932 elevate\n",
            "14933 tinkering\n",
            "14934 elses\n",
            "14935 58487\n",
            "14936 blatant\n",
            "14937 alexey\n",
            "14938 cristiano\n",
            "14939 cryptomining\n",
            "14940 cryptominers\n",
            "14941 funtouch\n",
            "14942 4030\n",
            "14943 sanon\n",
            "14944 integrates\n",
            "14945 45g\n",
            "14946 14gbps\n",
            "14947 1733mbpsboth\n",
            "14948 graduation\n",
            "14949 imagery\n",
            "14950 datarich\n",
            "14951 neda\n",
            "14952 palpable\n",
            "14953 hyped\n",
            "14954 rookie\n",
            "14955 mhamarnath\n",
            "14956 parlance\n",
            "14957 instrumental\n",
            "14958 timon\n",
            "14959 pdaf\n",
            "14960 fieldofview\n",
            "14961 kapadias\n",
            "14962 brotherinlaw\n",
            "14963 cognisance\n",
            "14964 furnished\n",
            "14965 myths\n",
            "14966 spoilt\n",
            "14967 dhenkanal\n",
            "14968 disclosing\n",
            "14969 coincides\n",
            "14970 preparations\n",
            "14971 assembler\n",
            "14972 degraded\n",
            "14973 reverted\n",
            "14974 toppled\n",
            "14975 no1\n",
            "14976 nurse\n",
            "14977 kuruba\n",
            "14978 chamundeshwari\n",
            "14979 monetise\n",
            "14980 readies\n",
            "14981 vanessa\n",
            "14982 kirby\n",
            "14983 shouldve\n",
            "14984 overtheair\n",
            "14985 baffled\n",
            "14986 mouth\n",
            "14987 farms\n",
            "14988 unreal\n",
            "14989 hornberger\n",
            "14990 mazes\n",
            "14991 icebergs\n",
            "14992 thirdquarter\n",
            "14993 1203\n",
            "14994 q3fy19\n",
            "14995 50bps\n",
            "14996 193\n",
            "14997 ravipudi\n",
            "14998 complementary\n",
            "14999 capitalist\n",
            "15000 medicines\n",
            "15001 clone\n",
            "15002 330\n",
            "15003 multiply\n",
            "15004 generous\n",
            "15005 snag\n",
            "15006 tack\n",
            "15007 harshil\n",
            "15008 3230\n",
            "15009 paidup\n",
            "15010 truce\n",
            "15011 dartmouth\n",
            "15012 translations\n",
            "15013 believers\n",
            "15014 clouded\n",
            "15015 854\n",
            "15016 airfares\n",
            "15017 upswing\n",
            "15018 ling\n",
            "15019 instantaneous\n",
            "15020 mouni\n",
            "15021 bela\n",
            "15022 surbhi\n",
            "15023 shivangi\n",
            "15024 regulates\n",
            "15025 cracks\n",
            "15026 9820\n",
            "15027 footprints\n",
            "15028 saunter\n",
            "15029 hololens\n",
            "15030 akin\n",
            "15031 mixes\n",
            "15032 2295\n",
            "15033 trousseau\n",
            "15034 noninterest\n",
            "15035 kishan\n",
            "15036 gorakhpur\n",
            "15037 inserted\n",
            "15038 newsai\n",
            "15039 incapplemark\n",
            "15040 kroger\n",
            "15041 ambition\n",
            "15042 renners\n",
            "15043 backstory\n",
            "15044 bazaar\n",
            "15045 ruler\n",
            "15046 shireen\n",
            "15047 ulterior\n",
            "15048 cerebral\n",
            "15049 palsy\n",
            "15050 823\n",
            "15051 534\n",
            "15052 dole\n",
            "15053 flipkartredmi\n",
            "15054 pricexiaomi\n",
            "15055 occupancy\n",
            "15056 glean\n",
            "15057 helpful\n",
            "15058 tradeoffs\n",
            "15059 internetenabled\n",
            "15060 outriders\n",
            "15061 genocide\n",
            "15062 warranted\n",
            "15063 snooze\n",
            "15064 maamatimanush\n",
            "15065 neomiddle\n",
            "15066 frontier\n",
            "15067 innovators\n",
            "15068 rampup\n",
            "15069 notredame\n",
            "15070 24inch\n",
            "15071 keypad\n",
            "15072 4way\n",
            "15073 2jio\n",
            "15074 pricejiojio\n",
            "15075 phonejiophone\n",
            "15076 reviewjio\n",
            "15077 nowjio\n",
            "15078 recruited\n",
            "15079 messing\n",
            "15080 stimulation\n",
            "15081 freeze\n",
            "15082 addictive\n",
            "15083 prioritize\n",
            "15084 centrality\n",
            "15085 towering\n",
            "15086 lipsyncing\n",
            "15087 musicallys\n",
            "15088 defunct\n",
            "15089 cardio\n",
            "15090 cycling\n",
            "15091 beginners\n",
            "15092 milestones\n",
            "15093 ester\n",
            "15094 waits\n",
            "15095 anxiously\n",
            "15096 correspondent\n",
            "15097 bharatiraja\n",
            "15098 multilayered\n",
            "15099 doubtful\n",
            "15100 forgo\n",
            "15101 precarious\n",
            "15102 affection\n",
            "15103 ironed\n",
            "15104 tthe\n",
            "15105 wastetoenergy\n",
            "15106 13th\n",
            "15107 manufactures\n",
            "15108 306\n",
            "15109 worryingly\n",
            "15110 612\n",
            "15111 29999\n",
            "15112 smash\n",
            "15113 wasp\n",
            "15114 endgamemcfeely\n",
            "15115 19th\n",
            "15116 archery\n",
            "15117 clints\n",
            "15118 wakanda\n",
            "15119 freak\n",
            "15120 thrilling\n",
            "15121 artifice\n",
            "15122 defraud\n",
            "15123 conjuring\n",
            "15124 llorana\n",
            "15125 cardellini\n",
            "15126 jayneelynne\n",
            "15127 muslimdominated\n",
            "15128 selves\n",
            "15129 bonanza\n",
            "15130 ncsc\n",
            "15131 msmes\n",
            "15132 infoline\n",
            "15133 largerthanlife\n",
            "15134 hayley\n",
            "15135 planemaker\n",
            "15136 dodgy\n",
            "15137 9to5google\n",
            "15138 undeniably\n",
            "15139 raymond\n",
            "15140 brakes\n",
            "15141 cvs\n",
            "15142 meteors\n",
            "15143 hive\n",
            "15144 carlyle\n",
            "15145 054\n",
            "15146 angrezi\n",
            "15147 futureproof\n",
            "15148 sangeet\n",
            "15149 sacrificing\n",
            "15150 outings\n",
            "15151 recurring\n",
            "15152 winfuture\n",
            "15153 importers\n",
            "15154 mecklai\n",
            "15155 augurs\n",
            "15156 crosscurrents\n",
            "15157 interestrate\n",
            "15158 murdoch\n",
            "15159 videotaped\n",
            "15160 167\n",
            "15161 ficciey\n",
            "15162 duhan\n",
            "15163 insight\n",
            "15164 livestreams\n",
            "15165 lanky\n",
            "15166 sith\n",
            "15167 gainers\n",
            "15168 godman\n",
            "15169 chopras\n",
            "15170 enthuse\n",
            "15171 lucknowbased\n",
            "15172 prioritization\n",
            "15173 edr\n",
            "15174 choreographers\n",
            "15175 welloiled\n",
            "15176 satrap\n",
            "15177 plummeting\n",
            "15178 heeding\n",
            "15179 clad\n",
            "15180 bankchain\n",
            "15181 commonwealth\n",
            "15182 12mp13mp\n",
            "15183 criscross\n",
            "15184 sidelining\n",
            "15185 thrice\n",
            "15186 touchless\n",
            "15187 soli\n",
            "15188 aboard\n",
            "15189 capitalisation\n",
            "15190 tvos\n",
            "15191 apurva\n",
            "15192 numerics\n",
            "15193 eliminated\n",
            "15194 fourthlargest\n",
            "15195 cfo\n",
            "15196 replicate\n",
            "15197 costeffective\n",
            "15198 customisation\n",
            "15199 intelintel\n",
            "15200 processorsintel\n",
            "15201 pricetechnology\n",
            "15202 195\n",
            "15203 suicides\n",
            "15204 schwartz\n",
            "15205 sabarmati\n",
            "15206 sardar\n",
            "15207 dday\n",
            "15208 tenth\n",
            "15209 12x\n",
            "15210 43990\n",
            "15211 a8samsung\n",
            "15212 googleowned\n",
            "15213 aahana\n",
            "15214 kumra\n",
            "15215 pam\n",
            "15216 saraf\n",
            "15217 ladies\n",
            "15218 miserable\n",
            "15219 darna\n",
            "15220 infineon\n",
            "15221 exhibited\n",
            "15222 michau\n",
            "15223 jockeying\n",
            "15224 mahat\n",
            "15225 echoing\n",
            "15226 agarwala\n",
            "15227 definitive\n",
            "15228 teary\n",
            "15229 12year\n",
            "15230 tales\n",
            "15231 starkly\n",
            "15232 contrasting\n",
            "15233 strachan\n",
            "15234 shfe\n",
            "15235 82000\n",
            "15236 genrobotics\n",
            "15237 sewers\n",
            "15238 ifr\n",
            "15239 affirmed\n",
            "15240 casteism\n",
            "15241 angrily\n",
            "15242 snacks\n",
            "15243 hostel\n",
            "15244 kuldeep\n",
            "15245 assorted\n",
            "15246 nose\n",
            "15247 stingy\n",
            "15248 40w\n",
            "15249 acerbic\n",
            "15250 899\n",
            "15251 gym\n",
            "15252 lmeregistered\n",
            "15253 grain\n",
            "15254 6814\n",
            "15255 2019high\n",
            "15256 bhairav\n",
            "15257 shiromani\n",
            "15258 steadied\n",
            "15259 closelywatched\n",
            "15260 wager\n",
            "15261 stumped\n",
            "15262 twinkle\n",
            "15263 telekom\n",
            "15264 spatial\n",
            "15265 jagtap\n",
            "15266 2244x1080\n",
            "15267 kaufman\n",
            "15268 mophie\n",
            "15269 juice\n",
            "15270 georgia\n",
            "15271 privacyrelated\n",
            "15272 pitiable\n",
            "15273 sonipat\n",
            "15274 jawan\n",
            "15275 kisaan\n",
            "15276 largesse\n",
            "15277 retaliation\n",
            "15278 garb\n",
            "15279 consults\n",
            "15280 propagate\n",
            "15281 allocating\n",
            "15282 filings\n",
            "15283 shobhit\n",
            "15284 comscore\n",
            "15285 grouping\n",
            "15286 publishes\n",
            "15287 grille\n",
            "15288 spoons\n",
            "15289 duraimurugan\n",
            "15290 donor\n",
            "15291 pranab\n",
            "15292 masked\n",
            "15293 chhapaak\n",
            "15294 meghna\n",
            "15295 gulzar\n",
            "15296 laxmi\n",
            "15297 scavengers\n",
            "15298 eradicate\n",
            "15299 scavenging\n",
            "15300 septic\n",
            "15301 cantonment\n",
            "15302 chhattisgarhs\n",
            "15303 bytedances\n",
            "15304 neelamegam\n",
            "15305 waris\n",
            "15306 techlegis\n",
            "15307 intervening\n",
            "15308 exorbitant\n",
            "15309 patented\n",
            "15310 15minute\n",
            "15311 sari\n",
            "15312 aaj\n",
            "15313 knowlescarter\n",
            "15314 duffer\n",
            "15315 hovered\n",
            "15316 swiss\n",
            "15317 roiled\n",
            "15318 1258\n",
            "15319 raking\n",
            "15320 latino\n",
            "15321 louder\n",
            "15322 cranky\n",
            "15323 deliberating\n",
            "15324 worship\n",
            "15325 thali\n",
            "15326 twomonth\n",
            "15327 venezuelas\n",
            "15328 compounded\n",
            "15329 1145\n",
            "15330 updateshowever\n",
            "15331 aparajita\n",
            "15332 manson\n",
            "15333 presidentelect\n",
            "15334 perks\n",
            "15335 motorcades\n",
            "15336 taxis\n",
            "15337 superbikes\n",
            "15338 fiveday\n",
            "15339 disposable\n",
            "15340 commuting\n",
            "15341 restraint\n",
            "15342 bikes\n",
            "15343 tencents\n",
            "15344 valve\n",
            "15345 cheaters\n",
            "15346 presentday\n",
            "15347 recur\n",
            "15348 dictate\n",
            "15349 solidified\n",
            "15350 hinterlands\n",
            "15351 pramukh\n",
            "15352 designation\n",
            "15353 uncannily\n",
            "15354 arcade\n",
            "15355 tetris\n",
            "15356 akshat\n",
            "15357 monetized\n",
            "15358 bribed\n",
            "15359 ghayal\n",
            "15360 x7nokia\n",
            "15361 griffin\n",
            "15362 archrivals\n",
            "15363 senator\n",
            "15364 1256\n",
            "15365 075\n",
            "15366 093\n",
            "15367 ravaged\n",
            "15368 estates\n",
            "15369 breadwinner\n",
            "15370 7967\n",
            "15371 2270x1080\n",
            "15372 7283\n",
            "15373 7109\n",
            "15374 currie\n",
            "15375 headcount\n",
            "15376 hrithik\n",
            "15377 roshan\n",
            "15378 cruises\n",
            "15379 ethan\n",
            "15380 glittering\n",
            "15381 induced\n",
            "15382 67inch\n",
            "15383 hound\n",
            "15384 podrick\n",
            "15385 payne\n",
            "15386 dolorous\n",
            "15387 edd\n",
            "15388 callers\n",
            "15389 45inch\n",
            "15390 dindori\n",
            "15391 jatt\n",
            "15392 ludhiyane\n",
            "15393 budding\n",
            "15394 18990\n",
            "15395 33990\n",
            "15396 yorkbased\n",
            "15397 flourishing\n",
            "15398 jajmau\n",
            "15399 hasty\n",
            "15400 satyadev\n",
            "15401 pachauri\n",
            "15402 beamforming\n",
            "15403 909\n",
            "15404 trainer\n",
            "15405 berry\n",
            "15406 sls\n",
            "15407 titanium\n",
            "15408 abs\n",
            "15409 interiors\n",
            "15410 sekhar\n",
            "15411 porika\n",
            "15412 prabhakars\n",
            "15413 bandi\n",
            "15414 kunalkemmu\n",
            "15415 adityaroykapur\n",
            "15416 karanjohar\n",
            "15417 abhivarman\n",
            "15418 starcast\n",
            "15419 chikmagalur\n",
            "15420 6792\n",
            "15421 6869\n",
            "15422 favouring\n",
            "15423 kale\n",
            "15424 chandana\n",
            "15425 narayan\n",
            "15426 hides\n",
            "15427 bedtime\n",
            "15428 instinctively\n",
            "15429 longing\n",
            "15430 halves\n",
            "15431 participates\n",
            "15432 finland\n",
            "15433 jagged\n",
            "15434 updatesamsung\n",
            "15435 updategoogle\n",
            "15436 updaterealme\n",
            "15437 updatenokia\n",
            "15438 updateoneplus\n",
            "15439 updatemotorola\n",
            "15440 dashed\n",
            "15441 pedder\n",
            "15442 1094\n",
            "15443 ian\n",
            "15444 toyota\n",
            "15445 daimler\n",
            "15446 depreciating\n",
            "15447 loser\n",
            "15448 incharges\n",
            "15449 mohanlal\n",
            "15450 airconditioned\n",
            "15451 commuter\n",
            "15452 patnaiks\n",
            "15453 folklore\n",
            "15454 endeared\n",
            "15455 bhadrak\n",
            "15456 jagatsinghpur\n",
            "15457 compulsion\n",
            "15458 judice\n",
            "15459 criticize\n",
            "15460 jangipur\n",
            "15461 kokrajhar\n",
            "15462 rani\n",
            "15463 sevenphases\n",
            "15464 resuming\n",
            "15465 disposed\n",
            "15466 disconnected\n",
            "15467 sivakumar\n",
            "15468 echoes\n",
            "15469 sail\n",
            "15470 kotigobba\n",
            "15471 kaun\n",
            "15472 banega\n",
            "15473 crorepati\n",
            "15474 agar\n",
            "15475 hogi\n",
            "15476 pacman\n",
            "15477 tumultuous\n",
            "15478 ghazal\n",
            "15479 roam\n",
            "15480 diya\n",
            "15481 cutnokia\n",
            "15482 dougherty\n",
            "15483 monarch\n",
            "15484 godsized\n",
            "15485 threeheaded\n",
            "15486 delta\n",
            "15487 consequence\n",
            "15488 objection\n",
            "15489 spoiled\n",
            "15490 4200\n",
            "15491 stifled\n",
            "15492 silenced\n",
            "15493 inertia\n",
            "15494 diverting\n",
            "15495 216\n",
            "15496 mahadev\n",
            "15497 paksha\n",
            "15498 bhim\n",
            "15499 4590\n",
            "15500 yasuo\n",
            "15501 imanaka\n",
            "15502 rakuten\n",
            "15503 handpainted\n",
            "15504 intimidation\n",
            "15505 uncontested\n",
            "15506 whipping\n",
            "15507 shree\n",
            "15508 mythical\n",
            "15509 monkey\n",
            "15510 blaring\n",
            "15511 wrath\n",
            "15512 lifts\n",
            "15513 namaste\n",
            "15514 appeasement\n",
            "15515 karmakar\n",
            "15516 coexist\n",
            "15517 vip\n",
            "15518 asteria\n",
            "15519 alison\n",
            "15520 533538\n",
            "15521 1107\n",
            "15522 legion\n",
            "15523 champions\n",
            "15524 deoras\n",
            "15525 blur\n",
            "15526 foreground\n",
            "15527 brandnew\n",
            "15528 landscapes\n",
            "15529 richly\n",
            "15530 dispel\n",
            "15531 tanushree\n",
            "15532 myth\n",
            "15533 biggies\n",
            "15534 tashkent\n",
            "15535 braves\n",
            "15536 unaltered\n",
            "15537 83000\n",
            "15538 ding\n",
            "15539 dispatched\n",
            "15540 antennas\n",
            "15541 ping\n",
            "15542 mischief\n",
            "15543 memo\n",
            "15544 spalding\n",
            "15545 retire\n",
            "15546 oath\n",
            "15547 lahore\n",
            "15548 arab\n",
            "15549 acceptability\n",
            "15550 multilateral\n",
            "15551 unbiased\n",
            "15552 cag\n",
            "15553 sins\n",
            "15554 repose\n",
            "15555 201314\n",
            "15556 undercurrent\n",
            "15557 selfserving\n",
            "15558 allround\n",
            "15559 reactor\n",
            "15560 rainier\n",
            "15561 volcano\n",
            "15562 enjoyable\n",
            "15563 mahbubnagar\n",
            "15564 nonmuslims\n",
            "15565 roth\n",
            "15566 17000\n",
            "15567 subsidiaries\n",
            "15568 stoke\n",
            "15569 depicting\n",
            "15570 displacement\n",
            "15571 elevation\n",
            "15572 reside\n",
            "15573 lancer\n",
            "15574 heldtomaturity\n",
            "15575 shields\n",
            "15576 buckets\n",
            "15577 samsungcom\n",
            "15578 baaghi\n",
            "15579 woody\n",
            "15580 dylan\n",
            "15581 allens\n",
            "15582 mia\n",
            "15583 farrow\n",
            "15584 gimbal\n",
            "15585 5360\n",
            "15586 rejuvenate\n",
            "15587 12490\n",
            "15588 ax0\n",
            "15589 7884\n",
            "15590 devising\n",
            "15591 thirdly\n",
            "15592 everybodys\n",
            "15593 cohesion\n",
            "15594 tukdetukde\n",
            "15595 oldfashioned\n",
            "15596 49inch\n",
            "15597 licensed\n",
            "15598 riddled\n",
            "15599 18999\n",
            "15600 24999\n",
            "15601 49999\n",
            "15602 straggly\n",
            "15603 paunch\n",
            "15604 thanoss\n",
            "15605 wallowing\n",
            "15606 coder\n",
            "15607 troughtonsmith\n",
            "15608 consciously\n",
            "15609 sculptor\n",
            "15610 645pm\n",
            "15611 raghurajpur\n",
            "15612 halfadozen\n",
            "15613 pattachitra\n",
            "15614 paintings\n",
            "15615 classical\n",
            "15616 mythology\n",
            "15617 jagannath\n",
            "15618 dealers\n",
            "15619 oblige\n",
            "15620 52000\n",
            "15621 bathrooms\n",
            "15622 splits\n",
            "15623 12hour\n",
            "15624 1800\n",
            "15625 towels\n",
            "15626 detergent\n",
            "15627 3700mah\n",
            "15628 kartik\n",
            "15629 zindagi\n",
            "15630 noteworthy\n",
            "15631 vinu\n",
            "15632 sreenivasan\n",
            "15633 dileep\n",
            "15634 happily\n",
            "15635 zach\n",
            "15636 acche\n",
            "15637 amrit\n",
            "15638 bottomline\n",
            "15639 aga\n",
            "15640 mohsin\n",
            "15641 irfan\n",
            "15642 polished\n",
            "15643 326\n",
            "15644 158\n",
            "15645 daunting\n",
            "15646 andy\n",
            "15647 alabama\n",
            "15648 tamannaah\n",
            "15649 petitions\n",
            "15650 threehour\n",
            "15651 grief\n",
            "15652 entrust\n",
            "15653 screenwriters\n",
            "15654 bucky\n",
            "15655 punching\n",
            "15656 lang\n",
            "15657 72hour\n",
            "15658 occurs\n",
            "15659 eagerness\n",
            "15660 mp4\n",
            "15661 avi\n",
            "15662 disguised\n",
            "15663 emotive\n",
            "15664 cleanliness\n",
            "15665 salesman\n",
            "15666 rishikesh\n",
            "15667 rivers\n",
            "15668 ujala\n",
            "15669 pauri\n",
            "15670 haridwar\n",
            "15671 434\n",
            "15672 885\n",
            "15673 eponymous\n",
            "15674 endorse\n",
            "15675 38100\n",
            "15676 232\n",
            "15677 maid\n",
            "15678 amaravathi\n",
            "15679 unarguably\n",
            "15680 promotes\n",
            "15681 pundits\n",
            "15682 supplying\n",
            "15683 485\n",
            "15684 cave\n",
            "15685 nautical\n",
            "15686 karachi\n",
            "15687 480\n",
            "15688 jails\n",
            "15689 1150\n",
            "15690 boatsabout\n",
            "15691 regionhave\n",
            "15692 confiscated\n",
            "15693 abysmally\n",
            "15694 6409\n",
            "15695 nilesh\n",
            "15696 assuming\n",
            "15697 aggregation\n",
            "15698 123\n",
            "15699 voteshare\n",
            "15700 velocity\n",
            "15701 galileo\n",
            "15702 formalities\n",
            "15703 noble\n",
            "15704 bard\n",
            "15705 taapsee\n",
            "15706 npas\n",
            "15707 indiaamazon\n",
            "15708 drawings\n",
            "15709 prejudices\n",
            "15710 rajagopalan\n",
            "15711 tuticorin\n",
            "15712 geography\n",
            "15713 annadurai\n",
            "15714 chanted\n",
            "15715 dayanidhi\n",
            "15716 strip\n",
            "15717 reminisce\n",
            "15718 quasimodo\n",
            "15719 bait\n",
            "15720 20934\n",
            "15721 5gb\n",
            "15722 50gb\n",
            "15723 mfine\n",
            "15724 medication\n",
            "15725 consult\n",
            "15726 diagnosis\n",
            "15727 underperform\n",
            "15728 retest\n",
            "15729 reversing\n",
            "15730 complications\n",
            "15731 ekg\n",
            "15732 sarcasm\n",
            "15733 wit\n",
            "15734 marathon\n",
            "15735 humanitys\n",
            "15736 achche\n",
            "15737 garners\n",
            "15738 myers\n",
            "15739 shalini\n",
            "15740 langer\n",
            "15741 laboured\n",
            "15742 sendoff\n",
            "15743 misbehaviour\n",
            "15744 threeeyedraven\n",
            "15745 dreamt\n",
            "15746 34000\n",
            "15747 castaldi\n",
            "15748 heaven\n",
            "15749 stocked\n",
            "15750 wiring\n",
            "15751 poked\n",
            "15752 variantswhite\n",
            "15753 mulberry\n",
            "15754 goalbased\n",
            "15755 launchnokia\n",
            "15756 displaynokia\n",
            "15757 cameranokia\n",
            "15758 featuresnokia\n",
            "15759 datenokia\n",
            "15760 polymer\n",
            "15761 teases\n",
            "15762 arundhati\n",
            "15763 etawah\n",
            "15764 usp\n",
            "15765 foremost\n",
            "15766 samir\n",
            "15767 exempted\n",
            "15768 exemptions\n",
            "15769 fy1921\n",
            "15770 1400\n",
            "15771 2800\n",
            "15772 switchgears\n",
            "15773 scaleup\n",
            "15774 yseries\n",
            "15775 irreparable\n",
            "15776 suppress\n",
            "15777 punishment\n",
            "15778 audis\n",
            "15779 heavyduty\n",
            "15780 fret\n",
            "15781 excluded\n",
            "15782 rub\n",
            "15783 idioms\n",
            "15784 translators\n",
            "15785 carriage\n",
            "15786 riches\n",
            "15787 siren\n",
            "15788 bases\n",
            "15789 nizamuddin\n",
            "15790 karyakartas\n",
            "15791 inciting\n",
            "15792 skyrocketed\n",
            "15793 koul\n",
            "15794 cyclicals\n",
            "15795 oversold\n",
            "15796 favor\n",
            "15797 compliment\n",
            "15798 konnur\n",
            "15799 transitions\n",
            "15800 delicate\n",
            "15801 entrusted\n",
            "15802 deprived\n",
            "15803 petrochemicals\n",
            "15804 overtightening\n",
            "15805 heirs\n",
            "15806 departed\n",
            "15807 lux\n",
            "15808 daylight\n",
            "15809 starbucks\n",
            "15810 clamped\n",
            "15811 madam\n",
            "15812 nab\n",
            "15813 kapadia\n",
            "15814 garu\n",
            "15815 poppins\n",
            "15816 dynastic\n",
            "15817 gosling\n",
            "15818 shareeka\n",
            "15819 breathe\n",
            "15820 unstable\n",
            "15821 yahoo\n",
            "15822 behzad\n",
            "15823 khambata\n",
            "15824 pnotes\n",
            "15825 drains\n",
            "15826 throttle\n",
            "15827 nlg\n",
            "15828 2012s\n",
            "15829 organs\n",
            "15830 6184\n",
            "15831 tweetthreads\n",
            "15832 onlinefocused\n",
            "15833 384\n",
            "15834 kharif\n",
            "15835 shame\n",
            "15836 ballots\n",
            "15837 margdarshak\n",
            "15838 6525\n",
            "15839 0713\n",
            "15840 5594\n",
            "15841 yearlong\n",
            "15842 pledges\n",
            "15843 substantive\n",
            "15844 frightening\n",
            "15845 soninlaw\n",
            "15846 1952\n",
            "15847 hauntings\n",
            "15848 nora\n",
            "15849 infinix\n",
            "15850 faceunlock\n",
            "15851 chinabased\n",
            "15852 agnihotris\n",
            "15853 barinholtz\n",
            "15854 reimbursed\n",
            "15855 boutrous\n",
            "15856 overestimate\n",
            "15857 lull\n",
            "15858 skyrocketing\n",
            "15859 diminutive\n",
            "15860 hesitate\n",
            "15861 warwick\n",
            "15862 3100mah\n",
            "15863 adventures\n",
            "15864 badass\n",
            "15865 vigilantes\n",
            "15866 animation\n",
            "15867 backupwhatsapp\n",
            "15868 potter\n",
            "15869 deloitte\n",
            "15870 bewildering\n",
            "15871 downright\n",
            "15872 chacko\n",
            "15873 4apple\n",
            "15874 priceecgecg\n",
            "15875 appecg\n",
            "15876 693\n",
            "15877 corroborated\n",
            "15878 krebsonsecurity\n",
            "15879 hotmail\n",
            "15880 msn\n",
            "15881 revolting\n",
            "15882 tanwar\n",
            "15883 princes\n",
            "15884 intolerance\n",
            "15885 dhanorkar\n",
            "15886 vinayak\n",
            "15887 ninth\n",
            "15888 threemember\n",
            "15889 interfering\n",
            "15890 nonpremium\n",
            "15891 lengthening\n",
            "15892 scanners\n",
            "15893 gillan\n",
            "15894 sadak\n",
            "15895 tinker\n",
            "15896 transpired\n",
            "15897 lastminute\n",
            "15898 inexplicable\n",
            "15899 pala\n",
            "15900 overtly\n",
            "15901 chakma\n",
            "15902 61yearold\n",
            "15903 katwal\n",
            "15904 kiren\n",
            "15905 jarjum\n",
            "15906 ete\n",
            "15907 aver\n",
            "15908 1921\n",
            "15909 seveneight\n",
            "15910 mustafa\n",
            "15911 rampage\n",
            "15912 sixweek\n",
            "15913 ninemonth\n",
            "15914 132630\n",
            "15915 featuresrealme\n",
            "15916 raked\n",
            "15917 1415\n",
            "15918 finalist\n",
            "15919 medley\n",
            "15920 milao\n",
            "15921 crooning\n",
            "15922 thoogudeepa\n",
            "15923 yashwho\n",
            "15924 sumalathahave\n",
            "15925 disassemble\n",
            "15926 discovering\n",
            "15927 disassembled\n",
            "15928 allinone\n",
            "15929 917\n",
            "15930 unparalleled\n",
            "15931 underline\n",
            "15932 pasted\n",
            "15933 deadlines\n",
            "15934 evaluation\n",
            "15935 gradual\n",
            "15936 4th\n",
            "15937 midnovember\n",
            "15938 fundstrat\n",
            "15939 merges\n",
            "15940 buffs\n",
            "15941 13megapixel\n",
            "15942 megabudget\n",
            "15943 brouhaha\n",
            "15944 kchandrashekar\n",
            "15945 spelling\n",
            "15946 swathes\n",
            "15947 grains\n",
            "15948 apj\n",
            "15949 uniformity\n",
            "15950 arabic\n",
            "15951 doubting\n",
            "15952 adopts\n",
            "15953 gsuite\n",
            "15954 rockford\n",
            "15955 incapable\n",
            "15956 bhatlekarmint\n",
            "15957 nonregistered\n",
            "15958 sampark\n",
            "15959 nambikkaitrust\n",
            "15960 palmist\n",
            "15961 israelite\n",
            "15962 6910\n",
            "15963 domestically\n",
            "15964 odins\n",
            "15965 lokis\n",
            "15966 malekith\n",
            "15967 elves\n",
            "15968 alignment\n",
            "15969 misfit\n",
            "15970 remonetising\n",
            "15971 disarray\n",
            "15972 yamaji\n",
            "15973 usersinstagram\n",
            "15974 delaying\n",
            "15975 reshaping\n",
            "15976 corpses\n",
            "15977 hunter\n",
            "15978 pellets\n",
            "15979 390\n",
            "15980 effigies\n",
            "15981 toffee\n",
            "15982 15yearold\n",
            "15983 4998\n",
            "15984 mill\n",
            "15985 facilitator\n",
            "15986 solidarity\n",
            "15987 submerged\n",
            "15988 vanguard\n",
            "15989 lefts\n",
            "15990 aanand\n",
            "15991 26999\n",
            "15992 postpaid\n",
            "15993 120gb\n",
            "15994 499\n",
            "15995 circle\n",
            "15996 insulated\n",
            "15997 distraction\n",
            "15998 sphci9btpioneer\n",
            "15999 reviewcar\n",
            "16000 indiapioneer\n",
            "16001 syncdriver\n",
            "16002 distractioncar\n",
            "16003 usagecar\n",
            "16004 10000pioneer\n",
            "16005 vulture\n",
            "16006 tambor\n",
            "16007 rudin\n",
            "16008 shaft\n",
            "16009 peele\n",
            "16010 resolutions\n",
            "16011 crises\n",
            "16012 7mp\n",
            "16013 predictor\n",
            "16014 patralekhaa\n",
            "16015 1600\n",
            "16016 purchasers\n",
            "16017 lovebirds\n",
            "16018 kochi\n",
            "16019 gown\n",
            "16020 resin\n",
            "16021 chalked\n",
            "16022 975\n",
            "16023 noobjection\n",
            "16024 padalkar\n",
            "16025 rdp\n",
            "16026 assistenza\n",
            "16027 colette\n",
            "16028 naths\n",
            "16029 vinta\n",
            "16030 absent\n",
            "16031 sasikala\n",
            "16032 mercy\n",
            "16033 456\n",
            "16034 trumped\n",
            "16035 medal\n",
            "16036 shattered\n",
            "16037 elated\n",
            "16038 privacyprivacy\n",
            "16039 gdprdata\n",
            "16040 lawsright\n",
            "16041 fani\n",
            "16042 dismissal\n",
            "16043 petitioners\n",
            "16044 lied\n",
            "16045 gvl\n",
            "16046 jis\n",
            "16047 bone\n",
            "16048 10episode\n",
            "16049 mtv\n",
            "16050 backstage\n",
            "16051 harbour\n",
            "16052 illness\n",
            "16053 20s\n",
            "16054 maut\n",
            "16055 laga\n",
            "16056 bumbling\n",
            "16057 boxer\n",
            "16058 trivia\n",
            "16059 tires\n",
            "16060 khannas\n",
            "16061 chartbuster\n",
            "16062 koi\n",
            "16063 laila\n",
            "16064 cso\n",
            "16065 sparsely\n",
            "16066 lifeline\n",
            "16067 franell\n",
            "16068 exerted\n",
            "16069 prospered\n",
            "16070 mobilephone\n",
            "16071 ooh\n",
            "16072 container\n",
            "16073 baez\n",
            "16074 consensual\n",
            "16075 burman\n",
            "16076 tik\n",
            "16077 tok\n",
            "16078 enacting\n",
            "16079 statute\n",
            "16080 enacted\n",
            "16081 jajpur\n",
            "16082 county\n",
            "16083 trove\n",
            "16084 palo\n",
            "16085 applying\n",
            "16086 reinvent\n",
            "16087 saaho\n",
            "16088 diminishes\n",
            "16089 terminals\n",
            "16090 7034\n",
            "16091 dictation\n",
            "16092 rebrand\n",
            "16093 7091\n",
            "16094 7113\n",
            "16095 loads\n",
            "16096 rearcamera\n",
            "16097 shamsheer\n",
            "16098 violative\n",
            "16099 curating\n",
            "16100 sampling\n",
            "16101 tick\n",
            "16102 speculate\n",
            "16103 cabin\n",
            "16104 clearances\n",
            "16105 holes\n",
            "16106 mcleod\n",
            "16107 russel\n",
            "16108 dutyfree\n",
            "16109 indiakorea\n",
            "16110 selfuse\n",
            "16111 housestar\n",
            "16112 yotaphone\n",
            "16113 dualdisplay\n",
            "16114 ananthkumar\n",
            "16115 clinic\n",
            "16116 nodal\n",
            "16117 beamed\n",
            "16118 prohibits\n",
            "16119 cinematograph\n",
            "16120 apparatus\n",
            "16121 attendant\n",
            "16122 klein\n",
            "16123 melissa\n",
            "16124 southwest\n",
            "16125 yearthe\n",
            "16126 ambush\n",
            "16127 1879\n",
            "16128 varvatos\n",
            "16129 wardrobe\n",
            "16130 newsroom\n",
            "16131 unreleased\n",
            "16132 laserpowered\n",
            "16133 darker\n",
            "16134 eis\n",
            "16135 outrightly\n",
            "16136 bhayani\n",
            "16137 refer\n",
            "16138 curate\n",
            "16139 hires\n",
            "16140 immerses\n",
            "16141 saturated\n",
            "16142 wonky\n",
            "16143 noholdsbarred\n",
            "16144 oneupmanship\n",
            "16145 unites\n",
            "16146 zeitgeist\n",
            "16147 cacophony\n",
            "16148 grizzly\n",
            "16149 cynically\n",
            "16150 faultline\n",
            "16151 groupings\n",
            "16152 rabid\n",
            "16153 acrimony\n",
            "16154 flown\n",
            "16155 embedding\n",
            "16156 cve201711882\n",
            "16157 cve20180802\n",
            "16158 initiation\n",
            "16159 frozen\n",
            "16160 geetha\n",
            "16161 govindam\n",
            "16162 mandanna\n",
            "16163 citation\n",
            "16164 purse\n",
            "16165 ordersiphone\n",
            "16166 pricesiphone\n",
            "16167 kasargod\n",
            "16168 esc\n",
            "16169 escs\n",
            "16170 udham\n",
            "16171 1940s\n",
            "16172 assassinated\n",
            "16173 odwyer\n",
            "16174 jallianwala\n",
            "16175 bagh\n",
            "16176 semiannual\n",
            "16177 aether\n",
            "16178 holi\n",
            "16179 cans\n",
            "16180 peaking\n",
            "16181 jaikishan\n",
            "16182 parmar\n",
            "16183 recognises\n",
            "16184 vijayapura\n",
            "16185 phenomenal\n",
            "16186 7411\n",
            "16187 thumbs\n",
            "16188 crowe\n",
            "16189 watts\n",
            "16190 abovetherest\n",
            "16191 audrey\n",
            "16192 072\n",
            "16193 012\n",
            "16194 overhauled\n",
            "16195 complacent\n",
            "16196 complacency\n",
            "16197 cyberattackers\n",
            "16198 carrey\n",
            "16199 eternal\n",
            "16200 spotless\n",
            "16201 massachusetts\n",
            "16202 32450\n",
            "16203 gpay\n",
            "16204 wholetime\n",
            "16205 fraudulent\n",
            "16206 tbt\n",
            "16207 limaye\n",
            "16208 juggernaut\n",
            "16209 umbrella\n",
            "16210 veracity\n",
            "16211 tailor\n",
            "16212 kaling\n",
            "16213 liquidcrystal\n",
            "16214 stiffening\n",
            "16215 smashed\n",
            "16216 jawdropping\n",
            "16217 seize\n",
            "16218 humane\n",
            "16219 fifthgeneration\n",
            "16220 10minute\n",
            "16221 concealed\n",
            "16222 valuers\n",
            "16223 rightly\n",
            "16224 beric\n",
            "16225 dondarrion\n",
            "16226 resurrect\n",
            "16227 experiencing\n",
            "16228 127\n",
            "16229 bhanu\n",
            "16230 chikkaballapura\n",
            "16231 grass\n",
            "16232 sos\n",
            "16233 topple\n",
            "16234 jbl\n",
            "16235 late2017\n",
            "16236 retweet\n",
            "16237 antagonistic\n",
            "16238 rahuls\n",
            "16239 freeing\n",
            "16240 saleable\n",
            "16241 4931\n",
            "16242 tel\n",
            "16243 aviv\n",
            "16244 yuva\n",
            "16245 vahini\n",
            "16246 inflate\n",
            "16247 intellectuals\n",
            "16248 reckless\n",
            "16249 liberties\n",
            "16250 patriots\n",
            "16251 politicizing\n",
            "16252 yanked\n",
            "16253 qualms\n",
            "16254 retd\n",
            "16255 worstcase\n",
            "16256 compounding\n",
            "16257 hangs\n",
            "16258 fourthquarter\n",
            "16259 contrasted\n",
            "16260 astounding\n",
            "16261 13asus\n",
            "16262 dependant\n",
            "16263 opportunist\n",
            "16264 cofounders\n",
            "16265 systrom\n",
            "16266 krieger\n",
            "16267 sxsw\n",
            "16268 zuckerbergled\n",
            "16269 silchar\n",
            "16270 brahmaputra\n",
            "16271 accelerates\n",
            "16272 unfollowed\n",
            "16273 265\n",
            "16274 49400\n",
            "16275 736786\n",
            "16276 traces\n",
            "16277 interestfree\n",
            "16278 reiterating\n",
            "16279 panorama\n",
            "16280 brookfield\n",
            "16281 recalling\n",
            "16282 hacks\n",
            "16283 evaluates\n",
            "16284 liking\n",
            "16285 10bit\n",
            "16286 patchwall\n",
            "16287 han\n",
            "16288 bounds\n",
            "16289 coppa\n",
            "16290 boseman\n",
            "16291 optically\n",
            "16292 gfk\n",
            "16293 dex\n",
            "16294 shortermaturity\n",
            "16295 lewis\n",
            "16296 ioswhatsapp\n",
            "16297 selva\n",
            "16298 kondaen\n",
            "16299 540\n",
            "16300 thieves\n",
            "16301 nescafe\n",
            "16302 acs\n",
            "16303 inverter\n",
            "16304 hcpn\n",
            "16305 clouds\n",
            "16306 bungalow\n",
            "16307 sized\n",
            "16308 addicted\n",
            "16309 waiter\n",
            "16310 heck\n",
            "16311 270\n",
            "16312 315\n",
            "16313 finalized\n",
            "16314 bhupendra\n",
            "16315 socialist\n",
            "16316 romeo\n",
            "16317 meat\n",
            "16318 loyalists\n",
            "16319 bangaldeshi\n",
            "16320 saandkiaankh\n",
            "16321 weekends\n",
            "16322 twoyearold\n",
            "16323 childish\n",
            "16324 linein\n",
            "16325 spdif\n",
            "16326 brahmaji\n",
            "16327 subbaraju\n",
            "16328 ronit\n",
            "16329 eyewatering\n",
            "16330 unbelievable\n",
            "16331 smoking\n",
            "16332 sensuous\n",
            "16333 purshottam\n",
            "16334 gaikwads\n",
            "16335 nudity\n",
            "16336 salvage\n",
            "16337 n11micromax\n",
            "16338 n12micromax\n",
            "16339 prevails\n",
            "16340 bribes\n",
            "16341 topdown\n",
            "16342 accrue\n",
            "16343 postpoll\n",
            "16344 designhuawei\n",
            "16345 20rs\n",
            "16346 89000\n",
            "16347 cheque\n",
            "16348 synthetics\n",
            "16349 variantoneplus\n",
            "16350 stocknote\n",
            "16351 sideways\n",
            "16352 smallcaps\n",
            "16353 interrupt\n",
            "16354 closeup\n",
            "16355 wronged\n",
            "16356 thicker\n",
            "16357 1987\n",
            "16358 hizbul\n",
            "16359 mujahideen\n",
            "16360 hurriyat\n",
            "16361 brisk\n",
            "16362 ltdrun\n",
            "16363 1337\n",
            "16364 retweets\n",
            "16365 33000\n",
            "16366 coolest\n",
            "16367 pfc\n",
            "16368 rec\n",
            "16369 mopping\n",
            "16370 sectorspecific\n",
            "16371 mai\n",
            "16372 choreographed\n",
            "16373 bates\n",
            "16374 1988\n",
            "16375 reprising\n",
            "16376 hugos\n",
            "16377 tellers\n",
            "16378 raamdeo\n",
            "16379 picker\n",
            "16380 3060\n",
            "16381 51360\n",
            "16382 7155\n",
            "16383 ritesh\n",
            "16384 senseforth\n",
            "16385 prebuilt\n",
            "16386 mirrorsize\n",
            "16387 measurements\n",
            "16388 piers\n",
            "16389 timeintensive\n",
            "16390 statistic\n",
            "16391 capt\n",
            "16392 transports\n",
            "16393 subordinate\n",
            "16394 punctuated\n",
            "16395 searing\n",
            "16396 sharmas\n",
            "16397 varthamans\n",
            "16398 murky\n",
            "16399 cells\n",
            "16400 gamecool\n",
            "16401 vapour\n",
            "16402 terrence\n",
            "16403 smollett\n",
            "16404 daniels\n",
            "16405 danny\n",
            "16406 customise\n",
            "16407 rubbing\n",
            "16408 sadagopan\n",
            "16409 clarke\n",
            "16410 800000\n",
            "16411 exempt\n",
            "16412 fouryear\n",
            "16413 129inch\n",
            "16414 512\n",
            "16415 cuba\n",
            "16416 colliders\n",
            "16417 weintraub\n",
            "16418 marvelstudios\n",
            "16419 influx\n",
            "16420 premieres\n",
            "16421 severed\n",
            "16422 vaibhav\n",
            "16423 500010000\n",
            "16424 judiciary\n",
            "16425 underpin\n",
            "16426 financier\n",
            "16427 primates\n",
            "16428 primnet\n",
            "16429 wrangling\n",
            "16430 assemble\n",
            "16431 lineups\n",
            "16432 introspect\n",
            "16433 ritual\n",
            "16434 morphing\n",
            "16435 indiapakistan\n",
            "16436 reorder\n",
            "16437 etch\n",
            "16438 hazard\n",
            "16439 362\n",
            "16440 ore\n",
            "16441 limestone\n",
            "16442 graphite\n",
            "16443 smelters\n",
            "16444 electromagnet\n",
            "16445 flowing\n",
            "16446 flux\n",
            "16447 induces\n",
            "16448 inductor\n",
            "16449 sockets\n",
            "16450 shrugging\n",
            "16451 utilities\n",
            "16452 720x1280\n",
            "16453 296ppi\n",
            "16454 rajamoulis\n",
            "16455 sharpness\n",
            "16456 m1asus\n",
            "16457 keira\n",
            "16458 knightley\n",
            "16459 righton\n",
            "16460 amateur\n",
            "16461 disbelief\n",
            "16462 terminated\n",
            "16463 recruiters\n",
            "16464 circuits\n",
            "16465 shrugged\n",
            "16466 shun\n",
            "16467 2900000\n",
            "16468 1846715\n",
            "16469 relating\n",
            "16470 abnormal\n",
            "16471 scoops\n",
            "16472 aprilmay\n",
            "16473 pot\n",
            "16474 viruses\n",
            "16475 farrelly\n",
            "16476 joanna\n",
            "16477 utilitarian\n",
            "16478 starters\n",
            "16479 citibank\n",
            "16480 parleys\n",
            "16481 wasnik\n",
            "16482 pray\n",
            "16483 tufts\n",
            "16484 haunting\n",
            "16485 octa\n",
            "16486 allotting\n",
            "16487 mcmc\n",
            "16488 kisses\n",
            "16489 sprawling\n",
            "16490 bordering\n",
            "16491 feelings\n",
            "16492 shrunk\n",
            "16493 4230mah\n",
            "16494 fest\n",
            "16495 ekgs\n",
            "16496 watchs\n",
            "16497 6021405\n",
            "16498 1530\n",
            "16499 collider\n",
            "16500 myscoot\n",
            "16501 yeddyurappas\n",
            "16502 52inch\n",
            "16503 snapper\n",
            "16504 deflect\n",
            "16505 backer\n",
            "16506 hillary\n",
            "16507 clinton\n",
            "16508 gaspard\n",
            "16509 reprehensible\n",
            "16510 bitcoins\n",
            "16511 periphery\n",
            "16512 malthusian\n",
            "16513 lstejasvi\n",
            "16514 171\n",
            "16515 bachupally\n",
            "16516 483\n",
            "16517 comprehensively\n",
            "16518 investigators\n",
            "16519 akiv\n",
            "16520 psephologists\n",
            "16521 numbered\n",
            "16522 dos\n",
            "16523 datar\n",
            "16524 amicus\n",
            "16525 curae\n",
            "16526 buzzing\n",
            "16527 greener\n",
            "16528 pastures\n",
            "16529 024\n",
            "16530 outlays\n",
            "16531 allimportant\n",
            "16532 amjad\n",
            "16533 scotland\n",
            "16534 transforms\n",
            "16535 prasthanam\n",
            "16536 20112012\n",
            "16537 underlined\n",
            "16538 suburb\n",
            "16539 realizations\n",
            "16540 fy2021\n",
            "16541 komal\n",
            "16542 namumkin\n",
            "16543 threeeyed\n",
            "16544 raven\n",
            "16545 pegatron\n",
            "16546 embroiled\n",
            "16547 0009\n",
            "16548 opting\n",
            "16549 flexing\n",
            "16550 constituent\n",
            "16551 ishan\n",
            "16552 ode\n",
            "16553 uninstall\n",
            "16554 deletesamsung\n",
            "16555 disablesamsung\n",
            "16556 mali\n",
            "16557 g76\n",
            "16558 mp10\n",
            "16559 201\n",
            "16560 stereoscopic\n",
            "16561 5ghz\n",
            "16562 way2wealth\n",
            "16563 objected\n",
            "16564 refurbished\n",
            "16565 gloves\n",
            "16566 bullies\n",
            "16567 intensely\n",
            "16568 splash\n",
            "16569 aarzoo\n",
            "16570 walabot\n",
            "16571 lowpower\n",
            "16572 nathaniel\n",
            "16573 gleicher\n",
            "16574 perish\n",
            "16575 shashikant\n",
            "16576 babasaheb\n",
            "16577 decimated\n",
            "16578 boroughs\n",
            "16579 envisages\n",
            "16580 oriental\n",
            "16581 derived\n",
            "16582 conceptual\n",
            "16583 ireland\n",
            "16584 scrutinising\n",
            "16585 merging\n",
            "16586 kalraj\n",
            "16587 1523\n",
            "16588 drafted\n",
            "16589 unspecified\n",
            "16590 counsels\n",
            "16591 pha\n",
            "16592 definite\n",
            "16593 propagating\n",
            "16594 exposing\n",
            "16595 framing\n",
            "16596 blurred\n",
            "16597 fernando\n",
            "16598 alonso\n",
            "16599 mclarenoneplus\n",
            "16600 indiamclaren\n",
            "16601 f1mclaren\n",
            "16602 p1oneplus\n",
            "16603 offenders\n",
            "16604 rebuked\n",
            "16605 advisories\n",
            "16606 growers\n",
            "16607 koppikar\n",
            "16608 nusrat\n",
            "16609 lankan\n",
            "16610 remainder\n",
            "16611 frontrunner\n",
            "16612 escalated\n",
            "16613 320\n",
            "16614 083\n",
            "16615 clamshelllike\n",
            "16616 euisuk\n",
            "16617 extremity\n",
            "16618 statewide\n",
            "16619 secularcongress\n",
            "16620 adbuying\n",
            "16621 placards\n",
            "16622 farrah\n",
            "16623 gasps\n",
            "16624 eyebrows\n",
            "16625 sebastian\n",
            "16626 accorded\n",
            "16627 voiced\n",
            "16628 snappily\n",
            "16629 sanctum\n",
            "16630 alipurduar\n",
            "16631 nalgonda\n",
            "16632 2032\n",
            "16633 mahal\n",
            "16634 cryptography\n",
            "16635 doodle\n",
            "16636 malpractices\n",
            "16637 mum\n",
            "16638 stubborn\n",
            "16639 slowest\n",
            "16640 messengers\n",
            "16641 rationale\n",
            "16642 haircuts\n",
            "16643 bachelors\n",
            "16644 164\n",
            "16645 submenu\n",
            "16646 tricolon\n",
            "16647 outdated\n",
            "16648 glittery\n",
            "16649 ethiopian\n",
            "16650 induct\n",
            "16651 kathua\n",
            "16652 impatient\n",
            "16653 rattled\n",
            "16654 bow\n",
            "16655 lehigh\n",
            "16656 pym\n",
            "16657 zolas\n",
            "16658 guessing\n",
            "16659 bumps\n",
            "16660 asgardian\n",
            "16661 erasing\n",
            "16662 singleday\n",
            "16663 portraying\n",
            "16664 companions\n",
            "16665 31500\n",
            "16666 midfebruary\n",
            "16667 jewelers\n",
            "16668 21335\n",
            "16669 convenor\n",
            "16670 surfaces\n",
            "16671 technologically\n",
            "16672 newlyformed\n",
            "16673 uttarakhands\n",
            "16674 7065\n",
            "16675 worldbeating\n",
            "16676 7388\n",
            "16677 jiosaavn\n",
            "16678 punctured\n",
            "16679 sevenmonth\n",
            "16680 2810\n",
            "16681 outcry\n",
            "16682 shouts\n",
            "16683 interval\n",
            "16684 unnatural\n",
            "16685 brimming\n",
            "16686 stagnated\n",
            "16687 fixing\n",
            "16688 overshadowing\n",
            "16689 dwellers\n",
            "16690 masse\n",
            "16691 cropped\n",
            "16692 mindful\n",
            "16693 acceded\n",
            "16694 oyj\n",
            "16695 poland\n",
            "16696 addis\n",
            "16697 ababa\n",
            "16698 monde\n",
            "16699 financed\n",
            "16700 girard\n",
            "16701 pogo\n",
            "16702 vacations\n",
            "16703 ukraine\n",
            "16704 sitcom\n",
            "16705 clearest\n",
            "16706 rant\n",
            "16707 ominous\n",
            "16708 datamining\n",
            "16709 disasters\n",
            "16710 humanlike\n",
            "16711 cerf\n",
            "16712 inflammatory\n",
            "16713 theron\n",
            "16714 leaner\n",
            "16715 fintech\n",
            "16716 764\n",
            "16717 replenishing\n",
            "16718 jem\n",
            "16719 horsepower\n",
            "16720 carolina\n",
            "16721 slackens\n",
            "16722 proindependence\n",
            "16723 angami\n",
            "16724 zapu\n",
            "16725 phizo\n",
            "16726 mpcs\n",
            "16727 neiphiu\n",
            "16728 ultraviolet\n",
            "16729 lithography\n",
            "16730 plasma\n",
            "16731 nanometers\n",
            "16732 yokohama\n",
            "16733 osamu\n",
            "16734 beware\n",
            "16735 usurping\n",
            "16736 flouting\n",
            "16737 sellout\n",
            "16738 manisha\n",
            "16739 adorable\n",
            "16740 middleman\n",
            "16741 dehradun\n",
            "16742 rehashing\n",
            "16743 insinuations\n",
            "16744 philosopher\n",
            "16745 monumental\n",
            "16746 texting\n",
            "16747 conferencing\n",
            "16748 1282\n",
            "16749 multiphased\n",
            "16750 961\n",
            "16751 onsite\n",
            "16752 214\n",
            "16753 nawabs\n",
            "16754 consolation\n",
            "16755 acharya\n",
            "16756 awe\n",
            "16757 jatavs\n",
            "16758 recap\n",
            "16759 infant\n",
            "16760 demonstrating\n",
            "16761 aidriven\n",
            "16762 residing\n",
            "16763 intelligently\n",
            "16764 kpop\n",
            "16765 maroon\n",
            "16766 loudly\n",
            "16767 winnability\n",
            "16768 glide\n",
            "16769 confederation\n",
            "16770 formulated\n",
            "16771 slabs\n",
            "16772 secondmost\n",
            "16773 coviewer\n",
            "16774 minimised\n",
            "16775 multistarrer\n",
            "16776 para\n",
            "16777 shears\n",
            "16778 diminishing\n",
            "16779 tensorflow\n",
            "16780 vino\n",
            "16781 rosemarie\n",
            "16782 nita\n",
            "16783 anils\n",
            "16784 broadening\n",
            "16785 seated\n",
            "16786 cushioned\n",
            "16787 singam\n",
            "16788 previouslybut\n",
            "16789 ballgame\n",
            "16790 raided\n",
            "16791 gulab\n",
            "16792 nubia\n",
            "16793 multitask\n",
            "16794 rearmounted\n",
            "16795 2611\n",
            "16796 suo\n",
            "16797 organiser\n",
            "16798 endlessly\n",
            "16799 organise\n",
            "16800 beatmap\n",
            "16801 darknet\n",
            "16802 indexed\n",
            "16803 insignificant\n",
            "16804 aggregators\n",
            "16805 xperia\n",
            "16806 lecture\n",
            "16807 empathy\n",
            "16808 padma\n",
            "16809 civilian\n",
            "16810 tieups\n",
            "16811 chandel\n",
            "16812 rangoli\n",
            "16813 sonic\n",
            "16814 fowler\n",
            "16815 vacating\n",
            "16816 icms\n",
            "16817 cutthroat\n",
            "16818 ashland\n",
            "16819 olsens\n",
            "16820 cobie\n",
            "16821 gurira\n",
            "16822 jarring\n",
            "16823 bored\n",
            "16824 superman\n",
            "16825 6068\n",
            "16826 617\n",
            "16827 7198\n",
            "16828 4427\n",
            "16829 golf\n",
            "16830 boarded\n",
            "16831 high5\n",
            "16832 vela\n",
            "16833 ramamoorthy\n",
            "16834 babus\n",
            "16835 riteish\n",
            "16836 mamatas\n",
            "16837 postpolls\n",
            "16838 antitheft\n",
            "16839 penning\n",
            "16840 jonah\n",
            "16841 leonardo\n",
            "16842 80211abgnac\n",
            "16843 scion\n",
            "16844 hilly\n",
            "16845 backwardness\n",
            "16846 backseat\n",
            "16847 capita\n",
            "16848 windfall\n",
            "16849 lineupthe\n",
            "16850 pocophone\n",
            "16851 m1901f9t\n",
            "16852 7series\n",
            "16853 psychological\n",
            "16854 actresses\n",
            "16855 nears\n",
            "16856 valueconscious\n",
            "16857 krishnakumar\n",
            "16858 europeans\n",
            "16859 swapped\n",
            "16860 overruns\n",
            "16861 curriculum\n",
            "16862 anecdote\n",
            "16863 2560x1440\n",
            "16864 topspec\n",
            "16865 506\n",
            "16866 anecdotal\n",
            "16867 undisclosed\n",
            "16868 rick\n",
            "16869 sophomore\n",
            "16870 shrine\n",
            "16871 semiautomatic\n",
            "16872 rsa\n",
            "16873 bidar\n",
            "16874 davangere\n",
            "16875 ballari\n",
            "16876 percapita\n",
            "16877 varsity\n",
            "16878 1718\n",
            "16879 facto\n",
            "16880 chinaus\n",
            "16881 131060\n",
            "16882 hamlets\n",
            "16883 indelible\n",
            "16884 swarm\n",
            "16885 6563\n",
            "16886 1142\n",
            "16887 motivate\n",
            "16888 somen\n",
            "16889 highcommand\n",
            "16890 draghi\n",
            "16891 hardhitting\n",
            "16892 erin\n",
            "16893 slipping\n",
            "16894 neighborhood\n",
            "16895 balances\n",
            "16896 forbidden\n",
            "16897 4999\n",
            "16898 groundlevel\n",
            "16899 muddied\n",
            "16900 bankura\n",
            "16901 500kg\n",
            "16902 loath\n",
            "16903 moons\n",
            "16904 bickert\n",
            "16905 045\n",
            "16906 downdetector\n",
            "16907 peru\n",
            "16908 apologized\n",
            "16909 evidenced\n",
            "16910 ramdas\n",
            "16911 dalits\n",
            "16912 concerted\n",
            "16913 11900\n",
            "16914 3334\n",
            "16915 28000\n",
            "16916 powerpoint\n",
            "16917 tags\n",
            "16918 addon\n",
            "16919 periodically\n",
            "16920 consequently\n",
            "16921 reinventing\n",
            "16922 programmatic\n",
            "16923 bedroom\n",
            "16924 escapism\n",
            "16925 435\n",
            "16926 steepening\n",
            "16927 groped\n",
            "16928 offerhonor\n",
            "16929 goxiaomixiaomi\n",
            "16930 smartphoneredmi\n",
            "16931 certifying\n",
            "16932 ayushmann\n",
            "16933 trolling\n",
            "16934 breeds\n",
            "16935 antidote\n",
            "16936 sania\n",
            "16937 idol\n",
            "16938 befriends\n",
            "16939 cena\n",
            "16940 courtney\n",
            "16941 macedonia\n",
            "16942 kosovo\n",
            "16943 outpacing\n",
            "16944 unnoticed\n",
            "16945 esteemed\n",
            "16946 galvanising\n",
            "16947 skull\n",
            "16948 snows\n",
            "16949 crypts\n",
            "16950 statue\n",
            "16951 rhaegar\n",
            "16952 normalized\n",
            "16953 glaring\n",
            "16954 contributors\n",
            "16955 veyil\n",
            "16956 shane\n",
            "16957 frenzied\n",
            "16958 killers\n",
            "16959 souled\n",
            "16960 waived\n",
            "16961 pastel\n",
            "16962 pitts\n",
            "16963 cameos\n",
            "16964 karthi\n",
            "16965 bombed\n",
            "16966 rammed\n",
            "16967 tense\n",
            "16968 adams\n",
            "16969 skrulls\n",
            "16970 avenue\n",
            "16971 masters\n",
            "16972 baron\n",
            "16973 jarvis\n",
            "16974 faithful\n",
            "16975 comprised\n",
            "16976 detachable\n",
            "16977 lyca\n",
            "16978 dovetail\n",
            "16979 counterattack\n",
            "16980 arrogance\n",
            "16981 economical\n",
            "16982 dreamers\n",
            "16983 coveting\n",
            "16984 dharampeth\n",
            "16985 dhantoli\n",
            "16986 toast\n",
            "16987 tar\n",
            "16988 mushrooming\n",
            "16989 sirens\n",
            "16990 sprang\n",
            "16991 fledgling\n",
            "16992 36999\n",
            "16993 samoled\n",
            "16994 1859\n",
            "16995 392ppi\n",
            "16996 centrum\n",
            "16997 offershuawei\n",
            "16998 avert\n",
            "16999 barfi\n",
            "17000 prudential\n",
            "17001 chatted\n",
            "17002 jurors\n",
            "17003 flared\n",
            "17004 siliguri\n",
            "17005 thakor\n",
            "17006 szymon\n",
            "17007 hsieh\n",
            "17008 antony\n",
            "17009 recommending\n",
            "17010 wala\n",
            "17011 guddu\n",
            "17012 shroffs\n",
            "17013 upliftment\n",
            "17014 sarfarosh\n",
            "17015 dasmint\n",
            "17016 openmarket\n",
            "17017 rathi\n",
            "17018 11400\n",
            "17019 hubhopper\n",
            "17020 podcasting\n",
            "17021 jukebox\n",
            "17022 neat\n",
            "17023 centrepiece\n",
            "17024 tupac\n",
            "17025 racist\n",
            "17026 rosewood\n",
            "17027 destructive\n",
            "17028 cocaine\n",
            "17029 jacksons\n",
            "17030 dsd\n",
            "17031 flac\n",
            "17032 mp3\n",
            "17033 kohli\n",
            "17034 2299\n",
            "17035 crowding\n",
            "17036 banaras\n",
            "17037 billionaires\n",
            "17038 932\n",
            "17039 owaisis\n",
            "17040 defying\n",
            "17041 midyear\n",
            "17042 actionable\n",
            "17043 leftofcentre\n",
            "17044 granting\n",
            "17045 worstoff\n",
            "17046 baniphone\n",
            "17047 rami\n",
            "17048 malek\n",
            "17049 esmail\n",
            "17050 urbanization\n",
            "17051 rustic\n",
            "17052 kalam\n",
            "17053 mudra\n",
            "17054 oem\n",
            "17055 offputting\n",
            "17056 quips\n",
            "17057 eid\n",
            "17058 undermine\n",
            "17059 hesitation\n",
            "17060 infra\n",
            "17061 timothy\n",
            "17062 olyphant\n",
            "17063 knack\n",
            "17064 dlfs\n",
            "17065 debtfree\n",
            "17066 2250\n",
            "17067 adhala\n",
            "17068 varma\n",
            "17069 visakhapatnam\n",
            "17070 governmentrun\n",
            "17071 offloaded\n",
            "17072 imacs\n",
            "17073 crossplatform\n",
            "17074 dec\n",
            "17075 98000\n",
            "17076 148000\n",
            "17077 carlos\n",
            "17078 establishment\n",
            "17079 domination\n",
            "17080 enveloped\n",
            "17081 propose\n",
            "17082 holden\n",
            "17083 insensitive\n",
            "17084 facade\n",
            "17085 categorically\n",
            "17086 atrocity\n",
            "17087 48800\n",
            "17088 vijayendra\n",
            "17089 protector\n",
            "17090 incorporates\n",
            "17091 lauren\n",
            "17092 nell\n",
            "17093 kumail\n",
            "17094 nanjiani\n",
            "17095 loren\n",
            "17096 unscripted\n",
            "17097 lavish\n",
            "17098 dickinson\n",
            "17099 showtimes\n",
            "17100 cruz\n",
            "17101 thangs\n",
            "17102 saira\n",
            "17103 aaditya\n",
            "17104 landslide\n",
            "17105 pinterest\n",
            "17106 testify\n",
            "17107 egregious\n",
            "17108 javed\n",
            "17109 haji\n",
            "17110 baramulla\n",
            "17111 3series\n",
            "17112 metaphysical\n",
            "17113 hunkydory\n",
            "17114 bite\n",
            "17115 rationalize\n",
            "17116 shied\n",
            "17117 gratification\n",
            "17118 coherent\n",
            "17119 farrukhabad\n",
            "17120 khurshid\n",
            "17121 spirited\n",
            "17122 5150\n",
            "17123 aggregating\n",
            "17124 rsp\n",
            "17125 gritty\n",
            "17126 sincere\n",
            "17127 livestream\n",
            "17128 failings\n",
            "17129 hackles\n",
            "17130 280\n",
            "17131 712\n",
            "17132 updatewhatsapp\n",
            "17133 replywhatsapp\n",
            "17134 backupwhats\n",
            "17135 updatewhats\n",
            "17136 kathgodam\n",
            "17137 shatabdi\n",
            "17138 reprimanded\n",
            "17139 lackadaisical\n",
            "17140 legacies\n",
            "17141 cauvery\n",
            "17142 longpress\n",
            "17143 practicality\n",
            "17144 katzenberg\n",
            "17145 virtualreality\n",
            "17146 racket\n",
            "17147 wagers\n",
            "17148 sandip\n",
            "17149 biopics\n",
            "17150 pitbull\n",
            "17151 vitality\n",
            "17152 prepping\n",
            "17153 goodfellas\n",
            "17154 bangalorebased\n",
            "17155 suparna\n",
            "17156 drafting\n",
            "17157 waymo\n",
            "17158 ais\n",
            "17159 enjoying\n",
            "17160 weaknesses\n",
            "17161 a8ssamsung\n",
            "17162 ensuing\n",
            "17163 5redmi\n",
            "17164 proxiaomiredmi\n",
            "17165 showrunners\n",
            "17166 wagner\n",
            "17167 hampered\n",
            "17168 uavs\n",
            "17169 bhansalis\n",
            "17170 diversion\n",
            "17171 shady\n",
            "17172 raced\n",
            "17173 vellaikaran\n",
            "17174 deshpande\n",
            "17175 dangare\n",
            "17176 parlay\n",
            "17177 costcutting\n",
            "17178 funnier\n",
            "17179 comedies\n",
            "17180 summits\n",
            "17181 confounding\n",
            "17182 majors\n",
            "17183 defies\n",
            "17184 smallscale\n",
            "17185 entrepreneur\n",
            "17186 barrackpore\n",
            "17187 denominated\n",
            "17188 rites\n",
            "17189 pwd\n",
            "17190 reunions\n",
            "17191 amareswar\n",
            "17192 concurs\n",
            "17193 miniseries\n",
            "17194 wells\n",
            "17195 nathan\n",
            "17196 catherine\n",
            "17197 shortcoming\n",
            "17198 groundbut\n",
            "17199 literature\n",
            "17200 oneonone\n",
            "17201 testified\n",
            "17202 drill\n",
            "17203 instinct\n",
            "17204 moviegoers\n",
            "17205 nlu\n",
            "17206 shallow\n",
            "17207 recognizing\n",
            "17208 intents\n",
            "17209 materialize\n",
            "17210 discountredmi\n",
            "17211 revolved\n",
            "17212 phillip\n",
            "17213 terrorismfree\n",
            "17214 moustache\n",
            "17215 veriown\n",
            "17216 chicagobased\n",
            "17217 kerosene\n",
            "17218 lamps\n",
            "17219 matoshri\n",
            "17220 turnouts\n",
            "17221 251\n",
            "17222 triathlon\n",
            "17223 micropayments\n",
            "17224 citic\n",
            "17225 picc\n",
            "17226 beneath\n",
            "17227 730g\n",
            "17228 2ghz\n",
            "17229 wakes\n",
            "17230 beatles\n",
            "17231 teller\n",
            "17232 redevelopment\n",
            "17233 refrigerators\n",
            "17234 hassle\n",
            "17235 chloe\n",
            "17236 jarmusch\n",
            "17237 ramzan\n",
            "17238 1053\n",
            "17239 washingtons\n",
            "17240 napkin\n",
            "17241 reinvention\n",
            "17242 chandrakala\n",
            "17243 objectionable\n",
            "17244 beed\n",
            "17245 buldhana\n",
            "17246 pravin\n",
            "17247 lesspowerful\n",
            "17248 codesigned\n",
            "17249 bro\n",
            "17250 dpi\n",
            "17251 conspiring\n",
            "17252 nyongo\n",
            "17253 pointbypoint\n",
            "17254 rambus\n",
            "17255 accelerating\n",
            "17256 swinburne\n",
            "17257 handcurated\n",
            "17258 parvathys\n",
            "17259 6896\n",
            "17260 pharmaceuticals\n",
            "17261 irrational\n",
            "17262 labelling\n",
            "17263 structuring\n",
            "17264 statistical\n",
            "17265 documentation\n",
            "17266 transferring\n",
            "17267 labor\n",
            "17268 dir\n",
            "17269 gripping\n",
            "17270 sathyaprakash\n",
            "17271 battatawada\n",
            "17272 mehandicircus\n",
            "17273 roldan\n",
            "17274 icea\n",
            "17275 optimise\n",
            "17276 ballad\n",
            "17277 atlanta\n",
            "17278 nepotism\n",
            "17279 decay\n",
            "17280 pessimism\n",
            "17281 rulingtelugu\n",
            "17282 facebookfake\n",
            "17283 splitscreen\n",
            "17284 cowatching\n",
            "17285 rovio\n",
            "17286 achievable\n",
            "17287 beneficiary\n",
            "17288 pennsylvania\n",
            "17289 responsibly\n",
            "17290 agencys\n",
            "17291 commencement\n",
            "17292 roadblock\n",
            "17293 balbir\n",
            "17294 jjp\n",
            "17295 unimplementable\n",
            "17296 sympathizers\n",
            "17297 amend\n",
            "17298 reasoned\n",
            "17299 380\n",
            "17300 artem\n",
            "17301 surviving\n",
            "17302 teenager\n",
            "17303 polarization\n",
            "17304 varsities\n",
            "17305 locking\n",
            "17306 puppet\n",
            "17307 thirds\n",
            "17308 destined\n",
            "17309 selfcontained\n",
            "17310 70s\n",
            "17311 danvers\n",
            "17312 superpowers\n",
            "17313 tenacity\n",
            "17314 rhetorically\n",
            "17315 beginnings\n",
            "17316 transporting\n",
            "17317 scabal\n",
            "17318 leaderboards\n",
            "17319 blocker\n",
            "17320 gali\n",
            "17321 rational\n",
            "17322 stacking\n",
            "17323 realistically\n",
            "17324 stimulate\n",
            "17325 2400\n",
            "17326 underpinning\n",
            "17327 94000\n",
            "17328 stationsbooths\n",
            "17329 424\n",
            "17330 6922\n",
            "17331 sanya\n",
            "17332 6969\n",
            "17333 commissionwarns\n",
            "17334 nvdias\n",
            "17335 antialiasing\n",
            "17336 2080ti\n",
            "17337 2160p\n",
            "17338 talents\n",
            "17339 finest\n",
            "17340 mongering\n",
            "17341 sao\n",
            "17342 paulo\n",
            "17343 mohapatra\n",
            "17344 sheldon\n",
            "17345 romcoms\n",
            "17346 coms\n",
            "17347 chasm\n",
            "17348 academia\n",
            "17349 nods\n",
            "17350 sullivan\n",
            "17351 brussels\n",
            "17352 similarity\n",
            "17353 delightful\n",
            "17354 envoy\n",
            "17355 finalise\n",
            "17356 squandered\n",
            "17357 antisocial\n",
            "17358 storms\n",
            "17359 acumen\n",
            "17360 flaunting\n",
            "17361 supplementary\n",
            "17362 chargesheet\n",
            "17363 salenokia\n",
            "17364 offernokia\n",
            "17365 storenokia\n",
            "17366 kathmandu\n",
            "17367 muddy\n",
            "17368 simplemake\n",
            "17369 7499\n",
            "17370 ugly\n",
            "17371 megastar\n",
            "17372 tejs\n",
            "17373 prioritizes\n",
            "17374 countering\n",
            "17375 resetting\n",
            "17376 f16\n",
            "17377 injuring\n",
            "17378 flock\n",
            "17379 360000\n",
            "17380 sana\n",
            "17381 alice\n",
            "17382 webb\n",
            "17383 cbbc\n",
            "17384 mya\n",
            "17385 uncharted\n",
            "17386 gibsons\n",
            "17387 ciso\n",
            "17388 audited\n",
            "17389 derives\n",
            "17390 exportoriented\n",
            "17391 onus\n",
            "17392 underperforming\n",
            "17393 7990\n",
            "17394 cpis\n",
            "17395 929\n",
            "17396 prospect\n",
            "17397 firmer\n",
            "17398 threeweek\n",
            "17399 karthik\n",
            "17400 str\n",
            "17401 testosterone\n",
            "17402 illconceived\n",
            "17403 sequels\n",
            "17404 rohan\n",
            "17405 modestly\n",
            "17406 matrimonycom\n",
            "17407 jeevansathicom\n",
            "17408 madly\n",
            "17409 winters\n",
            "17410 nexus\n",
            "17411 reign\n",
            "17412 olive\n",
            "17413 surpass\n",
            "17414 licensee\n",
            "17415 jayam\n",
            "17416 foldgate\n",
            "17417 vijaysethupathi\n",
            "17418 kodambakkam\n",
            "17419 tnelections2019\n",
            "17420 vijaysethuoffl\n",
            "17421 webs\n",
            "17422 doll\n",
            "17423 nickname\n",
            "17424 finserv\n",
            "17425 runofthemill\n",
            "17426 recordhigh\n",
            "17427 mira\n",
            "17428 shanta\n",
            "17429 electionsrahul\n",
            "17430 gandhiassembly\n",
            "17431 electionsvidhan\n",
            "17432 1865\n",
            "17433 sweetness\n",
            "17434 rooting\n",
            "17435 ratnagiri\n",
            "17436 likeminded\n",
            "17437 hunters\n",
            "17438 absorbed\n",
            "17439 raichur\n",
            "17440 stints\n",
            "17441 mastermind\n",
            "17442 chetna\n",
            "17443 sood\n",
            "17444 divya\n",
            "17445 132\n",
            "17446 memorialized\n",
            "17447 menon\n",
            "17448 redemptions\n",
            "17449 respected\n",
            "17450 propelled\n",
            "17451 059\n",
            "17452 carriall\n",
            "17453 vasco\n",
            "17454 tanuj\n",
            "17455 riya\n",
            "17456 messaged\n",
            "17457 katial\n",
            "17458 majorly\n",
            "17459 kothari\n",
            "17460 neon\n",
            "17461 lastmile\n",
            "17462 calibrating\n",
            "17463 dicaprios\n",
            "17464 dalton\n",
            "17465 whistleblower\n",
            "17466 pill\n",
            "17467 swallow\n",
            "17468 aiocd\n",
            "17469 innate\n",
            "17470 vibrance\n",
            "17471 possess\n",
            "17472 zindagii\n",
            "17473 koum\n",
            "17474 reinstate\n",
            "17475 indraganti\n",
            "17476 lopez\n",
            "17477 derek\n",
            "17478 estranged\n",
            "17479 r1899\n",
            "17480 36500\n",
            "17481 vin\n",
            "17482 horses\n",
            "17483 hasbeens\n",
            "17484 veeru\n",
            "17485 dena\n",
            "17486 belts\n",
            "17487 recognisable\n",
            "17488 costlier\n",
            "17489 valmiki\n",
            "17490 unicorns\n",
            "17491 402ppi\n",
            "17492 unabashedly\n",
            "17493 illnesses\n",
            "17494 cardiac\n",
            "17495 complying\n",
            "17496 doctorpatient\n",
            "17497 derail\n",
            "17498 discern\n",
            "17499 mortal\n",
            "17500 switzerland\n",
            "17501 xiaomihonor\n",
            "17502 9nrealmeredmi\n",
            "17503 prorealme\n",
            "17504 2honor\n",
            "17505 comparisonhonor\n",
            "17506 1119\n",
            "17507 13270\n",
            "17508 1378\n",
            "17509 13580\n",
            "17510 scoff\n",
            "17511 audiobooks\n",
            "17512 pedals\n",
            "17513 tube\n",
            "17514 cords\n",
            "17515 guitarsgibson\n",
            "17516 guitarsguitar\n",
            "17517 pickupsguitar\n",
            "17518 ampsfender\n",
            "17519 ampsroland\n",
            "17520 ampsguitar\n",
            "17521 techbluetooth\n",
            "17522 trinity\n",
            "17523 heald\n",
            "17524 cabs\n",
            "17525 ferry\n",
            "17526 pani\n",
            "17527 cricketers\n",
            "17528 cutsiphone\n",
            "17529 kardon\n",
            "17530 specialists\n",
            "17531 transcripts\n",
            "17532 categorize\n",
            "17533 laura\n",
            "17534 restored\n",
            "17535 twominute\n",
            "17536 urges\n",
            "17537 genres\n",
            "17538 dateredmi\n",
            "17539 saleredmi\n",
            "17540 discountblack\n",
            "17541 improperly\n",
            "17542 modified\n",
            "17543 memojis\n",
            "17544 1030\n",
            "17545 12appleios\n",
            "17546 6siphone\n",
            "17547 5siphone\n",
            "17548 6iphone\n",
            "17549 7iphone\n",
            "17550 8iphone\n",
            "17551 8siphone\n",
            "17552 xiphone\n",
            "17553 xrhow\n",
            "17554 iphonehow\n",
            "17555 quorum\n",
            "17556 cosby\n",
            "17557 witnesses\n",
            "17558 comedians\n",
            "17559 laurence\n",
            "17560 mimic\n",
            "17561 950000\n",
            "17562 passcode\n",
            "17563 digit\n",
            "17564 kgf\n",
            "17565 22yearold\n",
            "17566 rishab\n",
            "17567 incapacitated\n",
            "17568 eldest\n",
            "17569 dampener\n",
            "17570 erosion\n",
            "17571 militancy\n",
            "17572 plumber\n",
            "17573 gaze\n",
            "17574 carnegie\n",
            "17575 endowment\n",
            "17576 pacts\n",
            "17577 cliff\n",
            "17578 tan\n",
            "17579 specifcationsmi\n",
            "17580 micommi\n",
            "17581 44inch\n",
            "17582 skidded\n",
            "17583 applauded\n",
            "17584 tomb\n",
            "17585 machinelearning\n",
            "17586 mathematical\n",
            "17587 seymour\n",
            "17588 hazardous\n",
            "17589 snapr\n",
            "17590 gaga\n",
            "17591 bhakt\n",
            "17592 laav\n",
            "17593 dictatorial\n",
            "17594 maas\n",
            "17595 ridehailing\n",
            "17596 432\n",
            "17597 rajini\n",
            "17598 precursor\n",
            "17599 kazhakam\n",
            "17600 nikki\n",
            "17601 tamboli\n",
            "17602 aurangzeb\n",
            "17603 singles\n",
            "17604 lenient\n",
            "17605 aarti\n",
            "17606 6tamazon\n",
            "17607 orderoneplus\n",
            "17608 obligated\n",
            "17609 071\n",
            "17610 3802432\n",
            "17611 074\n",
            "17612 38989\n",
            "17613 lumpsum\n",
            "17614 leelaventure\n",
            "17615 saree\n",
            "17616 herald\n",
            "17617 publications\n",
            "17618 rimi\n",
            "17619 royce\n",
            "17620 backend\n",
            "17621 13051315\n",
            "17622 10or\n",
            "17623 a2s\n",
            "17624 reviewmi\n",
            "17625 fouryearold\n",
            "17626 arson\n",
            "17627 hygiene\n",
            "17628 tecno\n",
            "17629 demetriades\n",
            "17630 rampal\n",
            "17631 moods\n",
            "17632 eagerly\n",
            "17633 cosmopolitan\n",
            "17634 middleaged\n",
            "17635 milawat\n",
            "17636 retreat\n",
            "17637 moviemaking\n",
            "17638 bjpruled\n",
            "17639 dmonte\n",
            "17640 ispirt\n",
            "17641 jayanth\n",
            "17642 kolla\n",
            "17643 indragantis\n",
            "17644 bijnor\n",
            "17645 linkages\n",
            "17646 seeds\n",
            "17647 honey\n",
            "17648 consortia\n",
            "17649 marine\n",
            "17650 lowerend\n",
            "17651 prevail\n",
            "17652 uniquely\n",
            "17653 kayak\n",
            "17654 30005000\n",
            "17655 alisa\n",
            "17656 212k\n",
            "17657 scalability\n",
            "17658 tractors\n",
            "17659 lyft\n",
            "17660 encrypts\n",
            "17661 actionpacked\n",
            "17662 wounded\n",
            "17663 pained\n",
            "17664 affinity\n",
            "17665 mangoes\n",
            "17666 docs\n",
            "17667 somashekar\n",
            "17668 usdinr\n",
            "17669 13000\n",
            "17670 kermit\n",
            "17671 eller\n",
            "17672 survived\n",
            "17673 slomo\n",
            "17674 37yearold\n",
            "17675 6676\n",
            "17676 viacom18\n",
            "17677 pleasure\n",
            "17678 internetbased\n",
            "17679 hypertext\n",
            "17680 invention\n",
            "17681 installs\n",
            "17682 runtime\n",
            "17683 countrywide\n",
            "17684 disbanded\n",
            "17685 sigh\n",
            "17686 regained\n",
            "17687 airspace\n",
            "17688 assortment\n",
            "17689 acer\n",
            "17690 prejudice\n",
            "17691 differentiator\n",
            "17692 attaching\n",
            "17693 unleashed\n",
            "17694 goingson\n",
            "17695 incredulous\n",
            "17696 3hourplus\n",
            "17697 sags\n",
            "17698 bangforthebuck\n",
            "17699 unexplained\n",
            "17700 slowdowns\n",
            "17701 logins\n",
            "17702 204\n",
            "17703 liquefied\n",
            "17704 learnings\n",
            "17705 artificially\n",
            "17706 255\n",
            "17707 reins\n",
            "17708 deverakonda\n",
            "17709 lips\n",
            "17710 continually\n",
            "17711 bragging\n",
            "17712 prof\n",
            "17713 narayanswamy\n",
            "17714 googlekpmg\n",
            "17715 responds\n",
            "17716 fnatic\n",
            "17717 invictus\n",
            "17718 exploits\n",
            "17719 starcraft\n",
            "17720 mindfulness\n",
            "17721 credential\n",
            "17722 stuffing\n",
            "17723 squarely\n",
            "17724 murderer\n",
            "17725 feeble\n",
            "17726 148\n",
            "17727 saeed\n",
            "17728 strategically\n",
            "17729 stonepelting\n",
            "17730 birds\n",
            "17731 blowing\n",
            "17732 siphoning\n",
            "17733 ngos\n",
            "17734 symantecs\n",
            "17735 fetches\n",
            "17736 cousins\n",
            "17737 electorally\n",
            "17738 symantec\n",
            "17739 mostwatched\n",
            "17740 adjacent\n",
            "17741 unadulterated\n",
            "17742 julie\n",
            "17743 gump\n",
            "17744 selfmade\n",
            "17745 saudiled\n",
            "17746 azeri\n",
            "17747 recouped\n",
            "17748 capitalized\n",
            "17749 heats\n",
            "17750 unreachable\n",
            "17751 muscles\n",
            "17752 innovating\n",
            "17753 securityselling\n",
            "17754 drivedata\n",
            "17755 theftwindows\n",
            "17756 dataandroidios\n",
            "17757 extravaganza\n",
            "17758 throws\n",
            "17759 bishop\n",
            "17760 lila\n",
            "17761 hail\n",
            "17762 scepter\n",
            "17763 recreated\n",
            "17764 vintage\n",
            "17765 pfeiffer\n",
            "17766 onset\n",
            "17767 584inch\n",
            "17768 beans\n",
            "17769 lodges\n",
            "17770 congratulatory\n",
            "17771 standout\n",
            "17772 rudrapur\n",
            "17773 focussing\n",
            "17774 villagelevel\n",
            "17775 barbara\n",
            "17776 broccoli\n",
            "17777 goahead\n",
            "17778 733\n",
            "17779 622\n",
            "17780 newsstand\n",
            "17781 surat\n",
            "17782 nestled\n",
            "17783 paypal\n",
            "17784 leftwing\n",
            "17785 restive\n",
            "17786 splitting\n",
            "17787 dushyant\n",
            "17788 unfolds\n",
            "17789 proposing\n",
            "17790 xii\n",
            "17791 criticising\n",
            "17792 mtn\n",
            "17793 3105\n",
            "17794 grins\n",
            "17795 cuff\n",
            "17796 headlining\n",
            "17797 sleeves\n",
            "17798 supersonic\n",
            "17799 putrid\n",
            "17800 rotting\n",
            "17801 fruits\n",
            "17802 jamaica\n",
            "17803 rgbw\n",
            "17804 27mm\n",
            "17805 prorich\n",
            "17806 backlog\n",
            "17807 shortcycle\n",
            "17808 expresses\n",
            "17809 highfidelity\n",
            "17810 decorative\n",
            "17811 multifunction\n",
            "17812 aftersales\n",
            "17813 crowdfunding\n",
            "17814 t1s\n",
            "17815 mutton\n",
            "17816 biryani\n",
            "17817 supervillain\n",
            "17818 hovers\n",
            "17819 kiwi\n",
            "17820 6001\n",
            "17821 aaron\n",
            "17822 kaifs\n",
            "17823 fairy\n",
            "17824 jawaani\n",
            "17825 camerasphone\n",
            "17826 40megapixel\n",
            "17827 pohkran\n",
            "17828 ajai\n",
            "17829 viewpoint\n",
            "17830 satram\n",
            "17831 kishangarh\n",
            "17832 protruding\n",
            "17833 jaisalmer\n",
            "17834 ramgarh\n",
            "17835 65yearold\n",
            "17836 aadhaarenabled\n",
            "17837 brazen\n",
            "17838 torch\n",
            "17839 popularly\n",
            "17840 osho\n",
            "17841 lena\n",
            "17842 outlookcom\n",
            "17843 lanthanides\n",
            "17844 fyi\n",
            "17845 blowout\n",
            "17846 dogged\n",
            "17847 colin\n",
            "17848 volvo\n",
            "17849 tables\n",
            "17850 detractors\n",
            "17851 awake\n",
            "17852 bombing\n",
            "17853 dekh\n",
            "17854 allplastic\n",
            "17855 shrewd\n",
            "17856 24990\n",
            "17857 5gequipped\n",
            "17858 nehal\n",
            "17859 obstacles\n",
            "17860 blockbusters\n",
            "17861 cultivate\n",
            "17862 entrepreneurial\n",
            "17863 sewing\n",
            "17864 unicef\n",
            "17865 ifsc\n",
            "17866 modernisation\n",
            "17867 crudes\n",
            "17868 298619\n",
            "17869 299871\n",
            "17870 codeswhatsapp\n",
            "17871 alexapowered\n",
            "17872 refundable\n",
            "17873 warmer\n",
            "17874 earmarked\n",
            "17875 creed\n",
            "17876 sabka\n",
            "17877 prey\n",
            "17878 unidimensional\n",
            "17879 actorpolitician\n",
            "17880 comavengers\n",
            "17881 nighttime\n",
            "17882 akhil\n",
            "17883 vidyarthi\n",
            "17884 abvp\n",
            "17885 pestered\n",
            "17886 lkadvani\n",
            "17887 amareshan\n",
            "17888 ruin\n",
            "17889 porbandarrajkot\n",
            "17890 smiles\n",
            "17891 twolane\n",
            "17892 gbps\n",
            "17893 gbpscertified\n",
            "17894 spoon\n",
            "17895 nagalands\n",
            "17896 ibis\n",
            "17897 nandamuri\n",
            "17898 balakrishna\n",
            "17899 pashupati\n",
            "17900 dals\n",
            "17901 flexwash\n",
            "17902 buildcon\n",
            "17903 mep\n",
            "17904 viola\n",
            "17905 wordofmouth\n",
            "17906 multiverse\n",
            "17907 gyllenhaal\n",
            "17908 exgirlfriend\n",
            "17909 synchronized\n",
            "17910 selections\n",
            "17911 infectious\n",
            "17912 mays\n",
            "17913 279\n",
            "17914 studded\n",
            "17915 businesss\n",
            "17916 dyfs\n",
            "17917 blackrock\n",
            "17918 carnage\n",
            "17919 nonperforming\n",
            "17920 firming\n",
            "17921 ailes\n",
            "17922 technalysis\n",
            "17923 unkept\n",
            "17924 16000\n",
            "17925 insists\n",
            "17926 licences\n",
            "17927 mercedes\n",
            "17928 fugitive\n",
            "17929 sanction\n",
            "17930 intervene\n",
            "17931 56288\n",
            "17932 tamasha\n",
            "17933 bagalkot\n",
            "17934 roohafza\n",
            "17935 agility\n",
            "17936 harnessing\n",
            "17937 interconnected\n",
            "17938 launchpad\n",
            "17939 targeryens\n",
            "17940 ramsey\n",
            "17941 airing\n",
            "17942 olds\n",
            "17943 32775\n",
            "17944 38500\n",
            "17945 restrain\n",
            "17946 dogs\n",
            "17947 precautions\n",
            "17948 ambushes\n",
            "17949 assessing\n",
            "17950 thakkar\n",
            "17951 konkan\n",
            "17952 bloombergquint\n",
            "17953 asymmetrical\n",
            "17954 angular\n",
            "17955 bogey\n",
            "17956 14th\n",
            "17957 incidentfree\n",
            "17958 whistle\n",
            "17959 precedence\n",
            "17960 recreations\n",
            "17961 mildly\n",
            "17962 habitual\n",
            "17963 indepth\n",
            "17964 manifestoes\n",
            "17965 feasible\n",
            "17966 efficiencies\n",
            "17967 permeates\n",
            "17968 meaningfully\n",
            "17969 npr\n",
            "17970 bhaduri\n",
            "17971 localised\n",
            "17972 hubris\n",
            "17973 ops\n",
            "17974 ramadoss\n",
            "17975 piggyback\n",
            "17976 allying\n",
            "17977 undisputed\n",
            "17978 snatch\n",
            "17979 stalins\n",
            "17980 vaikos\n",
            "17981 marumalarchi\n",
            "17982 baseband\n",
            "17983 57inches\n",
            "17984 1512x720\n",
            "17985 900000\n",
            "17986 dastardly\n",
            "17987 blatantly\n",
            "17988 shayari\n",
            "17989 gaye\n",
            "17990 dram\n",
            "17991 harshvardhan\n",
            "17992 parvesh\n",
            "17993 bhange\n",
            "17994 underlines\n",
            "17995 16990\n",
            "17996 4699\n",
            "17997 231\n",
            "17998 vpn\n",
            "17999 saket\n",
            "18000 lucideus\n",
            "18001 overfishing\n",
            "18002 churches\n",
            "18003 290\n",
            "18004 mmwave\n",
            "18005 byoc\n",
            "18006 glamourise\n",
            "18007 oversell\n",
            "18008 westgarth\n",
            "18009 kilometers\n",
            "18010 1930\n",
            "18011 servicing\n",
            "18012 khilnani\n",
            "18013 u1realmerealme\n",
            "18014 saleopporealme\n",
            "18015 offersrealme\n",
            "18016 3gbrealme\n",
            "18017 pedestal\n",
            "18018 4384\n",
            "18019 singledigit\n",
            "18020 amp\n",
            "18021 tutorials\n",
            "18022 showered\n",
            "18023 bjpsenarpi\n",
            "18024 partybjpshiv\n",
            "18025 erik\n",
            "18026 186g\n",
            "18027 cortex\n",
            "18028 3927564\n",
            "18029 1178715\n",
            "18030 luckily\n",
            "18031 anandan\n",
            "18032 transact\n",
            "18033 wifigoogle\n",
            "18034 fiber\n",
            "18035 secondhighest\n",
            "18036 hdrevanna\n",
            "18037 bodys\n",
            "18038 futureskills\n",
            "18039 labbased\n",
            "18040 ghoshal\n",
            "18041 albums\n",
            "18042 335\n",
            "18043 armoured\n",
            "18044 banker\n",
            "18045 saldana\n",
            "18046 designhill\n",
            "18047 bridging\n",
            "18048 banjara\n",
            "18049 territorial\n",
            "18050 onwards\n",
            "18051 6250\n",
            "18052 39141\n",
            "18053 44226\n",
            "18054 advisers\n",
            "18055 simmering\n",
            "18056 subpar\n",
            "18057 announcementmi\n",
            "18058 cutmi\n",
            "18059 adjoining\n",
            "18060 sangli\n",
            "18061 saraalikhan\n",
            "18062 evoting\n",
            "18063 topline\n",
            "18064 kaios\n",
            "18065 contributor\n",
            "18066 smartphonelike\n",
            "18067 joblessness\n",
            "18068 protracted\n",
            "18069 dairy\n",
            "18070 rundown\n",
            "18071 excludes\n",
            "18072 sitters\n",
            "18073 prominently\n",
            "18074 8400\n",
            "18075 empowering\n",
            "18076 shahid\n",
            "18077 25basis\n",
            "18078 playful\n",
            "18079 quik\n",
            "18080 usable\n",
            "18081 trais\n",
            "18082 barrage\n",
            "18083 rbc\n",
            "18084 dislodge\n",
            "18085 pilibhit\n",
            "18086 hived\n",
            "18087 592\n",
            "18088 outlines\n",
            "18089 southernmost\n",
            "18090 pronouncements\n",
            "18091 deviating\n",
            "18092 distractor\n",
            "18093 interruption\n",
            "18094 stereo\n",
            "18095 liv\n",
            "18096 aniruddha\n",
            "18097 sanju\n",
            "18098 jackman\n",
            "18099 dulhania\n",
            "18100 camerawork\n",
            "18101 naxalite\n",
            "18102 improvised\n",
            "18103 expedited\n",
            "18104 elephants\n",
            "18105 arabian\n",
            "18106 settling\n",
            "18107 surrogate\n",
            "18108 zte\n",
            "18109 degradation\n",
            "18110 imposes\n",
            "18111 overtheear\n",
            "18112 learningbased\n",
            "18113 santosh\n",
            "18114 hiniker\n",
            "18115 strides\n",
            "18116 jindal\n",
            "18117 occasional\n",
            "18118 ihs\n",
            "18119 markit\n",
            "18120 subhakeerthana\n",
            "18121 sneaking\n",
            "18122 1931\n",
            "18123 2865\n",
            "18124 3285\n",
            "18125 agp\n",
            "18126 expansionist\n",
            "18127 ipft\n",
            "18128 legitimize\n",
            "18129 shabir\n",
            "18130 countdown\n",
            "18131 700000\n",
            "18132 833\n",
            "18133 complement\n",
            "18134 therapies\n",
            "18135 cat\n",
            "18136 chalco\n",
            "18137 1960\n",
            "18138 tuberculosis\n",
            "18139 namesake\n",
            "18140 6tipad\n",
            "18141 xroneplus\n",
            "18142 octoberoneplus\n",
            "18143 leakstim\n",
            "18144 cookpete\n",
            "18145 lauapple\n",
            "18146 2apple\n",
            "18147 rfid\n",
            "18148 nielsen\n",
            "18149 354\n",
            "18150 200708\n",
            "18151 recalibration\n",
            "18152 201415\n",
            "18153 agricommodity\n",
            "18154 pupils\n",
            "18155 6477\n",
            "18156 marlena\n",
            "18157 decibel\n",
            "18158 mahagatbandhan\n",
            "18159 paswanled\n",
            "18160 kuwait\n",
            "18161 tariq\n",
            "18162 vacant\n",
            "18163 earcups\n",
            "18164 mocked\n",
            "18165 aise\n",
            "18166 logon\n",
            "18167 aapko\n",
            "18168 mud\n",
            "18169 descended\n",
            "18170 lulling\n",
            "18171 cecchini\n",
            "18172 cheers\n",
            "18173 gboard\n",
            "18174 299590\n",
            "18175 rs900\n",
            "18176 alam\n",
            "18177 headaches\n",
            "18178 adored\n",
            "18179 assignment\n",
            "18180 beck\n",
            "18181 mysterio\n",
            "18182 segmentation\n",
            "18183 2series\n",
            "18184 chartered\n",
            "18185 677\n",
            "18186 uniqueness\n",
            "18187 activa\n",
            "18188 tyres\n",
            "18189 quintessential\n",
            "18190 32500\n",
            "18191 beds\n",
            "18192 geo\n",
            "18193 vaccine\n",
            "18194 monika\n",
            "18195 completes\n",
            "18196 reeves\n",
            "18197 elton\n",
            "18198 marutis\n",
            "18199 oneeyed\n",
            "18200 dabur\n",
            "18201 storage3gb\n",
            "18202 a12bionic\n",
            "18203 backwards\n",
            "18204 yearearlier\n",
            "18205 phantom\n",
            "18206 fixture\n",
            "18207 bolt\n",
            "18208 geolocation\n",
            "18209 bent\n",
            "18210 barnum\n",
            "18211 celestials\n",
            "18212 lawsuitqualcomm\n",
            "18213 assemblers\n",
            "18214 hindered\n",
            "18215 nadia\n",
            "18216 challan\n",
            "18217 moneyjio\n",
            "18218 rechargejio\n",
            "18219 challanjio\n",
            "18220 mahamilavat\n",
            "18221 broadside\n",
            "18222 132954\n",
            "18223 133170\n",
            "18224 fascinating\n",
            "18225 forwards\n",
            "18226 34080\n",
            "18227 41000\n",
            "18228 duplication\n",
            "18229 deletion\n",
            "18230 518\n",
            "18231 lockwhatsapp\n",
            "18232 installwhatsapp\n",
            "18233 vishalshekhar\n",
            "18234 turbulence\n",
            "18235 1728\n",
            "18236 agritech\n",
            "18237 lacing\n",
            "18238 backetball\n",
            "18239 laces\n",
            "18240 adhir\n",
            "18241 chowdhury\n",
            "18242 philanthropic\n",
            "18243 brooklyn\n",
            "18244 541\n",
            "18245 856\n",
            "18246 dakshina\n",
            "18247 impunity\n",
            "18248 dimming\n",
            "18249 valuing\n",
            "18250 roes\n",
            "18251 communism\n",
            "18252 marxistled\n",
            "18253 bharath\n",
            "18254 plugs\n",
            "18255 nestl\n",
            "18256 ponnani\n",
            "18257 ganesh\n",
            "18258 moti\n",
            "18259 doongri\n",
            "18260 ghoonghat\n",
            "18261 tonesuntil\n",
            "18262 supportprime\n",
            "18263 rite\n",
            "18264 drugstore\n",
            "18265 nearnormal\n",
            "18266 rockerz\n",
            "18267 pecuniary\n",
            "18268 jurisdiction\n",
            "18269 corpus\n",
            "18270 kanchana2\n",
            "18271 balaji\n",
            "18272 bvs\n",
            "18273 feelgood\n",
            "18274 kannum\n",
            "18275 zoya\n",
            "18276 caseappleapple\n",
            "18277 xrapple\n",
            "18278 maxapple\n",
            "18279 earplugs\n",
            "18280 kickstand\n",
            "18281 boyega\n",
            "18282 27yearold\n",
            "18283 641\n",
            "18284 taping\n",
            "18285 cuoco\n",
            "18286 chuck\n",
            "18287 lorre\n",
            "18288 launchmicromax\n",
            "18289 31410\n",
            "18290 plaudits\n",
            "18291 blighted\n",
            "18292 bulges\n",
            "18293 combustible\n",
            "18294 kuala\n",
            "18295 boots\n",
            "18296 taper\n",
            "18297 655\n",
            "18298 paring\n",
            "18299 onesided\n",
            "18300 wink\n",
            "18301 deols\n",
            "18302 kottayam\n",
            "18303 norm\n",
            "18304 comiccon\n",
            "18305 robo\n",
            "18306 inaugurating\n",
            "18307 barb\n",
            "18308 curry\n",
            "18309 potato\n",
            "18310 swain\n",
            "18311 vde\n",
            "18312 aiadmkbjp\n",
            "18313 nut\n",
            "18314 affleck\n",
            "18315 insatiable\n",
            "18316 oversees\n",
            "18317 lockbox\n",
            "18318 119900\n",
            "18319 27inch\n",
            "18320 169900\n",
            "18321 tirelessly\n",
            "18322 bhilai\n",
            "18323 durg\n",
            "18324 flattened\n",
            "18325 kunbis\n",
            "18326 obsession\n",
            "18327 unorganized\n",
            "18328 dandekar\n",
            "18329 shouted\n",
            "18330 jharkhands\n",
            "18331 6851\n",
            "18332 6853\n",
            "18333 6859\n",
            "18334 saaton\n",
            "18335 seetein\n",
            "18336 449\n",
            "18337 torsekar\n",
            "18338 sidechannel\n",
            "18339 gesturesoneplus\n",
            "18340 uioneplus\n",
            "18341 leaksoneplus\n",
            "18342 gendry\n",
            "18343 myjio\n",
            "18344 purposely\n",
            "18345 deceive\n",
            "18346 alarms\n",
            "18347 aditi\n",
            "18348 venkateswara\n",
            "18349 rajrishi\n",
            "18350 cweo\n",
            "18351 wework\n",
            "18352 emc\n",
            "18353 sashi\n",
            "18354 inmobi\n",
            "18355 establishes\n",
            "18356 equates\n",
            "18357 trevor\n",
            "18358 stuntman\n",
            "18359 505\n",
            "18360 madhavan\n",
            "18361 trolled\n",
            "18362 exodus\n",
            "18363 brigade\n",
            "18364 furore\n",
            "18365 jasani\n",
            "18366 9huawei\n",
            "18367 maxsamsung\n",
            "18368 9mate\n",
            "18369 proa\n",
            "18370 juhi\n",
            "18371 anusha\n",
            "18372 homeless\n",
            "18373 bella\n",
            "18374 revered\n",
            "18375 radhikaa\n",
            "18376 sarathkumar\n",
            "18377 48th\n",
            "18378 amarkalam\n",
            "18379 instituted\n",
            "18380 familiarity\n",
            "18381 tomer\n",
            "18382 nets\n",
            "18383 marx\n",
            "18384 diagnosing\n",
            "18385 52yearold\n",
            "18386 rampaging\n",
            "18387 15day\n",
            "18388 653\n",
            "18389 moot\n",
            "18390 commensurate\n",
            "18391 cockpit\n",
            "18392 236\n",
            "18393 gopani\n",
            "18394 apsezs\n",
            "18395 fy21e\n",
            "18396 038\n",
            "18397 032\n",
            "18398 swipes\n",
            "18399 curbed\n",
            "18400 coloursdynamic\n",
            "18401 complicating\n",
            "18402 exert\n",
            "18403 sourced\n",
            "18404 bipartisan\n",
            "18405 onward\n",
            "18406 bristled\n",
            "18407 crucifixes\n",
            "18408 sundance\n",
            "18409 showrunner\n",
            "18410 nbc\n",
            "18411 prolific\n",
            "18412 imf\n",
            "18413 pses\n",
            "18414 676\n",
            "18415 492\n",
            "18416 deterrent\n",
            "18417 obamas\n",
            "18418 contingent\n",
            "18419 sonal\n",
            "18420 allari\n",
            "18421 aiassistant\n",
            "18422 dune\n",
            "18423 eqc\n",
            "18424 colony\n",
            "18425 chocolate\n",
            "18426 flickr\n",
            "18427 30odd\n",
            "18428 kalyanled\n",
            "18429 tommy\n",
            "18430 snapchatlike\n",
            "18431 sketchfab\n",
            "18432 commons\n",
            "18433 neogi\n",
            "18434 statewise\n",
            "18435 fairness\n",
            "18436 laughterq\n",
            "18437 catches\n",
            "18438 mayhem\n",
            "18439 osama\n",
            "18440 voiceover\n",
            "18441 ancient\n",
            "18442 farmiga\n",
            "18443 chandler\n",
            "18444 whitford\n",
            "18445 mehboob\n",
            "18446 5075\n",
            "18447 tokens\n",
            "18448 overstated\n",
            "18449 individuality\n",
            "18450 brendon\n",
            "18451 urie\n",
            "18452 panic\n",
            "18453 disco\n",
            "18454 pence\n",
            "18455 incowned\n",
            "18456 warnermedia\n",
            "18457 identifies\n",
            "18458 pegging\n",
            "18459 sushant\n",
            "18460 nov\n",
            "18461 hps\n",
            "18462 downdetectors\n",
            "18463 sonata\n",
            "18464 2799\n",
            "18465 webbased\n",
            "18466 cooperatives\n",
            "18467 onions\n",
            "18468 coincided\n",
            "18469 savitri\n",
            "18470 phule\n",
            "18471 lowerspec\n",
            "18472 sendall\n",
            "18473 dysfunctional\n",
            "18474 marginalized\n",
            "18475 partyrashtriya\n",
            "18476 implements\n",
            "18477 admits\n",
            "18478 sndp\n",
            "18479 1050\n",
            "18480 nonstop\n",
            "18481 wellspaced\n",
            "18482 hdd\n",
            "18483 elicited\n",
            "18484 bikas\n",
            "18485 bureaucracy\n",
            "18486 stylish\n",
            "18487 peasant\n",
            "18488 benami\n",
            "18489 potent\n",
            "18490 campaigner\n",
            "18491 280000\n",
            "18492 fiddling\n",
            "18493 985\n",
            "18494 giriraj\n",
            "18495 victor\n",
            "18496 fluctuating\n",
            "18497 kotaks\n",
            "18498 climactic\n",
            "18499 hiked\n",
            "18500 pcba\n",
            "18501 overpower\n",
            "18502 maaibaap\n",
            "18503 damper\n",
            "18504 intangible\n",
            "18505 andaman\n",
            "18506 lakshadweep\n",
            "18507 damini\n",
            "18508 bottle\n",
            "18509 nonapple\n",
            "18510 bloodshed\n",
            "18511 onboarded\n",
            "18512 sponsoring\n",
            "18513 4genabled\n",
            "18514 baheti\n",
            "18515 trendy\n",
            "18516 torres\n",
            "18517 curanderos\n",
            "18518 dutch\n",
            "18519 turbine\n",
            "18520 narrowing\n",
            "18521 moradabad\n",
            "18522 esportsleague\n",
            "18523 legendsinvictus\n",
            "18524 fnaticinvictus\n",
            "18525 gamingfnatic\n",
            "18526 gamingteam\n",
            "18527 galaxyfaker\n",
            "18528 t1lee\n",
            "18529 sangheyoklim\n",
            "18530 yohwanstarcraft\n",
            "18531 shelf\n",
            "18532 yaariyan\n",
            "18533 romania\n",
            "18534 cognizant\n",
            "18535 sundays\n",
            "18536 ubiquitous\n",
            "18537 dim\n",
            "18538 no4\n",
            "18539 cluttered\n",
            "18540 delve\n",
            "18541 identifiable\n",
            "18542 lisanti\n",
            "18543 chanda\n",
            "18544 manjhi\n",
            "18545 rows\n",
            "18546 varuna\n",
            "18547 smell\n",
            "18548 idhar\n",
            "18549 aate\n",
            "18550 barabar\n",
            "18551 hawa\n",
            "18552 bhaari\n",
            "18553 boatman\n",
            "18554 sails\n",
            "18555 historiography\n",
            "18556 instagrammable\n",
            "18557 coexistence\n",
            "18558 dissolve\n",
            "18559 validation\n",
            "18560 readiness\n",
            "18561 omissions\n",
            "18562 greenbacks\n",
            "18563 disha\n",
            "18564 patani\n",
            "18565 introduces\n",
            "18566 exposition\n",
            "18567 simons\n",
            "18568 childs\n",
            "18569 displayhuawei\n",
            "18570 deleveraged\n",
            "18571 6946\n",
            "18572 capitalising\n",
            "18573 hiatus\n",
            "18574 frontman\n",
            "18575 rightscale\n",
            "18576 multicloud\n",
            "18577 farah\n",
            "18578 motivations\n",
            "18579 minted\n",
            "18580 numero\n",
            "18581 uno\n",
            "18582 extremist\n",
            "18583 cvigil\n",
            "18584 asom\n",
            "18585 gana\n",
            "18586 fetch\n",
            "18587 laggard\n",
            "18588 reigning\n",
            "18589 hurled\n",
            "18590 iftda\n",
            "18591 meghwal\n",
            "18592 seers\n",
            "18593 saraswati\n",
            "18594 bangaon\n",
            "18595 moretz\n",
            "18596 divergent\n",
            "18597 ouriel\n",
            "18598 poison\n",
            "18599 patasani\n",
            "18600 drakht\n",
            "18601 tulsi\n",
            "18602 suyyash\n",
            "18603 vetted\n",
            "18604 insured\n",
            "18605 nametag\n",
            "18606 27990\n",
            "18607 naagin\n",
            "18608 hassanandani\n",
            "18609 pearl\n",
            "18610 wifiready\n",
            "18611 ezviz\n",
            "18612 hermes\n",
            "18613 rajvanshi\n",
            "18614 21920\n",
            "18615 fourday\n",
            "18616 guillermo\n",
            "18617 del\n",
            "18618 toro\n",
            "18619 choreography\n",
            "18620 confined\n",
            "18621 workfromhome\n",
            "18622 flew\n",
            "18623 vhs\n",
            "18624 betamax\n",
            "18625 enquiry\n",
            "18626 tahir\n",
            "18627 bhasin\n",
            "18628 gavaskar\n",
            "18629 saqib\n",
            "18630 ammy\n",
            "18631 jiiva\n",
            "18632 krishnamachari\n",
            "18633 srikkanth\n",
            "18634 sahil\n",
            "18635 kirmani\n",
            "18636 harrdy\n",
            "18637 stasis\n",
            "18638 overhead\n",
            "18639 chowkidari\n",
            "18640 5gcapable\n",
            "18641 seesaw\n",
            "18642 craving\n",
            "18643 obeisance\n",
            "18644 durgiana\n",
            "18645 amritsar\n",
            "18646 occupying\n",
            "18647 posh\n",
            "18648 navigating\n",
            "18649 boses\n",
            "18650 soldered\n",
            "18651 modelling\n",
            "18652 mozillas\n",
            "18653 firefox\n",
            "18654 clara\n",
            "18655 chipmakers\n",
            "18656 solis\n",
            "18657 cyril\n",
            "18658 yadvinder\n",
            "18659 guleria\n",
            "18660 125cc\n",
            "18661 earlyadopter\n",
            "18662 environmentally\n",
            "18663 miniature\n",
            "18664 authenticate\n",
            "18665 cbfc\n",
            "18666 discriminative\n",
            "18667 stigmatising\n",
            "18668 oza\n",
            "18669 aliyev\n",
            "18670 behavioral\n",
            "18671 simmons\n",
            "18672 renewal\n",
            "18673 nscnk\n",
            "18674 sangam\n",
            "18675 deserved\n",
            "18676 potentiality\n",
            "18677 pac\n",
            "18678 universally\n",
            "18679 quills\n",
            "18680 galaxyfold\n",
            "18681 1345crore\n",
            "18682 7407\n",
            "18683 inter\n",
            "18684 cicilline\n",
            "18685 ffo\n",
            "18686 tranche\n",
            "18687 countryside\n",
            "18688 anguish\n",
            "18689 nps\n",
            "18690 mistry\n",
            "18691 sripaada\n",
            "18692 communicating\n",
            "18693 gamingmonk\n",
            "18694 patilht\n",
            "18695 onepluss\n",
            "18696 jaspreet\n",
            "18697 7119\n",
            "18698 eventthe\n",
            "18699 huaweihuawei\n",
            "18700 6940\n",
            "18701 vfx\n",
            "18702 maam\n",
            "18703 urge\n",
            "18704 persisted\n",
            "18705 bombarded\n",
            "18706 baked\n",
            "18707 citywhere\n",
            "18708 wankaner\n",
            "18709 lalit\n",
            "18710 liya\n",
            "18711 earworn\n",
            "18712 uraizee\n",
            "18713 eyepopping\n",
            "18714 swifts\n",
            "18715 benches\n",
            "18716 verdicts\n",
            "18717 momentums\n",
            "18718 reconnect\n",
            "18719 rand\n",
            "18720 readable\n",
            "18721 2123\n",
            "18722 contemporaries\n",
            "18723 matinee\n",
            "18724 allmetal\n",
            "18725 penn\n",
            "18726 italia\n",
            "18727 ricci\n",
            "18728 disapproval\n",
            "18729 cortana\n",
            "18730 menstruating\n",
            "18731 awaas\n",
            "18732 toss\n",
            "18733 anheuserbusch\n",
            "18734 spiked\n",
            "18735 weiss\n",
            "18736 spill\n",
            "18737 fourteen\n",
            "18738 nodeal\n",
            "18739 hela\n",
            "18740 rollercoaster\n",
            "18741 specifics\n",
            "18742 eurozone\n",
            "18743 nonbanking\n",
            "18744 someones\n",
            "18745 craziest\n",
            "18746 48year\n",
            "18747 dhule\n",
            "18748 shirdi\n",
            "18749 mumbainorth\n",
            "18750 leone\n",
            "18751 hale\n",
            "18752 dissimilar\n",
            "18753 generators\n",
            "18754 oommen\n",
            "18755 bifurcated\n",
            "18756 creme\n",
            "18757 chikkodi\n",
            "18758 mgnrega\n",
            "18759 tackled\n",
            "18760 mozart\n",
            "18761 dekhen\n",
            "18762 diyan\n",
            "18763 backandforth\n",
            "18764 witty\n",
            "18765 congressmukt\n",
            "18766 showcasing\n",
            "18767 superintendent\n",
            "18768 batteryapple\n",
            "18769 incessant\n",
            "18770 chatter\n",
            "18771 excuses\n",
            "18772 boasted\n",
            "18773 5750\n",
            "18774 42tb\n",
            "18775 expelling\n",
            "18776 expulsion\n",
            "18777 chinapa\n",
            "18778 volunteering\n",
            "18779 heartstrings\n",
            "18780 italys\n",
            "18781 abpmjay\n",
            "18782 balshingkar\n",
            "18783 pots\n",
            "18784 labourers\n",
            "18785 nete\n",
            "18786 aani\n",
            "18787 ghari\n",
            "18788 socketed\n",
            "18789 philosophies\n",
            "18790 welsch\n",
            "18791 valyrian\n",
            "18792 dagger\n",
            "18793 madison\n",
            "18794 specializes\n",
            "18795 dhfls\n",
            "18796 interterminal\n",
            "18797 sinking\n",
            "18798 muscular\n",
            "18799 615\n",
            "18800 1455\n",
            "18801 96000\n",
            "18802 physics\n",
            "18803 momin\n",
            "18804 datacultr\n",
            "18805 22990\n",
            "18806 19990\n",
            "18807 peacock\n",
            "18808 paves\n",
            "18809 fabian\n",
            "18810 backdoors\n",
            "18811 softtouch\n",
            "18812 renegotiated\n",
            "18813 locate\n",
            "18814 rangrez\n",
            "18815 704\n",
            "18816 arrives\n",
            "18817 renuka\n",
            "18818 performs\n",
            "18819 olivia\n",
            "18820 corrections\n",
            "18821 tcil\n",
            "18822 examinations\n",
            "18823 hindumuslim\n",
            "18824 nutrition\n",
            "18825 enemys\n",
            "18826 enemy\n",
            "18827 obsessed\n",
            "18828 271\n",
            "18829 mumkin\n",
            "18830 namumkinmumkin\n",
            "18831 plumb\n",
            "18832 crept\n",
            "18833 salesforce\n",
            "18834 rategain\n",
            "18835 mast\n",
            "18836 kaushambi\n",
            "18837 bahraich\n",
            "18838 baddie\n",
            "18839 familial\n",
            "18840 neatly\n",
            "18841 weathering\n",
            "18842 matrimonial\n",
            "18843 launchhuawei\n",
            "18844 2280x1080\n",
            "18845 659\n",
            "18846 aggravated\n",
            "18847 electromotive\n",
            "18848 conductor\n",
            "18849 galvanometer\n",
            "18850 seasonally\n",
            "18851 issuebased\n",
            "18852 respectful\n",
            "18853 sympathizer\n",
            "18854 noor\n",
            "18855 130521\n",
            "18856 130569\n",
            "18857 trustees\n",
            "18858 obligation\n",
            "18859 wasseypur\n",
            "18860 bajrangi\n",
            "18861 bhaijaan\n",
            "18862 ikhwanis\n",
            "18863 pandits\n",
            "18864 hpcl\n",
            "18865 helming\n",
            "18866 indiaindia\n",
            "18867 importcanalys\n",
            "18868 indiatechnology\n",
            "18869 98147\n",
            "18870 98203\n",
            "18871 mozilla\n",
            "18872 dji\n",
            "18873 motivational\n",
            "18874 dmdk\n",
            "18875 conceals\n",
            "18876 baudrillards\n",
            "18877 simulacra\n",
            "18878 hyperreality\n",
            "18879 bigwigs\n",
            "18880 malcolm\n",
            "18881 17500\n",
            "18882 pohrekar\n",
            "18883 173inch\n",
            "18884 nvidias\n",
            "18885 bandxiaomi\n",
            "18886 3xiaomi\n",
            "18887 wearablesgoqii\n",
            "18888 20gear\n",
            "18889 sportgalaxy\n",
            "18890 watchsamsung\n",
            "18891 carl\n",
            "18892 50th\n",
            "18893 harsha\n",
            "18894 pdpbjp\n",
            "18895 squarish\n",
            "18896 voiceactivated\n",
            "18897 parsing\n",
            "18898 upandcoming\n",
            "18899 zomatos\n",
            "18900 298\n",
            "18901 404\n",
            "18902 loggerheads\n",
            "18903 iim\n",
            "18904 lalbhai\n",
            "18905 burdened\n",
            "18906 discom\n",
            "18907 olympians\n",
            "18908 adolescent\n",
            "18909 dealings\n",
            "18910 unknowingly\n",
            "18911 generator\n",
            "18912 broadcom\n",
            "18913 mineworkers\n",
            "18914 amcu\n",
            "18915 nonyielding\n",
            "18916 whollyowned\n",
            "18917 kidnapped\n",
            "18918 chopped\n",
            "18919 narrator\n",
            "18920 encrypting\n",
            "18921 crisisresponse\n",
            "18922 exhaustive\n",
            "18923 effectiveness\n",
            "18924 reflections\n",
            "18925 queensland\n",
            "18926 townsville\n",
            "18927 magicpin\n",
            "18928 maval\n",
            "18929 girigosavi\n",
            "18930 mahadik\n",
            "18931 educators\n",
            "18932 childcare\n",
            "18933 sanitation\n",
            "18934 essentials\n",
            "18935 182\n",
            "18936 shiny\n",
            "18937 geofencing\n",
            "18938 axing\n",
            "18939 kandukondain\n",
            "18940 762\n",
            "18941 24185\n",
            "18942 plummeted\n",
            "18943 21320\n",
            "18944 shetkari\n",
            "18945 sanghatana\n",
            "18946 georgetown\n",
            "18947 raging\n",
            "18948 pallav\n",
            "18949 bravia\n",
            "18950 nambiar\n",
            "18951 feigeavengers\n",
            "18952 humbucker\n",
            "18953 amplifiers\n",
            "18954 oppenheimer\n",
            "18955 nehruvian\n",
            "18956 charlotte\n",
            "18957 carrie\n",
            "18958 statistacom\n",
            "18959 bleached\n",
            "18960 deodorised\n",
            "18961 palmolein\n",
            "18962 acceptance\n",
            "18963 winke\n",
            "18964 discarding\n",
            "18965 soyoil\n",
            "18966 jons\n",
            "18967 3333\n",
            "18968 unraveling\n",
            "18969 nude\n",
            "18970 caller\n",
            "18971 chowdhary\n",
            "18972 h9i\n",
            "18973 pioneers\n",
            "18974 prosperous\n",
            "18975 xls\n",
            "18976 workday\n",
            "18977 cream\n",
            "18978 gadekar\n",
            "18979 hatkanangale\n",
            "18980 shirur\n",
            "18981 rocks\n",
            "18982 tvoneplus\n",
            "18983 confession\n",
            "18984 conclave\n",
            "18985 medisales\n",
            "18986 oneliners\n",
            "18987 sikandar\n",
            "18988 rubs\n",
            "18989 ignition\n",
            "18990 substances\n",
            "18991 spontaneously\n",
            "18992 ignite\n",
            "18993 cigarette\n",
            "18994 flint\n",
            "18995 gunpowder\n",
            "18996 andwell\n",
            "18997 jumps\n",
            "18998 luggage\n",
            "18999 raiser\n",
            "19000 phobias\n",
            "19001 marcus\n",
            "19002 epfr\n",
            "19003 bhamre\n",
            "19004 citicorp\n",
            "19005 manglunia\n",
            "19006 dedensification\n",
            "19007 transitoriented\n",
            "19008 reverence\n",
            "19009 airtelairtel\n",
            "19010 invit\n",
            "19011 quadcore\n",
            "19012 daychristmas\n",
            "19013 giftschristmas\n",
            "19014 ipadamazon\n",
            "19015 watchfossil\n",
            "19016 giting\n",
            "19017 omr\n",
            "19018 muchawaited\n",
            "19019 dashing\n",
            "19020 15588\n",
            "19021 14983\n",
            "19022 elusive\n",
            "19023 notional\n",
            "19024 militarygrade\n",
            "19025 bds2\n",
            "19026 intercaste\n",
            "19027 rabbit\n",
            "19028 timecrafters\n",
            "19029 eyelid\n",
            "19030 41100\n",
            "19031 intimidated\n",
            "19032 palghar\n",
            "19033 restoration\n",
            "19034 treats\n",
            "19035 humanmachine\n",
            "19036 tanu\n",
            "19037 weds\n",
            "19038 kunder\n",
            "19039 dard\n",
            "19040 daruvala\n",
            "19041 mavani\n",
            "19042 cps\n",
            "19043 wadia\n",
            "19044 llamas\n",
            "19045 nyays\n",
            "19046 lall\n",
            "19047 undercuts\n",
            "19048 ortiz\n",
            "19049 prajapati\n",
            "19050 memnagar\n",
            "19051 ppi\n",
            "19052 lonely\n",
            "19053 banalata\n",
            "19054 ghoshs\n",
            "19055 unmarried\n",
            "19056 borderline\n",
            "19057 grandalliance\n",
            "19058 dany\n",
            "19059 transorg\n",
            "19060 clonizo\n",
            "19061 behaviours\n",
            "19062 oratory\n",
            "19063 palshikar\n",
            "19064 meng\n",
            "19065 evenly\n",
            "19066 overlooks\n",
            "19067 elimination\n",
            "19068 bestever\n",
            "19069 upstage\n",
            "19070 yorksan\n",
            "19071 choicewithout\n",
            "19072 28core\n",
            "19073 xeon\n",
            "19074 w3175x\n",
            "19075 2in1s\n",
            "19076 conquerors\n",
            "19077 warhammer\n",
            "19078 220fps\n",
            "19079 deserving\n",
            "19080 shebang\n",
            "19081 4380mah\n",
            "19082 batteryone\n",
            "19083 360x360\n",
            "19084 230mah\n",
            "19085 waterresistance\n",
            "19086 24g\n",
            "19087 50meter\n",
            "19088 airpodswireless\n",
            "19089 qicharging\n",
            "19090 chillr\n",
            "19091 localisation\n",
            "19092 outlookcut\n",
            "19093 inception\n",
            "19094 whove\n",
            "19095 fat\n",
            "19096 prospectus\n",
            "19097 unnikrishnans\n",
            "19098 noufal\n",
            "19099 dulquers\n",
            "19100 charms\n",
            "19101 forcefully\n",
            "19102 lallus\n",
            "19103 unambitious\n",
            "19104 fissuring\n",
            "19105 competencies\n",
            "19106 taskrabbit\n",
            "19107 upwork\n",
            "19108 freelancercom\n",
            "19109 workersdelivery\n",
            "19110 cleaners\n",
            "19111 autonomythey\n",
            "19112 formalization\n",
            "19113 4550\n",
            "19114 1113\n",
            "19115 classrooms\n",
            "19116 datadriven\n",
            "19117 estonia\n",
            "19118 coding\n",
            "19119 rapidlyand\n",
            "19120 teething\n",
            "19121 canara\n",
            "19122 fy1518\n",
            "19123 spv\n",
            "19124 assetsale\n",
            "19125 1705\n",
            "19126 amrutlal\n",
            "19127 technoforge\n",
            "19128 roller\n",
            "19129 bearings\n",
            "19130 dries\n",
            "19131 monani\n",
            "19132 aji\n",
            "19133 kalaria\n",
            "19134 pratapgarh\n",
            "19135 naamdar\n",
            "19136 starstruck\n",
            "19137 panayiotous\n",
            "19138 itanthony\n",
            "19139 emotionsanthony\n",
            "19140 endingsanthony\n",
            "19141 frictionjoe\n",
            "19142 areanthony\n",
            "19143 analytic\n",
            "19144 duopoly\n",
            "19145 3qfy19\n",
            "19146 3235\n",
            "19147 abreast\n",
            "19148 rajas\n",
            "19149 revising\n",
            "19150 12months\n",
            "19151 adwares\n",
            "19152 muchtouted\n",
            "19153 lair\n",
            "19154 chavda\n",
            "19155 koli\n",
            "19156 jasdan\n",
            "19157 kunvarji\n",
            "19158 bavalia\n",
            "19159 unjha\n",
            "19160 turk\n",
            "19161 thakore\n",
            "19162 dhavalsinh\n",
            "19163 zala\n",
            "19164 reversalsevere\n",
            "19165 blows\n",
            "19166 gstare\n",
            "19167 keshubhai\n",
            "19168 modadaar\n",
            "19169 trickles\n",
            "19170 nondescript\n",
            "19171 kadegi\n",
            "19172 cheekbone\n",
            "19173 allos\n",
            "19174 subsystem\n",
            "19175 ims\n",
            "19176 nodes\n",
            "19177 preintegrated\n",
            "19178 validated\n",
            "19179 zac\n",
            "19180 detach\n",
            "19181 wicked\n",
            "19182 vile\n",
            "19183 biographical\n",
            "19184 berlinger\n",
            "19185 oppositionthe\n",
            "19186 congresswill\n",
            "19187 tele\n",
            "19188 midtones\n",
            "19189 airs\n",
            "19190 leanings\n",
            "19191 traveled\n",
            "19192 purposebuilt\n",
            "19193 chariots\n",
            "19194 sher\n",
            "19195 telli\n",
            "19196 ghagounda\n",
            "19197 sunder\n",
            "19198 299300\n",
            "19199 310\n",
            "19200 bei\n",
            "19201 evermore\n",
            "19202 quickest\n",
            "19203 uniquelooking\n",
            "19204 laughoutloud\n",
            "19205 eurotrip\n",
            "19206 entangled\n",
            "19207 technologyit\n",
            "19208 controllera\n",
            "19209 countrymans\n",
            "19210 headup\n",
            "19211 markers\n",
            "19212 carplaythese\n",
            "19213 extras\n",
            "19214 chatlike\n",
            "19215 whatsoever\n",
            "19216 waking\n",
            "19217 fy20despite\n",
            "19218 outlookis\n",
            "19219 celebs\n",
            "19220 fleeing\n",
            "19221 831\n",
            "19222 sciencebacked\n",
            "19223 traits\n",
            "19224 burnout\n",
            "19225 yourstruly\n",
            "19226 premiering\n",
            "19227 uridashi\n",
            "19228 bichirs\n",
            "19229 cuento\n",
            "19230 circo\n",
            "19231 loco\n",
            "19232 belated\n",
            "19233 afterthought\n",
            "19234 majboot\n",
            "19235 977\n",
            "19236 3013\n",
            "19237 822\n",
            "19238 laughing\n",
            "19239 archanas\n",
            "19240 briganzamaking\n",
            "19241 familyfocused\n",
            "19242 banknotes\n",
            "19243 gsts\n",
            "19244 cmies\n",
            "19245 vyas\n",
            "19246 farzana\n",
            "19247 nuren\n",
            "19248 nesas\n",
            "19249 embroidering\n",
            "19250 embroidery\n",
            "19251 dramas\n",
            "19252 straining\n",
            "19253 cooperating\n",
            "19254 vladimir\n",
            "19255 putin\n",
            "19256 restrained\n",
            "19257 radical\n",
            "19258 jeddah\n",
            "19259 provoked\n",
            "19260 riyadh\n",
            "19261 screenmy\n",
            "19262 actorearlier\n",
            "19263 skirt\n",
            "19264 makeup\n",
            "19265 jokers\n",
            "19266 loveinterest\n",
            "19267 ayurveda\n",
            "19268 modii\n",
            "19269 cycleeven\n",
            "19270 gujaratwhen\n",
            "19271 loudakshay\n",
            "19272 peon\n",
            "19273 secretariat\n",
            "19274 kurta\n",
            "19275 districtsjhansi\n",
            "19276 mahoba\n",
            "19277 chitrakoot\n",
            "19278 whatsappgold\n",
            "19279 guptaht\n",
            "19280 fategrand\n",
            "19281 162\n",
            "19282 314\n",
            "19283 csi\n",
            "19284 tallied\n",
            "19285 2432\n",
            "19286 15month\n",
            "19287 2377\n",
            "19288 yielding\n",
            "19289 2461\n",
            "19290 stampede\n",
            "19291 longerdated\n",
            "19292 dispelling\n",
            "19293 hirokazu\n",
            "19294 kabeya\n",
            "19295 daiwa\n",
            "19296 singlefamily\n",
            "19297 akira\n",
            "19298 takei\n",
            "19299 antirightwing\n",
            "19300 shaved\n",
            "19301 mustache\n",
            "19302 industrythree\n",
            "19303 beep\n",
            "19304 settopbox\n",
            "19305 crosscutting\n",
            "19306 powerfully\n",
            "19307 scepticism\n",
            "19308 concur\n",
            "19309 obedient\n",
            "19310 inequalities\n",
            "19311 defer\n",
            "19312 illustrating\n",
            "19313 hierarchies\n",
            "19314 twofifths\n",
            "19315 classbased\n",
            "19316 insecurity\n",
            "19317 selfdescription\n",
            "19318 eludes\n",
            "19319 abolishing\n",
            "19320 vegetarians\n",
            "19321 nonvegetarians\n",
            "19322 evelyn\n",
            "19323 12mm\n",
            "19324 129gb\n",
            "19325 archrival\n",
            "19326 supremos\n",
            "19327 nonstandalone\n",
            "19328 5gready\n",
            "19329 xmm\n",
            "19330 8160\n",
            "19331 kiara\n",
            "19332 kalankvarun\n",
            "19333 zafarin\n",
            "19334 raghus\n",
            "19335 dixitnishant\n",
            "19336 bhuse\n",
            "19337 lits\n",
            "19338 ngemovies\n",
            "19339 asusual\n",
            "19340 epickaran\n",
            "19341 jus\n",
            "19342 partsjournalist\n",
            "19343 kalanksophie\n",
            "19344 tku\n",
            "19345 raiisonai\n",
            "19346 anthemwide\n",
            "19347 releasekalank\n",
            "19348 projectsan\n",
            "19349 budgetwill\n",
            "19350 sukumar\n",
            "19351 nanduri\n",
            "19352 constables\n",
            "19353 hamlet\n",
            "19354 ceased\n",
            "19355 researches\n",
            "19356 displacements\n",
            "19357 sethu\n",
            "19358 surekha\n",
            "19359 59yearold\n",
            "19360 gynaecologist\n",
            "19361 cellphone\n",
            "19362 watchdogs\n",
            "19363 misrepresented\n",
            "19364 abducting\n",
            "19365 webcasting\n",
            "19366 videographed\n",
            "19367 50times\n",
            "19368 impersonation\n",
            "19369 firsthand\n",
            "19370 humancentric\n",
            "19371 fornite\n",
            "19372 hexagonal\n",
            "19373 reconstruction\n",
            "19374 repealed\n",
            "19375 aliabbaszafar\n",
            "19376 emteks\n",
            "19377 paresh\n",
            "19378 romaniabased\n",
            "19379 amazonowned\n",
            "19380 doorbell\n",
            "19381 florian\n",
            "19382 schaub\n",
            "19383 assumption\n",
            "19384 prawer\n",
            "19385 jhabvala\n",
            "19386 ismail\n",
            "19387 inaudible\n",
            "19388 rat\n",
            "19389 switchoff\n",
            "19390 connectionstwo\n",
            "19391 archives\n",
            "19392 krebs\n",
            "19393 precaution\n",
            "19394 canahuati\n",
            "19395 whilst\n",
            "19396 reversionrange\n",
            "19397 v15s\n",
            "19398 alexagoogle\n",
            "19399 3huawei\n",
            "19400 proappleai\n",
            "19401 plateful\n",
            "19402 tunday\n",
            "19403 kebabs\n",
            "19404 delicacy\n",
            "19405 appraisal\n",
            "19406 chased\n",
            "19407 discreet\n",
            "19408 quip\n",
            "19409 apphailed\n",
            "19410 cabbie\n",
            "19411 uppercrust\n",
            "19412 adityanaths\n",
            "19413 sectarian\n",
            "19414 wedge\n",
            "19415 yogis\n",
            "19416 conch\n",
            "19417 touchy\n",
            "19418 promandir\n",
            "19419 shrugs\n",
            "19420 sunni\n",
            "19421 shia\n",
            "19422 codiner\n",
            "19423 startled\n",
            "19424 tactless\n",
            "19425 outoftowner\n",
            "19426 taqiya\n",
            "19427 agape\n",
            "19428 tilts\n",
            "19429 hunch\n",
            "19430 cutapple\n",
            "19431 crisisapple\n",
            "19432 maxsmartphone\n",
            "19433 buffeted\n",
            "19434 rajgurus\n",
            "19435 17yearold\n",
            "19436 nagraj\n",
            "19437 manjules\n",
            "19438 64yearold\n",
            "19439 serampore\n",
            "19440 oust\n",
            "19441 peoplehad\n",
            "19442 arrears\n",
            "19443 dearness\n",
            "19444 womenonly\n",
            "19445 interviewer\n",
            "19446 interstitial\n",
            "19447 filmography\n",
            "19448 terse\n",
            "19449 prod\n",
            "19450 1973s\n",
            "19451 directoractor\n",
            "19452 mindspace\n",
            "19453 annapurna\n",
            "19454 jvmp\n",
            "19455 subodh\n",
            "19456 upposters\n",
            "19457 spotbehind\n",
            "19458 nightclub\n",
            "19459 raindrenched\n",
            "19460 halftime\n",
            "19461 newberg\n",
            "19462 364\n",
            "19463 favorable\n",
            "19464 tradein\n",
            "19465 coax\n",
            "19466 tethered\n",
            "19467 usmovie\n",
            "19468 ethane\n",
            "19469 mitsui\n",
            "19470 osk\n",
            "19471 multiples\n",
            "19472 vee\n",
            "19473 cme\n",
            "19474 onerous\n",
            "19475 abominable\n",
            "19476 awakened\n",
            "19477 possesses\n",
            "19478 pointy\n",
            "19479 darwin\n",
            "19480 foe\n",
            "19481 hitachi\n",
            "19482 10am\n",
            "19483 aldani\n",
            "19484 unamanned\n",
            "19485 mahmood\n",
            "19486 pranav\n",
            "19487 bezhalel\n",
            "19488 machlis\n",
            "19489 rajavanshi\n",
            "19490 delegates\n",
            "19491 fireravaged\n",
            "19492 inferno\n",
            "19493 spire\n",
            "19494 denver\n",
            "19495 tempe\n",
            "19496 133inch\n",
            "19497 suspectsan\n",
            "19498 solidstate\n",
            "19499 equator\n",
            "19500 securelynot\n",
            "19501 centimeter\n",
            "19502 dustsized\n",
            "19503 stealth\n",
            "19504 transmitters\n",
            "19505 receivers\n",
            "19506 ursi\n",
            "19507 aprasc\n",
            "19508 besties\n",
            "19509 madethe\n",
            "19510 pearlish\n",
            "19511 proclaimed\n",
            "19512 vapourchamber\n",
            "19513 overheated\n",
            "19514 warmth\n",
            "19515 identitymint\n",
            "19516 intricately\n",
            "19517 hanks\n",
            "19518 ryans\n",
            "19519 handmedown\n",
            "19520 youdespite\n",
            "19521 lipped\n",
            "19522 endgameavengers\n",
            "19523 ferrying\n",
            "19524 sharif\n",
            "19525 eateries\n",
            "19526 boe\n",
            "19527 756inch\n",
            "19528 flexpaiworlds\n",
            "19529 priceflexpai\n",
            "19530 flywheel\n",
            "19531 linings\n",
            "19532 lawsuits\n",
            "19533 viciously\n",
            "19534 1737\n",
            "19535 3794\n",
            "19536 seeding\n",
            "19537 10000mah\n",
            "19538 seperately\n",
            "19539 imx481\n",
            "19540 magnetically\n",
            "19541 eraser\n",
            "19542 reviewapple\n",
            "19543 anemic\n",
            "19544 frontline\n",
            "19545 busted\n",
            "19546 kuttichathan\n",
            "19547 jijo\n",
            "19548 punnooses\n",
            "19549 gamas\n",
            "19550 kannamthanam\n",
            "19551 ernakulamkerala\n",
            "19552 jaswantsinh\n",
            "19553 bhabhor\n",
            "19554 dahodgujarat\n",
            "19555 mainpuriuttar\n",
            "19556 dhubriassam\n",
            "19557 86year\n",
            "19558 dosa\n",
            "19559 criticizes\n",
            "19560 24mp10mp5mp\n",
            "19561 nonremoval\n",
            "19562 coloursblue\n",
            "19563 coders\n",
            "19564 swoop\n",
            "19565 trustreit\n",
            "19566 bolstering\n",
            "19567 106577\n",
            "19568 threecamera\n",
            "19569 laxman\n",
            "19570 lapped\n",
            "19571 ferrari\n",
            "19572 overin\n",
            "19573 adage\n",
            "19574 inducting\n",
            "19575 globals\n",
            "19576 hmds\n",
            "19577 599inches\n",
            "19578 wheelchair\n",
            "19579 marijuana\n",
            "19580 trian\n",
            "19581 muskaan\n",
            "19582 vows\n",
            "19583 kasam\n",
            "19584 quashed\n",
            "19585 remanded\n",
            "19586 respondent\n",
            "19587 moneymaking\n",
            "19588 conversing\n",
            "19589 thinkoutloud\n",
            "19590 thoughtful\n",
            "19591 storybook\n",
            "19592 monologue\n",
            "19593 kiss\n",
            "19594 mathurs\n",
            "19595 odinwhen\n",
            "19596 odin\n",
            "19597 underused\n",
            "19598 cryosleep\n",
            "19599 humanoid\n",
            "19600 lawenforcement\n",
            "19601 castbarring\n",
            "19602 broadsides\n",
            "19603 astute\n",
            "19604 80s\n",
            "19605 1966\n",
            "19606 sonofthesoil\n",
            "19607 reckoned\n",
            "19608 brotherwinning\n",
            "19609 conventionally\n",
            "19610 machineterrence\n",
            "19611 vanko\n",
            "19612 villainsmcu\n",
            "19613 vexing\n",
            "19614 oral\n",
            "19615 grosses\n",
            "19616 schindlers\n",
            "19617 disturbance\n",
            "19618 paidfor\n",
            "19619 factset\n",
            "19620 inquiries\n",
            "19621 streamingvideo\n",
            "19622 nongaming\n",
            "19623 disciplined\n",
            "19624 bajpai\n",
            "19625 kadbet\n",
            "19626 marathons\n",
            "19627 siddiquis\n",
            "19628 pvr\n",
            "19629 ysrc\n",
            "19630 samiti\n",
            "19631 gunadeep\n",
            "19632 secondyear\n",
            "19633 restarts\n",
            "19634 freezes\n",
            "19635 roxy\n",
            "19636 upbringings\n",
            "19637 flannery\n",
            "19638 oconnor\n",
            "19639 netflixhosted\n",
            "19640 fysee\n",
            "19641 nostalgic\n",
            "19642 seminars\n",
            "19643 itis\n",
            "19644 soods\n",
            "19645 tongue\n",
            "19646 stigma\n",
            "19647 languageshindi\n",
            "19648 haryanvi\n",
            "19649 rajasthani\n",
            "19650 22000\n",
            "19651 dearer\n",
            "19652 furthering\n",
            "19653 narasupra\n",
            "19654 25200\n",
            "19655 wanda\n",
            "19656 olsen\n",
            "19657 spook\n",
            "19658 fockeline\n",
            "19659 ouwerkerk\n",
            "19660 valk\n",
            "19661 katja\n",
            "19662 esmee\n",
            "19663 kampen\n",
            "19664 kasper\n",
            "19665 greidanus\n",
            "19666 edgar\n",
            "19667 romano\n",
            "19668 fulfilment\n",
            "19669 lifeaffirming\n",
            "19670 11800\n",
            "19671 vibes\n",
            "19672 flesh\n",
            "19673 padmaavat\n",
            "19674 alauddin\n",
            "19675 khilji\n",
            "19676 nominating\n",
            "19677 flipflop\n",
            "19678 sarkale\n",
            "19679 579\n",
            "19680 42500\n",
            "19681 629\n",
            "19682 7thgen\n",
            "19683 samaritans\n",
            "19684 momentspublic\n",
            "19685 beefier\n",
            "19686 6150\n",
            "19687 36776\n",
            "19688 37345921\n",
            "19689 25769\n",
            "19690 26304709\n",
            "19691 hdfcs\n",
            "19692 505242\n",
            "19693 33990642\n",
            "19694 366239\n",
            "19695 62001567\n",
            "19696 movieclint\n",
            "19697 bartonhawkeye\n",
            "19698 archer\n",
            "19699 anticorruption\n",
            "19700 basti\n",
            "19701 screenplays\n",
            "19702 denis\n",
            "19703 villeneuves\n",
            "19704 sicario\n",
            "19705 viber\n",
            "19706 gambling\n",
            "19707 gadar\n",
            "19708 maafi\n",
            "19709 indebtedness\n",
            "19710 m1s\n",
            "19711 successorthe\n",
            "19712 nirauha\n",
            "19713 shakya\n",
            "19714 jadun\n",
            "19715 patriarchs\n",
            "19716 partylohia\n",
            "19717 periscope\n",
            "19718 50x\n",
            "19719 shanimol\n",
            "19720 usman\n",
            "19721 adoor\n",
            "19722 organisational\n",
            "19723 threeyearold\n",
            "19724 disruptors\n",
            "19725 sipping\n",
            "19726 pings\n",
            "19727 cornerits\n",
            "19728 getaway\n",
            "19729 scenic\n",
            "19730 replicates\n",
            "19731 excursions\n",
            "19732 checkin\n",
            "19733 voila\n",
            "19734 dhaka\n",
            "19735 sensitise\n",
            "19736 frro\n",
            "19737 260km\n",
            "19738 khaleda\n",
            "19739 rust\n",
            "19740 indiana\n",
            "19741 kentucky\n",
            "19742 lobbies\n",
            "19743 interpret\n",
            "19744 cashierless\n",
            "19745 instore\n",
            "19746 workforces\n",
            "19747 manning\n",
            "19748 harpreet\n",
            "19749 mansukhani\n",
            "19750 speechesa\n",
            "19751 upbraid\n",
            "19752 aryama\n",
            "19753 sundaram\n",
            "19754 contradictory\n",
            "19755 misleading\n",
            "19756 gleaned\n",
            "19757 harmed\n",
            "19758 6005\n",
            "19759 3597406\n",
            "19760 cured\n",
            "19761 ailment\n",
            "19762 freeputting\n",
            "19763 rishis\n",
            "19764 66yearold\n",
            "19765 chintu\n",
            "19766 freefollowing\n",
            "19767 rawails\n",
            "19768 tezz\n",
            "19769 akshai\n",
            "19770 karda\n",
            "19771 dhupias\n",
            "19772 6918\n",
            "19773 6912\n",
            "19774 6921\n",
            "19775 7367\n",
            "19776 storefront\n",
            "19777 amazoncoms\n",
            "19778 itenabled\n",
            "19779 biotech\n",
            "19780 reinforcement\n",
            "19781 armchair\n",
            "19782 millionaires\n",
            "19783 freshers\n",
            "19784 nilekani\n",
            "19785 kris\n",
            "19786 gopalakrishnan\n",
            "19787 mazumdarshaw\n",
            "19788 bpac\n",
            "19789 productivitywe\n",
            "19790 b2c\n",
            "19791 lovehate\n",
            "19792 mananthony\n",
            "19793"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4g2PvRNn8fg"
      },
      "source": [
        "import tensorflow as tf\n",
        "embedding_layer_STORY =Embedding(num_words_STORY,300,weights=[embedding_matrix],input_length=MAX_SEQUENCE_LENGTH_STORY,trainable=False)\n",
        "sequence_input_STORY = tf.keras.Input(shape=(MAX_SEQUENCE_LENGTH_STORY,))\n",
        "embedded_sequences_STORY = embedding_layer_STORY(sequence_input_STORY)\n",
        "a = LSTM(50)(embedded_sequences_STORY)\n",
        "#a = Bidirectional(LSTM(50, activation='relu'))(embedded_sequences_STORY)\n",
        "a = Flatten()(a)\n",
        "a = Dense(512,activation='relu')(a)\n",
        "a = Dense(256,activation='relu')(a)\n",
        "a = Dropout(0.2)(a)\n",
        "a = Dense(64,activation='relu')(a)\n",
        "a = Dense(4,activation='softmax')(a)\n",
        "a = Model(inputs=sequence_input_STORY, outputs=a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VJloaiFrgwx",
        "outputId": "6f879c34-ff7c-462e-e126-3447c2ee19ee"
      },
      "source": [
        "a.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_10 (InputLayer)        [(None, 474)]             0         \n",
            "_________________________________________________________________\n",
            "embedding_10 (Embedding)     (None, 474, 300)          3000000   \n",
            "_________________________________________________________________\n",
            "bidirectional_18 (Bidirectio (None, 100)               140400    \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 4)                 404       \n",
            "=================================================================\n",
            "Total params: 3,140,804\n",
            "Trainable params: 140,804\n",
            "Non-trainable params: 3,000,000\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPsVxvHcx9LS"
      },
      "source": [
        "#Callbacks\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "filepath_1=\"best_model_1.hdf5\"\n",
        "filepath_2=\"best_model_2.hdf5\"\n",
        "filepath_3=\"best_model_3.hdf5\"\n",
        "\n",
        "checkpoint_1 = ModelCheckpoint(filepath_1, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "checkpoint_2 = ModelCheckpoint(filepath_2, monitor='rmse', verbose=1, save_best_only=True, mode='min')\n",
        "checkpoint_3 = ModelCheckpoint(filepath_3, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "es = EarlyStopping(monitor='val_acc', mode='max',patience=100)\n",
        "es2 = EarlyStopping(monitor='rmse', mode='min',patience=5)\n",
        "\n",
        "\n",
        "callbacks_list_1 = [checkpoint_1,es]\n",
        "callbacks_list_2 = [checkpoint_2,es2]\n",
        "callbacks_list_3 = [checkpoint_3,es]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFMfjtgOoOem",
        "outputId": "4051f692-f61f-4de2-bdfe-1a50e4aaf105"
      },
      "source": [
        "opt = tf.keras.optimizers.Adam(learning_rate=1e-3, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "a.compile(loss=\"categorical_crossentropy\", optimizer= opt ,metrics=['acc'])\n",
        "a.fit(x_train_STORY,y_train_oh,epochs=100, batch_size=256,\n",
        "      validation_data=(x_val_STORY, y_val_oh),callbacks=[callbacks_list_1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.0011 - acc: 0.9998\n",
            "Epoch 00001: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 96ms/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.4834 - val_acc: 0.9669\n",
            "Epoch 2/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.0019 - acc: 0.9992\n",
            "Epoch 00002: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 76ms/step - loss: 0.0019 - acc: 0.9992 - val_loss: 0.5177 - val_acc: 0.9649\n",
            "Epoch 3/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.0043 - acc: 0.9988\n",
            "Epoch 00003: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 77ms/step - loss: 0.0049 - acc: 0.9987 - val_loss: 0.5223 - val_acc: 0.9636\n",
            "Epoch 4/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.0056 - acc: 0.9990\n",
            "Epoch 00004: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 0.0055 - acc: 0.9990 - val_loss: 0.4295 - val_acc: 0.9669\n",
            "Epoch 5/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 8.6683e-04 - acc: 0.9995\n",
            "Epoch 00005: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 77ms/step - loss: 8.4570e-04 - acc: 0.9995 - val_loss: 0.4496 - val_acc: 0.9656\n",
            "Epoch 6/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 5.8389e-04 - acc: 0.9998\n",
            "Epoch 00006: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 77ms/step - loss: 9.1885e-04 - acc: 0.9997 - val_loss: 0.4442 - val_acc: 0.9676\n",
            "Epoch 7/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 5.8778e-04 - acc: 0.9997\n",
            "Epoch 00007: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 8.5338e-04 - acc: 0.9995 - val_loss: 0.4588 - val_acc: 0.9669\n",
            "Epoch 8/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.0095 - acc: 0.9975\n",
            "Epoch 00008: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 77ms/step - loss: 0.0093 - acc: 0.9975 - val_loss: 0.4385 - val_acc: 0.9596\n",
            "Epoch 9/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.0060 - acc: 0.9983\n",
            "Epoch 00009: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 77ms/step - loss: 0.0060 - acc: 0.9982 - val_loss: 0.3973 - val_acc: 0.9623\n",
            "Epoch 10/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.0012 - acc: 0.9995\n",
            "Epoch 00010: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 77ms/step - loss: 0.0012 - acc: 0.9995 - val_loss: 0.4056 - val_acc: 0.9636\n",
            "Epoch 11/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 7.0232e-04 - acc: 0.9997\n",
            "Epoch 00011: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 77ms/step - loss: 6.8816e-04 - acc: 0.9997 - val_loss: 0.4273 - val_acc: 0.9623\n",
            "Epoch 12/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.6873e-04 - acc: 0.9998\n",
            "Epoch 00012: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.5709e-04 - acc: 0.9998 - val_loss: 0.4305 - val_acc: 0.9629\n",
            "Epoch 13/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.9329e-04 - acc: 0.9998\n",
            "Epoch 00013: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 77ms/step - loss: 4.8108e-04 - acc: 0.9998 - val_loss: 0.4318 - val_acc: 0.9636\n",
            "Epoch 14/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 3.5964e-04 - acc: 0.9998\n",
            "Epoch 00014: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 77ms/step - loss: 3.5207e-04 - acc: 0.9998 - val_loss: 0.4346 - val_acc: 0.9643\n",
            "Epoch 15/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 3.6205e-04 - acc: 0.9998\n",
            "Epoch 00015: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.0263e-04 - acc: 0.9998 - val_loss: 0.4409 - val_acc: 0.9649\n",
            "Epoch 16/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.1236e-04 - acc: 0.9998\n",
            "Epoch 00016: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.0216e-04 - acc: 0.9998 - val_loss: 0.4487 - val_acc: 0.9649\n",
            "Epoch 17/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 3.6251e-04 - acc: 0.9998\n",
            "Epoch 00017: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 3.5442e-04 - acc: 0.9998 - val_loss: 0.4545 - val_acc: 0.9649\n",
            "Epoch 18/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.8922e-04 - acc: 0.9998\n",
            "Epoch 00018: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.7707e-04 - acc: 0.9998 - val_loss: 0.4576 - val_acc: 0.9643\n",
            "Epoch 19/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.2407e-04 - acc: 0.9998\n",
            "Epoch 00019: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 79ms/step - loss: 4.1365e-04 - acc: 0.9998 - val_loss: 0.4596 - val_acc: 0.9643\n",
            "Epoch 20/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.7537e-04 - acc: 0.9998\n",
            "Epoch 00020: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.6392e-04 - acc: 0.9998 - val_loss: 0.4613 - val_acc: 0.9629\n",
            "Epoch 21/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 3.8009e-04 - acc: 0.9998\n",
            "Epoch 00021: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 3.7054e-04 - acc: 0.9998 - val_loss: 0.4650 - val_acc: 0.9629\n",
            "Epoch 22/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 5.2931e-04 - acc: 0.9998\n",
            "Epoch 00022: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 77ms/step - loss: 5.1605e-04 - acc: 0.9998 - val_loss: 0.4676 - val_acc: 0.9636\n",
            "Epoch 23/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.4167e-04 - acc: 0.9998\n",
            "Epoch 00023: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 77ms/step - loss: 4.3060e-04 - acc: 0.9998 - val_loss: 0.4692 - val_acc: 0.9636\n",
            "Epoch 24/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.2756e-04 - acc: 0.9998\n",
            "Epoch 00024: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.1721e-04 - acc: 0.9998 - val_loss: 0.4710 - val_acc: 0.9636\n",
            "Epoch 25/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.2944e-04 - acc: 0.9998\n",
            "Epoch 00025: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 77ms/step - loss: 4.1864e-04 - acc: 0.9998 - val_loss: 0.4736 - val_acc: 0.9636\n",
            "Epoch 26/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 3.9794e-04 - acc: 0.9998\n",
            "Epoch 00026: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 3.8794e-04 - acc: 0.9998 - val_loss: 0.4759 - val_acc: 0.9636\n",
            "Epoch 27/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.3170e-04 - acc: 0.9998\n",
            "Epoch 00027: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 77ms/step - loss: 4.2084e-04 - acc: 0.9998 - val_loss: 0.4778 - val_acc: 0.9636\n",
            "Epoch 28/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 3.1957e-04 - acc: 0.9998\n",
            "Epoch 00028: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 3.4676e-04 - acc: 0.9998 - val_loss: 0.4806 - val_acc: 0.9636\n",
            "Epoch 29/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.5729e-04 - acc: 0.9998\n",
            "Epoch 00029: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.4589e-04 - acc: 0.9998 - val_loss: 0.4836 - val_acc: 0.9636\n",
            "Epoch 30/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 5.1928e-04 - acc: 0.9998\n",
            "Epoch 00030: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 77ms/step - loss: 5.0623e-04 - acc: 0.9998 - val_loss: 0.4852 - val_acc: 0.9636\n",
            "Epoch 31/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.0142e-04 - acc: 0.9998\n",
            "Epoch 00031: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 3.9132e-04 - acc: 0.9998 - val_loss: 0.4879 - val_acc: 0.9636\n",
            "Epoch 32/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.8654e-04 - acc: 0.9998\n",
            "Epoch 00032: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 77ms/step - loss: 4.7454e-04 - acc: 0.9998 - val_loss: 0.4894 - val_acc: 0.9643\n",
            "Epoch 33/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 3.9656e-04 - acc: 0.9998\n",
            "Epoch 00033: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 3.8666e-04 - acc: 0.9998 - val_loss: 0.4925 - val_acc: 0.9643\n",
            "Epoch 34/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 3.7071e-04 - acc: 0.9998\n",
            "Epoch 00034: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 77ms/step - loss: 3.6138e-04 - acc: 0.9998 - val_loss: 0.4953 - val_acc: 0.9643\n",
            "Epoch 35/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 3.8333e-04 - acc: 0.9998\n",
            "Epoch 00035: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 3.8333e-04 - acc: 0.9998 - val_loss: 0.4992 - val_acc: 0.9649\n",
            "Epoch 36/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.4706e-04 - acc: 0.9998\n",
            "Epoch 00036: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.3582e-04 - acc: 0.9998 - val_loss: 0.4996 - val_acc: 0.9643\n",
            "Epoch 37/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.3989e-04 - acc: 0.9998\n",
            "Epoch 00037: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 77ms/step - loss: 4.2884e-04 - acc: 0.9998 - val_loss: 0.5013 - val_acc: 0.9643\n",
            "Epoch 38/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.0613e-04 - acc: 0.9998\n",
            "Epoch 00038: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 77ms/step - loss: 3.9610e-04 - acc: 0.9998 - val_loss: 0.5036 - val_acc: 0.9643\n",
            "Epoch 39/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 3.8377e-04 - acc: 0.9998\n",
            "Epoch 00039: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 3.7414e-04 - acc: 0.9998 - val_loss: 0.5056 - val_acc: 0.9643\n",
            "Epoch 40/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.1677e-04 - acc: 0.9998\n",
            "Epoch 00040: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.0631e-04 - acc: 0.9998 - val_loss: 0.5130 - val_acc: 0.9636\n",
            "Epoch 41/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 3.3070e-04 - acc: 0.9998\n",
            "Epoch 00041: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 3.2238e-04 - acc: 0.9998 - val_loss: 0.5174 - val_acc: 0.9636\n",
            "Epoch 42/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.5332e-04 - acc: 0.9998\n",
            "Epoch 00042: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.4196e-04 - acc: 0.9998 - val_loss: 0.5211 - val_acc: 0.9636\n",
            "Epoch 43/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 5.0182e-04 - acc: 0.9998\n",
            "Epoch 00043: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 5.0182e-04 - acc: 0.9998 - val_loss: 0.5198 - val_acc: 0.9643\n",
            "Epoch 44/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.6353e-04 - acc: 0.9998\n",
            "Epoch 00044: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 77ms/step - loss: 4.5187e-04 - acc: 0.9998 - val_loss: 0.5200 - val_acc: 0.9643\n",
            "Epoch 45/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 5.0536e-04 - acc: 0.9998\n",
            "Epoch 00045: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.9266e-04 - acc: 0.9998 - val_loss: 0.5215 - val_acc: 0.9649\n",
            "Epoch 46/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.2944e-04 - acc: 0.9998\n",
            "Epoch 00046: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.1865e-04 - acc: 0.9998 - val_loss: 0.5227 - val_acc: 0.9643\n",
            "Epoch 47/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 3.9490e-04 - acc: 0.9998\n",
            "Epoch 00047: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 77ms/step - loss: 3.8497e-04 - acc: 0.9998 - val_loss: 0.5239 - val_acc: 0.9643\n",
            "Epoch 48/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.4533e-04 - acc: 0.9998\n",
            "Epoch 00048: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.3413e-04 - acc: 0.9998 - val_loss: 0.5256 - val_acc: 0.9643\n",
            "Epoch 49/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 3.8881e-04 - acc: 0.9998\n",
            "Epoch 00049: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 3.8881e-04 - acc: 0.9998 - val_loss: 0.5280 - val_acc: 0.9643\n",
            "Epoch 50/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.5466e-04 - acc: 0.9998\n",
            "Epoch 00050: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.4325e-04 - acc: 0.9998 - val_loss: 0.5308 - val_acc: 0.9643\n",
            "Epoch 51/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.1240e-04 - acc: 0.9998\n",
            "Epoch 00051: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.0202e-04 - acc: 0.9998 - val_loss: 0.5329 - val_acc: 0.9643\n",
            "Epoch 52/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.6812e-04 - acc: 0.9998\n",
            "Epoch 00052: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.5635e-04 - acc: 0.9998 - val_loss: 0.5351 - val_acc: 0.9643\n",
            "Epoch 53/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.2156e-04 - acc: 0.9998\n",
            "Epoch 00053: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.1099e-04 - acc: 0.9998 - val_loss: 0.5400 - val_acc: 0.9643\n",
            "Epoch 54/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.3936e-04 - acc: 0.9998\n",
            "Epoch 00054: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.2842e-04 - acc: 0.9998 - val_loss: 0.5459 - val_acc: 0.9636\n",
            "Epoch 55/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.0641e-04 - acc: 0.9998\n",
            "Epoch 00055: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 3.9619e-04 - acc: 0.9998 - val_loss: 0.5491 - val_acc: 0.9643\n",
            "Epoch 56/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 3.3527e-04 - acc: 0.9998\n",
            "Epoch 00056: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 3.3527e-04 - acc: 0.9998 - val_loss: 0.5508 - val_acc: 0.9643\n",
            "Epoch 57/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.2148e-04 - acc: 0.9998\n",
            "Epoch 00057: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.1090e-04 - acc: 0.9998 - val_loss: 0.5528 - val_acc: 0.9643\n",
            "Epoch 58/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 3.9303e-04 - acc: 0.9998\n",
            "Epoch 00058: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 77ms/step - loss: 3.8315e-04 - acc: 0.9998 - val_loss: 0.5549 - val_acc: 0.9643\n",
            "Epoch 59/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 3.7341e-04 - acc: 0.9998\n",
            "Epoch 00059: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.2798e-04 - acc: 0.9998 - val_loss: 0.5562 - val_acc: 0.9643\n",
            "Epoch 60/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 1.6803e-04 - acc: 1.0000\n",
            "Epoch 00060: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.5594e-04 - acc: 0.9998 - val_loss: 0.5571 - val_acc: 0.9636\n",
            "Epoch 61/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 3.7973e-04 - acc: 0.9998\n",
            "Epoch 00061: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 3.7019e-04 - acc: 0.9998 - val_loss: 0.5582 - val_acc: 0.9643\n",
            "Epoch 62/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 1.6586e-04 - acc: 1.0000\n",
            "Epoch 00062: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 3.9417e-04 - acc: 0.9998 - val_loss: 0.5607 - val_acc: 0.9643\n",
            "Epoch 63/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 3.5924e-04 - acc: 0.9998\n",
            "Epoch 00063: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 77ms/step - loss: 3.5022e-04 - acc: 0.9998 - val_loss: 0.5621 - val_acc: 0.9643\n",
            "Epoch 64/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.2296e-04 - acc: 0.9998\n",
            "Epoch 00064: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.1232e-04 - acc: 0.9998 - val_loss: 0.5641 - val_acc: 0.9643\n",
            "Epoch 65/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.9435e-04 - acc: 0.9998\n",
            "Epoch 00065: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.8193e-04 - acc: 0.9998 - val_loss: 0.5644 - val_acc: 0.9643\n",
            "Epoch 66/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.9018e-04 - acc: 0.9998\n",
            "Epoch 00066: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 77ms/step - loss: 4.7785e-04 - acc: 0.9998 - val_loss: 0.5653 - val_acc: 0.9643\n",
            "Epoch 67/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.2304e-04 - acc: 0.9998\n",
            "Epoch 00067: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.1242e-04 - acc: 0.9998 - val_loss: 0.5666 - val_acc: 0.9643\n",
            "Epoch 68/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 3.5763e-04 - acc: 0.9998\n",
            "Epoch 00068: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 3.5763e-04 - acc: 0.9998 - val_loss: 0.5675 - val_acc: 0.9636\n",
            "Epoch 69/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.5287e-04 - acc: 0.9998\n",
            "Epoch 00069: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.4148e-04 - acc: 0.9998 - val_loss: 0.5691 - val_acc: 0.9643\n",
            "Epoch 70/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 3.8215e-04 - acc: 0.9998\n",
            "Epoch 00070: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 3.8215e-04 - acc: 0.9998 - val_loss: 0.5708 - val_acc: 0.9636\n",
            "Epoch 71/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.3445e-04 - acc: 0.9998\n",
            "Epoch 00071: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.2352e-04 - acc: 0.9998 - val_loss: 0.5716 - val_acc: 0.9636\n",
            "Epoch 72/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.4088e-04 - acc: 0.9998\n",
            "Epoch 00072: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.2979e-04 - acc: 0.9998 - val_loss: 0.5730 - val_acc: 0.9636\n",
            "Epoch 73/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 3.5998e-04 - acc: 0.9998\n",
            "Epoch 00073: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 3.5092e-04 - acc: 0.9998 - val_loss: 0.5740 - val_acc: 0.9636\n",
            "Epoch 74/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 3.7002e-04 - acc: 0.9998\n",
            "Epoch 00074: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 3.7002e-04 - acc: 0.9998 - val_loss: 0.5761 - val_acc: 0.9636\n",
            "Epoch 75/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.3464e-04 - acc: 0.9998\n",
            "Epoch 00075: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.2370e-04 - acc: 0.9998 - val_loss: 0.5759 - val_acc: 0.9643\n",
            "Epoch 76/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.1237e-04 - acc: 0.9998\n",
            "Epoch 00076: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.0200e-04 - acc: 0.9998 - val_loss: 0.5767 - val_acc: 0.9643\n",
            "Epoch 77/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 3.8055e-04 - acc: 0.9998\n",
            "Epoch 00077: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 77ms/step - loss: 3.8055e-04 - acc: 0.9998 - val_loss: 0.5778 - val_acc: 0.9643\n",
            "Epoch 78/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.2637e-04 - acc: 0.9998\n",
            "Epoch 00078: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.7700e-04 - acc: 0.9998 - val_loss: 0.5782 - val_acc: 0.9643\n",
            "Epoch 79/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.3168e-04 - acc: 0.9998\n",
            "Epoch 00079: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.2087e-04 - acc: 0.9998 - val_loss: 0.5796 - val_acc: 0.9649\n",
            "Epoch 80/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.2761e-04 - acc: 0.9998\n",
            "Epoch 00080: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 77ms/step - loss: 4.1685e-04 - acc: 0.9998 - val_loss: 0.5816 - val_acc: 0.9643\n",
            "Epoch 81/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.2760e-04 - acc: 0.9998\n",
            "Epoch 00081: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.1684e-04 - acc: 0.9998 - val_loss: 0.5826 - val_acc: 0.9636\n",
            "Epoch 82/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 3.3332e-04 - acc: 0.9998\n",
            "Epoch 00082: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 3.7440e-04 - acc: 0.9998 - val_loss: 0.5842 - val_acc: 0.9643\n",
            "Epoch 83/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.0410e-04 - acc: 0.9998\n",
            "Epoch 00083: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 3.9393e-04 - acc: 0.9998 - val_loss: 0.5850 - val_acc: 0.9643\n",
            "Epoch 84/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.2979e-04 - acc: 0.9998\n",
            "Epoch 00084: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.1897e-04 - acc: 0.9998 - val_loss: 0.5860 - val_acc: 0.9643\n",
            "Epoch 85/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 3.9268e-04 - acc: 0.9998\n",
            "Epoch 00085: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 3.8280e-04 - acc: 0.9998 - val_loss: 0.5875 - val_acc: 0.9649\n",
            "Epoch 86/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 4.2449e-04 - acc: 0.9998\n",
            "Epoch 00086: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.2449e-04 - acc: 0.9998 - val_loss: 0.5883 - val_acc: 0.9643\n",
            "Epoch 87/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.6095e-04 - acc: 0.9998\n",
            "Epoch 00087: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.4937e-04 - acc: 0.9998 - val_loss: 0.5894 - val_acc: 0.9643\n",
            "Epoch 88/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.2376e-04 - acc: 0.9998\n",
            "Epoch 00088: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.1310e-04 - acc: 0.9998 - val_loss: 0.5919 - val_acc: 0.9643\n",
            "Epoch 89/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.1819e-04 - acc: 0.9998\n",
            "Epoch 00089: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 77ms/step - loss: 4.0767e-04 - acc: 0.9998 - val_loss: 0.5933 - val_acc: 0.9643\n",
            "Epoch 90/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 3.3305e-04 - acc: 0.9998\n",
            "Epoch 00090: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 3.3305e-04 - acc: 0.9998 - val_loss: 0.5952 - val_acc: 0.9643\n",
            "Epoch 91/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 3.8300e-04 - acc: 0.9998\n",
            "Epoch 00091: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 3.7339e-04 - acc: 0.9998 - val_loss: 0.5970 - val_acc: 0.9649\n",
            "Epoch 92/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.1052e-04 - acc: 0.9998\n",
            "Epoch 00092: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 77ms/step - loss: 4.0019e-04 - acc: 0.9998 - val_loss: 0.5987 - val_acc: 0.9643\n",
            "Epoch 93/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.3341e-04 - acc: 0.9998\n",
            "Epoch 00093: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.2250e-04 - acc: 0.9998 - val_loss: 0.5998 - val_acc: 0.9643\n",
            "Epoch 94/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 4.1743e-04 - acc: 0.9998\n",
            "Epoch 00094: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 78ms/step - loss: 4.1743e-04 - acc: 0.9998 - val_loss: 0.6012 - val_acc: 0.9643\n",
            "Epoch 95/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.0982e-04 - acc: 0.9998\n",
            "Epoch 00095: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 79ms/step - loss: 3.9951e-04 - acc: 0.9998 - val_loss: 0.6026 - val_acc: 0.9649\n",
            "Epoch 96/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 3.9059e-04 - acc: 0.9998\n",
            "Epoch 00096: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 79ms/step - loss: 3.8076e-04 - acc: 0.9998 - val_loss: 0.6044 - val_acc: 0.9649\n",
            "Epoch 97/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 4.3026e-04 - acc: 0.9998\n",
            "Epoch 00097: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 79ms/step - loss: 4.1943e-04 - acc: 0.9998 - val_loss: 0.6058 - val_acc: 0.9649\n",
            "Epoch 98/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 3.6472e-04 - acc: 0.9998\n",
            "Epoch 00098: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 79ms/step - loss: 3.5555e-04 - acc: 0.9998 - val_loss: 0.6071 - val_acc: 0.9649\n",
            "Epoch 99/100\n",
            "24/24 [==============================] - ETA: 0s - loss: 3.9168e-04 - acc: 0.9998\n",
            "Epoch 00099: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 79ms/step - loss: 3.9168e-04 - acc: 0.9998 - val_loss: 0.6077 - val_acc: 0.9643\n",
            "Epoch 100/100\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 3.6891e-04 - acc: 0.9998\n",
            "Epoch 00100: val_acc did not improve from 0.97022\n",
            "24/24 [==============================] - 2s 79ms/step - loss: 3.5963e-04 - acc: 0.9998 - val_loss: 0.6091 - val_acc: 0.9643\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1e543bb240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IL1Gp9i0XYOJ",
        "outputId": "fd676a6d-4dac-4752-c955-c0f1f0562d7f"
      },
      "source": [
        "from tensorflow.keras.models import Model,Sequential\n",
        "from tensorflow.keras.layers import Embedding,Dropout,BatchNormalization,Activation,concatenate,Input,Dense,LSTM,Bidirectional,Flatten\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(300, activation='relu'), input_shape=(1, X_pre.shape[1])))\n",
        "#model.add(Flatten())\n",
        "#model.add(Dense(1024,activation='relu'))\n",
        "#model.add(Dropout(0.2))\n",
        "#model.add(Dense(512,activation='relu'))\n",
        "#model.add(Dense(256,activation='relu'))\n",
        "#model.add(Dropout(0.2))\n",
        "#model.add(Dense(64,activation='relu'))\n",
        "model.add(Dense(4,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_9b_booh9vq",
        "outputId": "25837792-94fe-4568-e778-5b8717ca3770"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_13 (Bidirectio (None, 600)               2188800   \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 4)                 2404      \n",
            "=================================================================\n",
            "Total params: 2,191,204\n",
            "Trainable params: 2,191,204\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly8QBvNcXw71"
      },
      "source": [
        "#Callbacks\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "filepath_1=\"best_model_1.hdf5\"\n",
        "filepath_2=\"best_model_2.hdf5\"\n",
        "filepath_3=\"best_model_3.hdf5\"\n",
        "\n",
        "checkpoint_1 = ModelCheckpoint(filepath_1, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "checkpoint_2 = ModelCheckpoint(filepath_2, monitor='rmse', verbose=1, save_best_only=True, mode='min')\n",
        "checkpoint_3 = ModelCheckpoint(filepath_3, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "es = EarlyStopping(monitor='val_acc', mode='max',patience=100)\n",
        "es2 = EarlyStopping(monitor='rmse', mode='min',patience=5)\n",
        "\n",
        "\n",
        "callbacks_list_1 = [checkpoint_1,es]\n",
        "callbacks_list_2 = [checkpoint_2,es2]\n",
        "callbacks_list_3 = [checkpoint_3,es]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kU6yeCqRZB3G",
        "outputId": "87364bb9-cc4a-43a7-b5df-d1e1c2bfbf81"
      },
      "source": [
        "#to convert class labels to one hot encoded vectors.\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ohe=OneHotEncoder()\n",
        "y_ohe_train=ohe.fit_transform(np.array(y_train).reshape(-1,1)).todense()\n",
        "y_ohe_test=ohe.transform(np.array(y_val).reshape(-1,1)).todense()\t\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['acc'])\n",
        "model.fit(x_train.values.reshape((x_train.shape[0], 1, x_train.shape[1])),y_ohe_train,\n",
        "          epochs=1000,batch_size=32,\n",
        "          validation_data=(x_val.values.reshape((x_val.shape[0], 1, x_val.shape[1])),y_ohe_test),\n",
        "          callbacks=[callbacks_list_1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 120.1759 - acc: 0.2919\n",
            "Epoch 00001: val_acc improved from -inf to 0.32164, saving model to best_model_1.hdf5\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 118.1590 - acc: 0.2935 - val_loss: 53.8066 - val_acc: 0.3216\n",
            "Epoch 2/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 31.1946 - acc: 0.3512\n",
            "Epoch 00002: val_acc improved from 0.32164 to 0.38981, saving model to best_model_1.hdf5\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 31.0477 - acc: 0.3518 - val_loss: 26.0057 - val_acc: 0.3898\n",
            "Epoch 3/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 18.0432 - acc: 0.4129\n",
            "Epoch 00003: val_acc improved from 0.38981 to 0.46261, saving model to best_model_1.hdf5\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 17.9575 - acc: 0.4129 - val_loss: 10.9661 - val_acc: 0.4626\n",
            "Epoch 4/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 12.3549 - acc: 0.4429\n",
            "Epoch 00004: val_acc improved from 0.46261 to 0.47651, saving model to best_model_1.hdf5\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 12.3510 - acc: 0.4422 - val_loss: 10.9415 - val_acc: 0.4765\n",
            "Epoch 5/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 9.5016 - acc: 0.4776\n",
            "Epoch 00005: val_acc did not improve from 0.47651\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 9.5145 - acc: 0.4773 - val_loss: 12.9563 - val_acc: 0.3733\n",
            "Epoch 6/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 7.4609 - acc: 0.5179\n",
            "Epoch 00006: val_acc improved from 0.47651 to 0.60225, saving model to best_model_1.hdf5\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 7.4280 - acc: 0.5185 - val_loss: 4.2362 - val_acc: 0.6023\n",
            "Epoch 7/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 6.2694 - acc: 0.5528\n",
            "Epoch 00007: val_acc did not improve from 0.60225\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 6.2694 - acc: 0.5528 - val_loss: 7.6814 - val_acc: 0.4428\n",
            "Epoch 8/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 4.9464 - acc: 0.5845\n",
            "Epoch 00008: val_acc did not improve from 0.60225\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 4.9387 - acc: 0.5854 - val_loss: 3.9434 - val_acc: 0.5930\n",
            "Epoch 9/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 3.9005 - acc: 0.6062\n",
            "Epoch 00009: val_acc did not improve from 0.60225\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 3.9016 - acc: 0.6063 - val_loss: 4.8211 - val_acc: 0.5586\n",
            "Epoch 10/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 4.4330 - acc: 0.5971\n",
            "Epoch 00010: val_acc improved from 0.60225 to 0.67306, saving model to best_model_1.hdf5\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 4.4249 - acc: 0.5974 - val_loss: 2.9828 - val_acc: 0.6731\n",
            "Epoch 11/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 3.4189 - acc: 0.6459\n",
            "Epoch 00011: val_acc improved from 0.67306 to 0.68299, saving model to best_model_1.hdf5\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 3.4208 - acc: 0.6459 - val_loss: 2.6057 - val_acc: 0.6830\n",
            "Epoch 12/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 3.0569 - acc: 0.6632\n",
            "Epoch 00012: val_acc did not improve from 0.68299\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 3.0605 - acc: 0.6627 - val_loss: 3.6245 - val_acc: 0.6380\n",
            "Epoch 13/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 2.5618 - acc: 0.6935\n",
            "Epoch 00013: val_acc improved from 0.68299 to 0.84514, saving model to best_model_1.hdf5\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 2.5504 - acc: 0.6944 - val_loss: 0.9830 - val_acc: 0.8451\n",
            "Epoch 14/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 2.9721 - acc: 0.6760\n",
            "Epoch 00014: val_acc did not improve from 0.84514\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 2.9635 - acc: 0.6760 - val_loss: 3.5049 - val_acc: 0.6287\n",
            "Epoch 15/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 1.9615 - acc: 0.7440\n",
            "Epoch 00015: val_acc did not improve from 0.84514\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 1.9615 - acc: 0.7440 - val_loss: 1.7929 - val_acc: 0.7624\n",
            "Epoch 16/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 2.1108 - acc: 0.7334\n",
            "Epoch 00016: val_acc did not improve from 0.84514\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 2.0825 - acc: 0.7351 - val_loss: 1.1115 - val_acc: 0.8213\n",
            "Epoch 17/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 1.9061 - acc: 0.7443\n",
            "Epoch 00017: val_acc did not improve from 0.84514\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 1.9074 - acc: 0.7444 - val_loss: 0.9724 - val_acc: 0.8279\n",
            "Epoch 18/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 1.7571 - acc: 0.7517\n",
            "Epoch 00018: val_acc did not improve from 0.84514\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 1.7525 - acc: 0.7518 - val_loss: 1.6540 - val_acc: 0.7386\n",
            "Epoch 19/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 2.4951 - acc: 0.7104\n",
            "Epoch 00019: val_acc did not improve from 0.84514\n",
            "189/189 [==============================] - 1s 7ms/step - loss: 2.4925 - acc: 0.7108 - val_loss: 1.9276 - val_acc: 0.7141\n",
            "Epoch 20/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 2.3085 - acc: 0.7300\n",
            "Epoch 00020: val_acc did not improve from 0.84514\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 2.3184 - acc: 0.7296 - val_loss: 4.0435 - val_acc: 0.5996\n",
            "Epoch 21/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 1.4880 - acc: 0.7905\n",
            "Epoch 00021: val_acc improved from 0.84514 to 0.85639, saving model to best_model_1.hdf5\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 1.4674 - acc: 0.7922 - val_loss: 0.8091 - val_acc: 0.8564\n",
            "Epoch 22/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 1.7087 - acc: 0.7701\n",
            "Epoch 00022: val_acc did not improve from 0.85639\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 1.7068 - acc: 0.7704 - val_loss: 1.6088 - val_acc: 0.8021\n",
            "Epoch 23/1000\n",
            "182/189 [===========================>..] - ETA: 0s - loss: 1.8111 - acc: 0.7679\n",
            "Epoch 00023: val_acc did not improve from 0.85639\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 1.7956 - acc: 0.7680 - val_loss: 2.4995 - val_acc: 0.7922\n",
            "Epoch 24/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 1.2629 - acc: 0.8059\n",
            "Epoch 00024: val_acc improved from 0.85639 to 0.86962, saving model to best_model_1.hdf5\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 1.2649 - acc: 0.8058 - val_loss: 0.7370 - val_acc: 0.8696\n",
            "Epoch 25/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 1.2931 - acc: 0.7992\n",
            "Epoch 00025: val_acc did not improve from 0.86962\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 1.2922 - acc: 0.7995 - val_loss: 1.8799 - val_acc: 0.7465\n",
            "Epoch 26/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 1.7280 - acc: 0.7733\n",
            "Epoch 00026: val_acc did not improve from 0.86962\n",
            "189/189 [==============================] - 1s 7ms/step - loss: 1.7280 - acc: 0.7733 - val_loss: 2.2815 - val_acc: 0.7452\n",
            "Epoch 27/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.8875 - acc: 0.8385\n",
            "Epoch 00027: val_acc did not improve from 0.86962\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.8903 - acc: 0.8384 - val_loss: 0.6742 - val_acc: 0.8650\n",
            "Epoch 28/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 2.0750 - acc: 0.7476\n",
            "Epoch 00028: val_acc improved from 0.86962 to 0.90007, saving model to best_model_1.hdf5\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 2.0487 - acc: 0.7490 - val_loss: 0.5140 - val_acc: 0.9001\n",
            "Epoch 29/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 1.7114 - acc: 0.7628\n",
            "Epoch 00029: val_acc did not improve from 0.90007\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 1.6856 - acc: 0.7657 - val_loss: 1.3952 - val_acc: 0.7525\n",
            "Epoch 30/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.9844 - acc: 0.8231\n",
            "Epoch 00030: val_acc did not improve from 0.90007\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.9792 - acc: 0.8238 - val_loss: 1.0497 - val_acc: 0.8246\n",
            "Epoch 31/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 1.1859 - acc: 0.8068\n",
            "Epoch 00031: val_acc did not improve from 0.90007\n",
            "189/189 [==============================] - 1s 7ms/step - loss: 1.1813 - acc: 0.8076 - val_loss: 0.6172 - val_acc: 0.8650\n",
            "Epoch 32/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 1.2062 - acc: 0.8025\n",
            "Epoch 00032: val_acc did not improve from 0.90007\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 1.1878 - acc: 0.8053 - val_loss: 0.7604 - val_acc: 0.8379\n",
            "Epoch 33/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.8944 - acc: 0.8362\n",
            "Epoch 00033: val_acc did not improve from 0.90007\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.8900 - acc: 0.8373 - val_loss: 0.6743 - val_acc: 0.8432\n",
            "Epoch 34/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.7757 - acc: 0.8493\n",
            "Epoch 00034: val_acc improved from 0.90007 to 0.90602, saving model to best_model_1.hdf5\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.7796 - acc: 0.8482 - val_loss: 0.4486 - val_acc: 0.9060\n",
            "Epoch 35/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.9885 - acc: 0.8223\n",
            "Epoch 00035: val_acc did not improve from 0.90602\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.9862 - acc: 0.8220 - val_loss: 1.3001 - val_acc: 0.7710\n",
            "Epoch 36/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.6917 - acc: 0.8599\n",
            "Epoch 00036: val_acc did not improve from 0.90602\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.6919 - acc: 0.8606 - val_loss: 1.1655 - val_acc: 0.7849\n",
            "Epoch 37/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 1.0095 - acc: 0.8194\n",
            "Epoch 00037: val_acc did not improve from 0.90602\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 1.0249 - acc: 0.8174 - val_loss: 2.2252 - val_acc: 0.6876\n",
            "Epoch 38/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.8450 - acc: 0.8511\n",
            "Epoch 00038: val_acc did not improve from 0.90602\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.8402 - acc: 0.8515 - val_loss: 0.6968 - val_acc: 0.8445\n",
            "Epoch 39/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.8356 - acc: 0.8236\n",
            "Epoch 00039: val_acc did not improve from 0.90602\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.8406 - acc: 0.8250 - val_loss: 0.5707 - val_acc: 0.8690\n",
            "Epoch 40/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.5554 - acc: 0.8669\n",
            "Epoch 00040: val_acc improved from 0.90602 to 0.92786, saving model to best_model_1.hdf5\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.5783 - acc: 0.8651 - val_loss: 0.2973 - val_acc: 0.9279\n",
            "Epoch 41/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.7636 - acc: 0.8389\n",
            "Epoch 00041: val_acc did not improve from 0.92786\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.7602 - acc: 0.8391 - val_loss: 0.5732 - val_acc: 0.8670\n",
            "Epoch 42/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.9867 - acc: 0.8136\n",
            "Epoch 00042: val_acc did not improve from 0.92786\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.9867 - acc: 0.8136 - val_loss: 1.2875 - val_acc: 0.7598\n",
            "Epoch 43/1000\n",
            "182/189 [===========================>..] - ETA: 0s - loss: 0.7323 - acc: 0.8309\n",
            "Epoch 00043: val_acc did not improve from 0.92786\n",
            "189/189 [==============================] - 1s 7ms/step - loss: 0.7367 - acc: 0.8288 - val_loss: 2.0410 - val_acc: 0.6499\n",
            "Epoch 44/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.5017 - acc: 0.8673\n",
            "Epoch 00044: val_acc did not improve from 0.92786\n",
            "189/189 [==============================] - 1s 7ms/step - loss: 0.4989 - acc: 0.8679 - val_loss: 0.3394 - val_acc: 0.8968\n",
            "Epoch 45/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.6329 - acc: 0.8458\n",
            "Epoch 00045: val_acc did not improve from 0.92786\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.6391 - acc: 0.8452 - val_loss: 0.7879 - val_acc: 0.8504\n",
            "Epoch 46/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.4905 - acc: 0.8681\n",
            "Epoch 00046: val_acc did not improve from 0.92786\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.4852 - acc: 0.8694 - val_loss: 0.3145 - val_acc: 0.9073\n",
            "Epoch 47/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.3684 - acc: 0.8915\n",
            "Epoch 00047: val_acc did not improve from 0.92786\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.3727 - acc: 0.8904 - val_loss: 0.2767 - val_acc: 0.9159\n",
            "Epoch 48/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.5600 - acc: 0.8448\n",
            "Epoch 00048: val_acc did not improve from 0.92786\n",
            "189/189 [==============================] - 1s 7ms/step - loss: 0.5582 - acc: 0.8454 - val_loss: 0.4817 - val_acc: 0.8584\n",
            "Epoch 49/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.4094 - acc: 0.8753\n",
            "Epoch 00049: val_acc did not improve from 0.92786\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.4092 - acc: 0.8755 - val_loss: 0.3523 - val_acc: 0.8941\n",
            "Epoch 50/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.4024 - acc: 0.8806\n",
            "Epoch 00050: val_acc did not improve from 0.92786\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.4024 - acc: 0.8806 - val_loss: 0.3037 - val_acc: 0.9060\n",
            "Epoch 51/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.4366 - acc: 0.8853\n",
            "Epoch 00051: val_acc improved from 0.92786 to 0.93382, saving model to best_model_1.hdf5\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.4328 - acc: 0.8859 - val_loss: 0.2731 - val_acc: 0.9338\n",
            "Epoch 52/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.5483 - acc: 0.8623\n",
            "Epoch 00052: val_acc did not improve from 0.93382\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.5455 - acc: 0.8624 - val_loss: 0.3393 - val_acc: 0.9107\n",
            "Epoch 53/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.3657 - acc: 0.8933\n",
            "Epoch 00053: val_acc did not improve from 0.93382\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.3637 - acc: 0.8944 - val_loss: 0.2607 - val_acc: 0.9153\n",
            "Epoch 54/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.3769 - acc: 0.8861\n",
            "Epoch 00054: val_acc did not improve from 0.93382\n",
            "189/189 [==============================] - 1s 7ms/step - loss: 0.3769 - acc: 0.8861 - val_loss: 0.3086 - val_acc: 0.9027\n",
            "Epoch 55/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.4867 - acc: 0.8492\n",
            "Epoch 00055: val_acc did not improve from 0.93382\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.4875 - acc: 0.8493 - val_loss: 0.2533 - val_acc: 0.9232\n",
            "Epoch 56/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.4026 - acc: 0.8679\n",
            "Epoch 00056: val_acc did not improve from 0.93382\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.4020 - acc: 0.8679 - val_loss: 0.7301 - val_acc: 0.7783\n",
            "Epoch 57/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.3795 - acc: 0.8777\n",
            "Epoch 00057: val_acc did not improve from 0.93382\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.3776 - acc: 0.8780 - val_loss: 0.3965 - val_acc: 0.8789\n",
            "Epoch 58/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.2846 - acc: 0.9035\n",
            "Epoch 00058: val_acc did not improve from 0.93382\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.2840 - acc: 0.9040 - val_loss: 0.2653 - val_acc: 0.9153\n",
            "Epoch 59/1000\n",
            "182/189 [===========================>..] - ETA: 0s - loss: 0.2916 - acc: 0.9059\n",
            "Epoch 00059: val_acc did not improve from 0.93382\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.2921 - acc: 0.9055 - val_loss: 0.5567 - val_acc: 0.8319\n",
            "Epoch 60/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.3686 - acc: 0.8736\n",
            "Epoch 00060: val_acc did not improve from 0.93382\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.3701 - acc: 0.8733 - val_loss: 0.3511 - val_acc: 0.8776\n",
            "Epoch 61/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.3410 - acc: 0.8853\n",
            "Epoch 00061: val_acc did not improve from 0.93382\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.3442 - acc: 0.8844 - val_loss: 0.3127 - val_acc: 0.8941\n",
            "Epoch 62/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.3403 - acc: 0.8866\n",
            "Epoch 00062: val_acc did not improve from 0.93382\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.3401 - acc: 0.8868 - val_loss: 0.2383 - val_acc: 0.9298\n",
            "Epoch 63/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2712 - acc: 0.9061\n",
            "Epoch 00063: val_acc did not improve from 0.93382\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.2719 - acc: 0.9055 - val_loss: 0.2243 - val_acc: 0.9332\n",
            "Epoch 64/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.3128 - acc: 0.8903\n",
            "Epoch 00064: val_acc improved from 0.93382 to 0.93514, saving model to best_model_1.hdf5\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.3124 - acc: 0.8906 - val_loss: 0.2243 - val_acc: 0.9351\n",
            "Epoch 65/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.3029 - acc: 0.8952\n",
            "Epoch 00065: val_acc did not improve from 0.93514\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.3024 - acc: 0.8955 - val_loss: 0.3288 - val_acc: 0.8981\n",
            "Epoch 66/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.3030 - acc: 0.8984\n",
            "Epoch 00066: val_acc did not improve from 0.93514\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.3050 - acc: 0.8980 - val_loss: 0.4417 - val_acc: 0.8405\n",
            "Epoch 67/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.3655 - acc: 0.8797\n",
            "Epoch 00067: val_acc did not improve from 0.93514\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.3649 - acc: 0.8800 - val_loss: 0.3905 - val_acc: 0.8835\n",
            "Epoch 68/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.5367 - acc: 0.8095\n",
            "Epoch 00068: val_acc did not improve from 0.93514\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.5349 - acc: 0.8103 - val_loss: 0.4280 - val_acc: 0.8564\n",
            "Epoch 69/1000\n",
            "182/189 [===========================>..] - ETA: 0s - loss: 0.3818 - acc: 0.8705\n",
            "Epoch 00069: val_acc did not improve from 0.93514\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.3812 - acc: 0.8704 - val_loss: 0.3018 - val_acc: 0.9014\n",
            "Epoch 70/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.3765 - acc: 0.8711\n",
            "Epoch 00070: val_acc did not improve from 0.93514\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.3757 - acc: 0.8712 - val_loss: 0.2717 - val_acc: 0.9133\n",
            "Epoch 71/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.3393 - acc: 0.8782\n",
            "Epoch 00071: val_acc did not improve from 0.93514\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.3416 - acc: 0.8772 - val_loss: 0.4474 - val_acc: 0.8259\n",
            "Epoch 72/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.3689 - acc: 0.8709\n",
            "Epoch 00072: val_acc did not improve from 0.93514\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.3689 - acc: 0.8709 - val_loss: 0.3311 - val_acc: 0.8908\n",
            "Epoch 73/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.3799 - acc: 0.8609\n",
            "Epoch 00073: val_acc did not improve from 0.93514\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.3799 - acc: 0.8609 - val_loss: 0.2815 - val_acc: 0.9014\n",
            "Epoch 74/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.3712 - acc: 0.8733\n",
            "Epoch 00074: val_acc did not improve from 0.93514\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.3729 - acc: 0.8727 - val_loss: 0.3321 - val_acc: 0.8954\n",
            "Epoch 75/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.3506 - acc: 0.8787\n",
            "Epoch 00075: val_acc did not improve from 0.93514\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.3466 - acc: 0.8801 - val_loss: 0.3898 - val_acc: 0.8524\n",
            "Epoch 76/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2956 - acc: 0.8978\n",
            "Epoch 00076: val_acc did not improve from 0.93514\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2952 - acc: 0.8980 - val_loss: 0.2277 - val_acc: 0.9298\n",
            "Epoch 77/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.3145 - acc: 0.8871\n",
            "Epoch 00077: val_acc did not improve from 0.93514\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.3143 - acc: 0.8871 - val_loss: 0.3087 - val_acc: 0.8934\n",
            "Epoch 78/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.3065 - acc: 0.8937\n",
            "Epoch 00078: val_acc did not improve from 0.93514\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.3065 - acc: 0.8937 - val_loss: 0.2507 - val_acc: 0.9193\n",
            "Epoch 79/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2934 - acc: 0.8975\n",
            "Epoch 00079: val_acc did not improve from 0.93514\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.2953 - acc: 0.8975 - val_loss: 0.3121 - val_acc: 0.9001\n",
            "Epoch 80/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.3634 - acc: 0.8769\n",
            "Epoch 00080: val_acc did not improve from 0.93514\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.3610 - acc: 0.8783 - val_loss: 0.2418 - val_acc: 0.9272\n",
            "Epoch 81/1000\n",
            "182/189 [===========================>..] - ETA: 0s - loss: 0.2942 - acc: 0.9014\n",
            "Epoch 00081: val_acc did not improve from 0.93514\n",
            "189/189 [==============================] - 1s 7ms/step - loss: 0.2965 - acc: 0.9007 - val_loss: 0.2374 - val_acc: 0.9252\n",
            "Epoch 82/1000\n",
            "182/189 [===========================>..] - ETA: 0s - loss: 0.3163 - acc: 0.8870\n",
            "Epoch 00082: val_acc did not improve from 0.93514\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.3203 - acc: 0.8859 - val_loss: 0.2957 - val_acc: 0.8987\n",
            "Epoch 83/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.3921 - acc: 0.8644\n",
            "Epoch 00083: val_acc did not improve from 0.93514\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.3913 - acc: 0.8647 - val_loss: 0.2892 - val_acc: 0.9133\n",
            "Epoch 84/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.4149 - acc: 0.8581\n",
            "Epoch 00084: val_acc did not improve from 0.93514\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.4117 - acc: 0.8594 - val_loss: 0.3325 - val_acc: 0.8901\n",
            "Epoch 85/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.3820 - acc: 0.8670\n",
            "Epoch 00085: val_acc did not improve from 0.93514\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.3821 - acc: 0.8672 - val_loss: 0.2732 - val_acc: 0.9206\n",
            "Epoch 86/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.3140 - acc: 0.8900\n",
            "Epoch 00086: val_acc did not improve from 0.93514\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.3151 - acc: 0.8891 - val_loss: 0.3061 - val_acc: 0.9040\n",
            "Epoch 87/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.3369 - acc: 0.8864\n",
            "Epoch 00087: val_acc improved from 0.93514 to 0.93713, saving model to best_model_1.hdf5\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.3352 - acc: 0.8871 - val_loss: 0.2192 - val_acc: 0.9371\n",
            "Epoch 88/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.3111 - acc: 0.8952\n",
            "Epoch 00088: val_acc did not improve from 0.93713\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.3100 - acc: 0.8952 - val_loss: 0.2669 - val_acc: 0.9133\n",
            "Epoch 89/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2993 - acc: 0.8972\n",
            "Epoch 00089: val_acc did not improve from 0.93713\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.2992 - acc: 0.8974 - val_loss: 0.3278 - val_acc: 0.8842\n",
            "Epoch 90/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.3734 - acc: 0.8725\n",
            "Epoch 00090: val_acc did not improve from 0.93713\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.3726 - acc: 0.8728 - val_loss: 0.2459 - val_acc: 0.9279\n",
            "Epoch 91/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2921 - acc: 0.9035\n",
            "Epoch 00091: val_acc did not improve from 0.93713\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2943 - acc: 0.9028 - val_loss: 0.4044 - val_acc: 0.8478\n",
            "Epoch 92/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.3253 - acc: 0.8935\n",
            "Epoch 00092: val_acc did not improve from 0.93713\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.3253 - acc: 0.8935 - val_loss: 0.3492 - val_acc: 0.8809\n",
            "Epoch 93/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2917 - acc: 0.9017\n",
            "Epoch 00093: val_acc did not improve from 0.93713\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2945 - acc: 0.9003 - val_loss: 0.3927 - val_acc: 0.8518\n",
            "Epoch 94/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2782 - acc: 0.9067\n",
            "Epoch 00094: val_acc did not improve from 0.93713\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2804 - acc: 0.9061 - val_loss: 0.2302 - val_acc: 0.9332\n",
            "Epoch 95/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.3031 - acc: 0.8998\n",
            "Epoch 00095: val_acc did not improve from 0.93713\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.3039 - acc: 0.8992 - val_loss: 0.4353 - val_acc: 0.8253\n",
            "Epoch 96/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.3083 - acc: 0.8943\n",
            "Epoch 00096: val_acc did not improve from 0.93713\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.3076 - acc: 0.8947 - val_loss: 0.3167 - val_acc: 0.8934\n",
            "Epoch 97/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.3057 - acc: 0.8944\n",
            "Epoch 00097: val_acc did not improve from 0.93713\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.3050 - acc: 0.8947 - val_loss: 0.2317 - val_acc: 0.9292\n",
            "Epoch 98/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2939 - acc: 0.9004\n",
            "Epoch 00098: val_acc did not improve from 0.93713\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2933 - acc: 0.9003 - val_loss: 0.2995 - val_acc: 0.8968\n",
            "Epoch 99/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.2683 - acc: 0.9085\n",
            "Epoch 00099: val_acc did not improve from 0.93713\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2674 - acc: 0.9088 - val_loss: 0.2477 - val_acc: 0.9199\n",
            "Epoch 100/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.3064 - acc: 0.8971\n",
            "Epoch 00100: val_acc did not improve from 0.93713\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.3079 - acc: 0.8960 - val_loss: 0.2610 - val_acc: 0.9126\n",
            "Epoch 101/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.2783 - acc: 0.9095\n",
            "Epoch 00101: val_acc improved from 0.93713 to 0.94507, saving model to best_model_1.hdf5\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2783 - acc: 0.9088 - val_loss: 0.1944 - val_acc: 0.9451\n",
            "Epoch 102/1000\n",
            "182/189 [===========================>..] - ETA: 0s - loss: 0.3046 - acc: 0.8959\n",
            "Epoch 00102: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.3040 - acc: 0.8965 - val_loss: 0.2503 - val_acc: 0.9226\n",
            "Epoch 103/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.2975 - acc: 0.8989\n",
            "Epoch 00103: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.2953 - acc: 0.8995 - val_loss: 0.2381 - val_acc: 0.9259\n",
            "Epoch 104/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2634 - acc: 0.9056\n",
            "Epoch 00104: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2655 - acc: 0.9046 - val_loss: 0.4590 - val_acc: 0.8418\n",
            "Epoch 105/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.3248 - acc: 0.8918\n",
            "Epoch 00105: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.3255 - acc: 0.8914 - val_loss: 0.2140 - val_acc: 0.9358\n",
            "Epoch 106/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2892 - acc: 0.9031\n",
            "Epoch 00106: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.2903 - acc: 0.9031 - val_loss: 0.2303 - val_acc: 0.9279\n",
            "Epoch 107/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.2574 - acc: 0.9122\n",
            "Epoch 00107: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2565 - acc: 0.9126 - val_loss: 0.2498 - val_acc: 0.9193\n",
            "Epoch 108/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.2602 - acc: 0.9081\n",
            "Epoch 00108: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2604 - acc: 0.9079 - val_loss: 0.2224 - val_acc: 0.9285\n",
            "Epoch 109/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.2747 - acc: 0.9071\n",
            "Epoch 00109: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.2720 - acc: 0.9086 - val_loss: 0.2167 - val_acc: 0.9338\n",
            "Epoch 110/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2662 - acc: 0.9064\n",
            "Epoch 00110: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.2659 - acc: 0.9065 - val_loss: 0.3197 - val_acc: 0.8915\n",
            "Epoch 111/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.3535 - acc: 0.8832\n",
            "Epoch 00111: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.3493 - acc: 0.8848 - val_loss: 0.2693 - val_acc: 0.9087\n",
            "Epoch 112/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2821 - acc: 0.9054\n",
            "Epoch 00112: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.2817 - acc: 0.9055 - val_loss: 0.2444 - val_acc: 0.9206\n",
            "Epoch 113/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2584 - acc: 0.9147\n",
            "Epoch 00113: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.2579 - acc: 0.9149 - val_loss: 0.5590 - val_acc: 0.8041\n",
            "Epoch 114/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.4343 - acc: 0.8540\n",
            "Epoch 00114: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.4312 - acc: 0.8553 - val_loss: 0.2022 - val_acc: 0.9411\n",
            "Epoch 115/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.3027 - acc: 0.8954\n",
            "Epoch 00115: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.3027 - acc: 0.8954 - val_loss: 0.4598 - val_acc: 0.8425\n",
            "Epoch 116/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.3056 - acc: 0.8963\n",
            "Epoch 00116: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.3034 - acc: 0.8972 - val_loss: 0.2173 - val_acc: 0.9378\n",
            "Epoch 117/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2456 - acc: 0.9209\n",
            "Epoch 00117: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2450 - acc: 0.9210 - val_loss: 0.2371 - val_acc: 0.9252\n",
            "Epoch 118/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.2557 - acc: 0.9162\n",
            "Epoch 00118: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2532 - acc: 0.9175 - val_loss: 0.2011 - val_acc: 0.9365\n",
            "Epoch 119/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2603 - acc: 0.9119\n",
            "Epoch 00119: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2616 - acc: 0.9114 - val_loss: 0.2594 - val_acc: 0.9159\n",
            "Epoch 120/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.3406 - acc: 0.8855\n",
            "Epoch 00120: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.3371 - acc: 0.8861 - val_loss: 0.2166 - val_acc: 0.9318\n",
            "Epoch 121/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2754 - acc: 0.9069\n",
            "Epoch 00121: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2759 - acc: 0.9065 - val_loss: 0.2106 - val_acc: 0.9345\n",
            "Epoch 122/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2917 - acc: 0.9037\n",
            "Epoch 00122: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.2903 - acc: 0.9046 - val_loss: 0.4558 - val_acc: 0.8445\n",
            "Epoch 123/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.2885 - acc: 0.9051\n",
            "Epoch 00123: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2884 - acc: 0.9050 - val_loss: 0.2562 - val_acc: 0.9093\n",
            "Epoch 124/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2709 - acc: 0.9084\n",
            "Epoch 00124: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2707 - acc: 0.9086 - val_loss: 0.2440 - val_acc: 0.9179\n",
            "Epoch 125/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2964 - acc: 0.8960\n",
            "Epoch 00125: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.2966 - acc: 0.8964 - val_loss: 0.2413 - val_acc: 0.9318\n",
            "Epoch 126/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.2831 - acc: 0.9044\n",
            "Epoch 00126: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.2813 - acc: 0.9055 - val_loss: 0.2601 - val_acc: 0.9146\n",
            "Epoch 127/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.3004 - acc: 0.8989\n",
            "Epoch 00127: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.3000 - acc: 0.8990 - val_loss: 0.2201 - val_acc: 0.9345\n",
            "Epoch 128/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2404 - acc: 0.9217\n",
            "Epoch 00128: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.2404 - acc: 0.9217 - val_loss: 0.2226 - val_acc: 0.9272\n",
            "Epoch 129/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2947 - acc: 0.9054\n",
            "Epoch 00129: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.2941 - acc: 0.9055 - val_loss: 0.2686 - val_acc: 0.9080\n",
            "Epoch 130/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2358 - acc: 0.9250\n",
            "Epoch 00130: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2358 - acc: 0.9250 - val_loss: 0.2525 - val_acc: 0.9126\n",
            "Epoch 131/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.2619 - acc: 0.9127\n",
            "Epoch 00131: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2612 - acc: 0.9136 - val_loss: 0.2841 - val_acc: 0.9054\n",
            "Epoch 132/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.2481 - acc: 0.9170\n",
            "Epoch 00132: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2520 - acc: 0.9149 - val_loss: 0.2256 - val_acc: 0.9272\n",
            "Epoch 133/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2830 - acc: 0.9068\n",
            "Epoch 00133: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2830 - acc: 0.9068 - val_loss: 0.2115 - val_acc: 0.9338\n",
            "Epoch 134/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.2571 - acc: 0.9115\n",
            "Epoch 00134: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2566 - acc: 0.9121 - val_loss: 0.2103 - val_acc: 0.9325\n",
            "Epoch 135/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2673 - acc: 0.9108\n",
            "Epoch 00135: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2696 - acc: 0.9098 - val_loss: 0.5384 - val_acc: 0.8127\n",
            "Epoch 136/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2579 - acc: 0.9141\n",
            "Epoch 00136: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.2601 - acc: 0.9137 - val_loss: 0.2358 - val_acc: 0.9239\n",
            "Epoch 137/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.3060 - acc: 0.8969\n",
            "Epoch 00137: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.3060 - acc: 0.8969 - val_loss: 0.2712 - val_acc: 0.9107\n",
            "Epoch 138/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2577 - acc: 0.9152\n",
            "Epoch 00138: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.2577 - acc: 0.9152 - val_loss: 0.2100 - val_acc: 0.9398\n",
            "Epoch 139/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2926 - acc: 0.8989\n",
            "Epoch 00139: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.2949 - acc: 0.8980 - val_loss: 0.2697 - val_acc: 0.9126\n",
            "Epoch 140/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2586 - acc: 0.9122\n",
            "Epoch 00140: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2583 - acc: 0.9124 - val_loss: 0.2098 - val_acc: 0.9378\n",
            "Epoch 141/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2896 - acc: 0.9029\n",
            "Epoch 00141: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2956 - acc: 0.9002 - val_loss: 0.4087 - val_acc: 0.8432\n",
            "Epoch 142/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2395 - acc: 0.9168\n",
            "Epoch 00142: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.2381 - acc: 0.9174 - val_loss: 0.2398 - val_acc: 0.9279\n",
            "Epoch 143/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2395 - acc: 0.9184\n",
            "Epoch 00143: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2395 - acc: 0.9184 - val_loss: 0.1975 - val_acc: 0.9365\n",
            "Epoch 144/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2668 - acc: 0.9091\n",
            "Epoch 00144: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2664 - acc: 0.9089 - val_loss: 0.3496 - val_acc: 0.8756\n",
            "Epoch 145/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2582 - acc: 0.9147\n",
            "Epoch 00145: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2577 - acc: 0.9146 - val_loss: 0.2293 - val_acc: 0.9298\n",
            "Epoch 146/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.2602 - acc: 0.9122\n",
            "Epoch 00146: val_acc did not improve from 0.94507\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2602 - acc: 0.9119 - val_loss: 0.2082 - val_acc: 0.9371\n",
            "Epoch 147/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.2319 - acc: 0.9262\n",
            "Epoch 00147: val_acc improved from 0.94507 to 0.94573, saving model to best_model_1.hdf5\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2318 - acc: 0.9257 - val_loss: 0.1856 - val_acc: 0.9457\n",
            "Epoch 148/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2372 - acc: 0.9198\n",
            "Epoch 00148: val_acc did not improve from 0.94573\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2386 - acc: 0.9194 - val_loss: 0.4410 - val_acc: 0.8551\n",
            "Epoch 149/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2618 - acc: 0.9126\n",
            "Epoch 00149: val_acc did not improve from 0.94573\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2618 - acc: 0.9126 - val_loss: 0.2071 - val_acc: 0.9358\n",
            "Epoch 150/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2833 - acc: 0.9037\n",
            "Epoch 00150: val_acc improved from 0.94573 to 0.94639, saving model to best_model_1.hdf5\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2851 - acc: 0.9025 - val_loss: 0.2064 - val_acc: 0.9464\n",
            "Epoch 151/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2494 - acc: 0.9154\n",
            "Epoch 00151: val_acc did not improve from 0.94639\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2494 - acc: 0.9154 - val_loss: 0.2196 - val_acc: 0.9371\n",
            "Epoch 152/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2510 - acc: 0.9152\n",
            "Epoch 00152: val_acc did not improve from 0.94639\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2510 - acc: 0.9152 - val_loss: 0.2280 - val_acc: 0.9332\n",
            "Epoch 153/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.2413 - acc: 0.9224\n",
            "Epoch 00153: val_acc did not improve from 0.94639\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2425 - acc: 0.9217 - val_loss: 0.4674 - val_acc: 0.8478\n",
            "Epoch 154/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2908 - acc: 0.9057\n",
            "Epoch 00154: val_acc did not improve from 0.94639\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2936 - acc: 0.9040 - val_loss: 0.2731 - val_acc: 0.9100\n",
            "Epoch 155/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2850 - acc: 0.9008\n",
            "Epoch 00155: val_acc did not improve from 0.94639\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2844 - acc: 0.9010 - val_loss: 0.2911 - val_acc: 0.9060\n",
            "Epoch 156/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2373 - acc: 0.9226\n",
            "Epoch 00156: val_acc did not improve from 0.94639\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2369 - acc: 0.9228 - val_loss: 0.2848 - val_acc: 0.9054\n",
            "Epoch 157/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2780 - acc: 0.9088\n",
            "Epoch 00157: val_acc improved from 0.94639 to 0.94838, saving model to best_model_1.hdf5\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2780 - acc: 0.9088 - val_loss: 0.1820 - val_acc: 0.9484\n",
            "Epoch 158/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.2481 - acc: 0.9231\n",
            "Epoch 00158: val_acc did not improve from 0.94838\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.2487 - acc: 0.9227 - val_loss: 0.2089 - val_acc: 0.9351\n",
            "Epoch 159/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2782 - acc: 0.9115\n",
            "Epoch 00159: val_acc improved from 0.94838 to 0.95036, saving model to best_model_1.hdf5\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2782 - acc: 0.9111 - val_loss: 0.1773 - val_acc: 0.9504\n",
            "Epoch 160/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2401 - acc: 0.9209\n",
            "Epoch 00160: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2407 - acc: 0.9204 - val_loss: 0.2106 - val_acc: 0.9292\n",
            "Epoch 161/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2471 - acc: 0.9147\n",
            "Epoch 00161: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2466 - acc: 0.9151 - val_loss: 0.2036 - val_acc: 0.9365\n",
            "Epoch 162/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2360 - acc: 0.9202\n",
            "Epoch 00162: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2369 - acc: 0.9195 - val_loss: 0.1859 - val_acc: 0.9457\n",
            "Epoch 163/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2593 - acc: 0.9136\n",
            "Epoch 00163: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2590 - acc: 0.9136 - val_loss: 0.3001 - val_acc: 0.8934\n",
            "Epoch 164/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2426 - acc: 0.9157\n",
            "Epoch 00164: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2443 - acc: 0.9152 - val_loss: 0.4159 - val_acc: 0.8537\n",
            "Epoch 165/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.2384 - acc: 0.9243\n",
            "Epoch 00165: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2358 - acc: 0.9253 - val_loss: 0.2483 - val_acc: 0.9173\n",
            "Epoch 166/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2249 - acc: 0.9259\n",
            "Epoch 00166: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2242 - acc: 0.9262 - val_loss: 0.2556 - val_acc: 0.9173\n",
            "Epoch 167/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.2481 - acc: 0.9158\n",
            "Epoch 00167: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2516 - acc: 0.9151 - val_loss: 0.2821 - val_acc: 0.9087\n",
            "Epoch 168/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2831 - acc: 0.9073\n",
            "Epoch 00168: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2837 - acc: 0.9065 - val_loss: 0.3115 - val_acc: 0.8875\n",
            "Epoch 169/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2621 - acc: 0.9116\n",
            "Epoch 00169: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2614 - acc: 0.9119 - val_loss: 0.2733 - val_acc: 0.9093\n",
            "Epoch 170/1000\n",
            "182/189 [===========================>..] - ETA: 0s - loss: 0.2642 - acc: 0.9088\n",
            "Epoch 00170: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2618 - acc: 0.9099 - val_loss: 0.1968 - val_acc: 0.9365\n",
            "Epoch 171/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2176 - acc: 0.9292\n",
            "Epoch 00171: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.2177 - acc: 0.9293 - val_loss: 0.2679 - val_acc: 0.9140\n",
            "Epoch 172/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.2576 - acc: 0.9149\n",
            "Epoch 00172: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2591 - acc: 0.9136 - val_loss: 0.1880 - val_acc: 0.9477\n",
            "Epoch 173/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2213 - acc: 0.9251\n",
            "Epoch 00173: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2214 - acc: 0.9247 - val_loss: 0.2131 - val_acc: 0.9305\n",
            "Epoch 174/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.2348 - acc: 0.9196\n",
            "Epoch 00174: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2370 - acc: 0.9189 - val_loss: 0.2629 - val_acc: 0.9047\n",
            "Epoch 175/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2512 - acc: 0.9143\n",
            "Epoch 00175: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2518 - acc: 0.9141 - val_loss: 0.1815 - val_acc: 0.9404\n",
            "Epoch 176/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2619 - acc: 0.9126\n",
            "Epoch 00176: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.2622 - acc: 0.9126 - val_loss: 0.2239 - val_acc: 0.9232\n",
            "Epoch 177/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2926 - acc: 0.9214\n",
            "Epoch 00177: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2936 - acc: 0.9209 - val_loss: 0.3424 - val_acc: 0.9034\n",
            "Epoch 178/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.2435 - acc: 0.9200\n",
            "Epoch 00178: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2424 - acc: 0.9205 - val_loss: 0.2263 - val_acc: 0.9298\n",
            "Epoch 179/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2459 - acc: 0.9201\n",
            "Epoch 00179: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2462 - acc: 0.9200 - val_loss: 0.3263 - val_acc: 0.8974\n",
            "Epoch 180/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2472 - acc: 0.9239\n",
            "Epoch 00180: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2462 - acc: 0.9243 - val_loss: 0.1947 - val_acc: 0.9398\n",
            "Epoch 181/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.2675 - acc: 0.9064\n",
            "Epoch 00181: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.2650 - acc: 0.9070 - val_loss: 0.1765 - val_acc: 0.9497\n",
            "Epoch 182/1000\n",
            "182/189 [===========================>..] - ETA: 0s - loss: 0.2398 - acc: 0.9217\n",
            "Epoch 00182: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2353 - acc: 0.9233 - val_loss: 0.1917 - val_acc: 0.9404\n",
            "Epoch 183/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2264 - acc: 0.9230\n",
            "Epoch 00183: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.2281 - acc: 0.9220 - val_loss: 0.2135 - val_acc: 0.9332\n",
            "Epoch 184/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2295 - acc: 0.9250\n",
            "Epoch 00184: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.2301 - acc: 0.9252 - val_loss: 0.6709 - val_acc: 0.8015\n",
            "Epoch 185/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2554 - acc: 0.9125\n",
            "Epoch 00185: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2554 - acc: 0.9126 - val_loss: 0.2131 - val_acc: 0.9338\n",
            "Epoch 186/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2305 - acc: 0.9227\n",
            "Epoch 00186: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2298 - acc: 0.9230 - val_loss: 0.2297 - val_acc: 0.9239\n",
            "Epoch 187/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2406 - acc: 0.9204\n",
            "Epoch 00187: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2403 - acc: 0.9209 - val_loss: 0.2017 - val_acc: 0.9411\n",
            "Epoch 188/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2280 - acc: 0.9250\n",
            "Epoch 00188: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2280 - acc: 0.9248 - val_loss: 0.2379 - val_acc: 0.9232\n",
            "Epoch 189/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2363 - acc: 0.9213\n",
            "Epoch 00189: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2372 - acc: 0.9209 - val_loss: 0.3761 - val_acc: 0.8769\n",
            "Epoch 190/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2538 - acc: 0.9130\n",
            "Epoch 00190: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2534 - acc: 0.9134 - val_loss: 0.3031 - val_acc: 0.8948\n",
            "Epoch 191/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2309 - acc: 0.9213\n",
            "Epoch 00191: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2313 - acc: 0.9217 - val_loss: 0.2920 - val_acc: 0.9014\n",
            "Epoch 192/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.2659 - acc: 0.9114\n",
            "Epoch 00192: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2659 - acc: 0.9111 - val_loss: 0.2721 - val_acc: 0.9193\n",
            "Epoch 193/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2490 - acc: 0.9140\n",
            "Epoch 00193: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2501 - acc: 0.9137 - val_loss: 0.3332 - val_acc: 0.8921\n",
            "Epoch 194/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2410 - acc: 0.9192\n",
            "Epoch 00194: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2410 - acc: 0.9192 - val_loss: 0.2973 - val_acc: 0.9001\n",
            "Epoch 195/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2412 - acc: 0.9173\n",
            "Epoch 00195: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2404 - acc: 0.9177 - val_loss: 0.2246 - val_acc: 0.9332\n",
            "Epoch 196/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2529 - acc: 0.9166\n",
            "Epoch 00196: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2532 - acc: 0.9162 - val_loss: 0.2190 - val_acc: 0.9332\n",
            "Epoch 197/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.2398 - acc: 0.9182\n",
            "Epoch 00197: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2375 - acc: 0.9190 - val_loss: 0.2010 - val_acc: 0.9298\n",
            "Epoch 198/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2272 - acc: 0.9255\n",
            "Epoch 00198: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2286 - acc: 0.9253 - val_loss: 0.1737 - val_acc: 0.9471\n",
            "Epoch 199/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.2069 - acc: 0.9334\n",
            "Epoch 00199: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2074 - acc: 0.9329 - val_loss: 0.2596 - val_acc: 0.9146\n",
            "Epoch 200/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.2577 - acc: 0.9158\n",
            "Epoch 00200: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2560 - acc: 0.9166 - val_loss: 0.2156 - val_acc: 0.9298\n",
            "Epoch 201/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2204 - acc: 0.9275\n",
            "Epoch 00201: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2214 - acc: 0.9270 - val_loss: 0.2102 - val_acc: 0.9332\n",
            "Epoch 202/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2483 - acc: 0.9207\n",
            "Epoch 00202: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2482 - acc: 0.9205 - val_loss: 0.2459 - val_acc: 0.9239\n",
            "Epoch 203/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2216 - acc: 0.9283\n",
            "Epoch 00203: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2211 - acc: 0.9283 - val_loss: 0.1935 - val_acc: 0.9391\n",
            "Epoch 204/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2372 - acc: 0.9215\n",
            "Epoch 00204: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2365 - acc: 0.9217 - val_loss: 0.2015 - val_acc: 0.9404\n",
            "Epoch 205/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2532 - acc: 0.9139\n",
            "Epoch 00205: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2538 - acc: 0.9136 - val_loss: 0.4245 - val_acc: 0.8551\n",
            "Epoch 206/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.2546 - acc: 0.9185\n",
            "Epoch 00206: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2590 - acc: 0.9166 - val_loss: 0.7115 - val_acc: 0.7624\n",
            "Epoch 207/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2445 - acc: 0.9187\n",
            "Epoch 00207: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2445 - acc: 0.9187 - val_loss: 0.2059 - val_acc: 0.9404\n",
            "Epoch 208/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2301 - acc: 0.9227\n",
            "Epoch 00208: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2301 - acc: 0.9227 - val_loss: 0.2081 - val_acc: 0.9351\n",
            "Epoch 209/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.2579 - acc: 0.9160\n",
            "Epoch 00209: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 1s 8ms/step - loss: 0.2571 - acc: 0.9159 - val_loss: 0.2186 - val_acc: 0.9292\n",
            "Epoch 210/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.2485 - acc: 0.9166\n",
            "Epoch 00210: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2476 - acc: 0.9166 - val_loss: 0.1917 - val_acc: 0.9464\n",
            "Epoch 211/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2163 - acc: 0.9275\n",
            "Epoch 00211: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2152 - acc: 0.9278 - val_loss: 0.1946 - val_acc: 0.9431\n",
            "Epoch 212/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2283 - acc: 0.9222\n",
            "Epoch 00212: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2277 - acc: 0.9222 - val_loss: 0.1809 - val_acc: 0.9490\n",
            "Epoch 213/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2219 - acc: 0.9262\n",
            "Epoch 00213: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2207 - acc: 0.9262 - val_loss: 0.1947 - val_acc: 0.9358\n",
            "Epoch 214/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2647 - acc: 0.9129\n",
            "Epoch 00214: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2631 - acc: 0.9134 - val_loss: 0.1883 - val_acc: 0.9457\n",
            "Epoch 215/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.2923 - acc: 0.9012\n",
            "Epoch 00215: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2955 - acc: 0.9003 - val_loss: 0.3332 - val_acc: 0.9073\n",
            "Epoch 216/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.3259 - acc: 0.8878\n",
            "Epoch 00216: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.3247 - acc: 0.8882 - val_loss: 0.3930 - val_acc: 0.8643\n",
            "Epoch 217/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2230 - acc: 0.9267\n",
            "Epoch 00217: val_acc did not improve from 0.95036\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2228 - acc: 0.9265 - val_loss: 0.1763 - val_acc: 0.9490\n",
            "Epoch 218/1000\n",
            "182/189 [===========================>..] - ETA: 0s - loss: 0.2266 - acc: 0.9287\n",
            "Epoch 00218: val_acc improved from 0.95036 to 0.95103, saving model to best_model_1.hdf5\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2305 - acc: 0.9263 - val_loss: 0.1868 - val_acc: 0.9510\n",
            "Epoch 219/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2309 - acc: 0.9218\n",
            "Epoch 00219: val_acc did not improve from 0.95103\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2315 - acc: 0.9217 - val_loss: 0.2277 - val_acc: 0.9292\n",
            "Epoch 220/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2099 - acc: 0.9301\n",
            "Epoch 00220: val_acc did not improve from 0.95103\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2086 - acc: 0.9306 - val_loss: 0.2369 - val_acc: 0.9332\n",
            "Epoch 221/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2385 - acc: 0.9220\n",
            "Epoch 00221: val_acc did not improve from 0.95103\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2373 - acc: 0.9225 - val_loss: 0.1827 - val_acc: 0.9477\n",
            "Epoch 222/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.2466 - acc: 0.9220\n",
            "Epoch 00222: val_acc did not improve from 0.95103\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2457 - acc: 0.9220 - val_loss: 0.2769 - val_acc: 0.9113\n",
            "Epoch 223/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2371 - acc: 0.9198\n",
            "Epoch 00223: val_acc did not improve from 0.95103\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2374 - acc: 0.9197 - val_loss: 0.2297 - val_acc: 0.9272\n",
            "Epoch 224/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2241 - acc: 0.9237\n",
            "Epoch 00224: val_acc did not improve from 0.95103\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2234 - acc: 0.9238 - val_loss: 0.1904 - val_acc: 0.9437\n",
            "Epoch 225/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2157 - acc: 0.9291\n",
            "Epoch 00225: val_acc did not improve from 0.95103\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2167 - acc: 0.9285 - val_loss: 0.2004 - val_acc: 0.9358\n",
            "Epoch 226/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2539 - acc: 0.9152\n",
            "Epoch 00226: val_acc did not improve from 0.95103\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2543 - acc: 0.9149 - val_loss: 0.2320 - val_acc: 0.9173\n",
            "Epoch 227/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2496 - acc: 0.9152\n",
            "Epoch 00227: val_acc did not improve from 0.95103\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2508 - acc: 0.9147 - val_loss: 0.5144 - val_acc: 0.7565\n",
            "Epoch 228/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2471 - acc: 0.9111\n",
            "Epoch 00228: val_acc improved from 0.95103 to 0.95301, saving model to best_model_1.hdf5\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2503 - acc: 0.9106 - val_loss: 0.1680 - val_acc: 0.9530\n",
            "Epoch 229/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2257 - acc: 0.9259\n",
            "Epoch 00229: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2265 - acc: 0.9257 - val_loss: 0.2029 - val_acc: 0.9411\n",
            "Epoch 230/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2409 - acc: 0.9205\n",
            "Epoch 00230: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2409 - acc: 0.9205 - val_loss: 0.3237 - val_acc: 0.9034\n",
            "Epoch 231/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.2229 - acc: 0.9240\n",
            "Epoch 00231: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2193 - acc: 0.9253 - val_loss: 0.2296 - val_acc: 0.9298\n",
            "Epoch 232/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2470 - acc: 0.9215\n",
            "Epoch 00232: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2470 - acc: 0.9215 - val_loss: 0.1987 - val_acc: 0.9404\n",
            "Epoch 233/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.2396 - acc: 0.9143\n",
            "Epoch 00233: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2365 - acc: 0.9156 - val_loss: 0.1884 - val_acc: 0.9424\n",
            "Epoch 234/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2175 - acc: 0.9267\n",
            "Epoch 00234: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2175 - acc: 0.9267 - val_loss: 0.2298 - val_acc: 0.9173\n",
            "Epoch 235/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.2331 - acc: 0.9221\n",
            "Epoch 00235: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2320 - acc: 0.9228 - val_loss: 0.2579 - val_acc: 0.9193\n",
            "Epoch 236/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.2058 - acc: 0.9317\n",
            "Epoch 00236: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2087 - acc: 0.9308 - val_loss: 0.3050 - val_acc: 0.8981\n",
            "Epoch 237/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.2454 - acc: 0.9166\n",
            "Epoch 00237: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 11ms/step - loss: 0.2496 - acc: 0.9154 - val_loss: 0.3172 - val_acc: 0.8855\n",
            "Epoch 238/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2622 - acc: 0.9139\n",
            "Epoch 00238: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2607 - acc: 0.9144 - val_loss: 0.1819 - val_acc: 0.9490\n",
            "Epoch 239/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2148 - acc: 0.9285\n",
            "Epoch 00239: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2152 - acc: 0.9283 - val_loss: 0.1765 - val_acc: 0.9497\n",
            "Epoch 240/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2308 - acc: 0.9248\n",
            "Epoch 00240: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2298 - acc: 0.9250 - val_loss: 0.3005 - val_acc: 0.9040\n",
            "Epoch 241/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2263 - acc: 0.9241\n",
            "Epoch 00241: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2268 - acc: 0.9238 - val_loss: 0.2332 - val_acc: 0.9193\n",
            "Epoch 242/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2446 - acc: 0.9230\n",
            "Epoch 00242: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2446 - acc: 0.9230 - val_loss: 0.2261 - val_acc: 0.9279\n",
            "Epoch 243/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2072 - acc: 0.9323\n",
            "Epoch 00243: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2055 - acc: 0.9329 - val_loss: 0.2170 - val_acc: 0.9312\n",
            "Epoch 244/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.2215 - acc: 0.9266\n",
            "Epoch 00244: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2209 - acc: 0.9268 - val_loss: 0.2807 - val_acc: 0.9027\n",
            "Epoch 245/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2460 - acc: 0.9190\n",
            "Epoch 00245: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2458 - acc: 0.9190 - val_loss: 0.2034 - val_acc: 0.9351\n",
            "Epoch 246/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2080 - acc: 0.9332\n",
            "Epoch 00246: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2087 - acc: 0.9329 - val_loss: 0.1902 - val_acc: 0.9411\n",
            "Epoch 247/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.1853 - acc: 0.9389\n",
            "Epoch 00247: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1853 - acc: 0.9389 - val_loss: 0.1805 - val_acc: 0.9504\n",
            "Epoch 248/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.2483 - acc: 0.9169\n",
            "Epoch 00248: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2528 - acc: 0.9157 - val_loss: 0.3597 - val_acc: 0.8795\n",
            "Epoch 249/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2463 - acc: 0.9207\n",
            "Epoch 00249: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2447 - acc: 0.9214 - val_loss: 0.2325 - val_acc: 0.9265\n",
            "Epoch 250/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2234 - acc: 0.9296\n",
            "Epoch 00250: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2225 - acc: 0.9295 - val_loss: 0.1781 - val_acc: 0.9523\n",
            "Epoch 251/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.2054 - acc: 0.9322\n",
            "Epoch 00251: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2092 - acc: 0.9313 - val_loss: 0.1985 - val_acc: 0.9385\n",
            "Epoch 252/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2394 - acc: 0.9222\n",
            "Epoch 00252: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2394 - acc: 0.9222 - val_loss: 0.1804 - val_acc: 0.9510\n",
            "Epoch 253/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.1961 - acc: 0.9357\n",
            "Epoch 00253: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1954 - acc: 0.9358 - val_loss: 0.2176 - val_acc: 0.9358\n",
            "Epoch 254/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2313 - acc: 0.9231\n",
            "Epoch 00254: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2300 - acc: 0.9233 - val_loss: 0.1715 - val_acc: 0.9457\n",
            "Epoch 255/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2048 - acc: 0.9316\n",
            "Epoch 00255: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2036 - acc: 0.9320 - val_loss: 0.2606 - val_acc: 0.9133\n",
            "Epoch 256/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2591 - acc: 0.9110\n",
            "Epoch 00256: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2576 - acc: 0.9119 - val_loss: 0.2433 - val_acc: 0.9305\n",
            "Epoch 257/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2463 - acc: 0.9186\n",
            "Epoch 00257: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2466 - acc: 0.9187 - val_loss: 0.2364 - val_acc: 0.9365\n",
            "Epoch 258/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2156 - acc: 0.9300\n",
            "Epoch 00258: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2158 - acc: 0.9301 - val_loss: 0.1795 - val_acc: 0.9477\n",
            "Epoch 259/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2201 - acc: 0.9273\n",
            "Epoch 00259: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2195 - acc: 0.9275 - val_loss: 0.1967 - val_acc: 0.9437\n",
            "Epoch 260/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.2533 - acc: 0.9168\n",
            "Epoch 00260: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2560 - acc: 0.9162 - val_loss: 0.2814 - val_acc: 0.9080\n",
            "Epoch 261/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2048 - acc: 0.9328\n",
            "Epoch 00261: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 8ms/step - loss: 0.2039 - acc: 0.9331 - val_loss: 0.2465 - val_acc: 0.9252\n",
            "Epoch 262/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2068 - acc: 0.9343\n",
            "Epoch 00262: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2064 - acc: 0.9346 - val_loss: 0.2329 - val_acc: 0.9232\n",
            "Epoch 263/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2260 - acc: 0.9280\n",
            "Epoch 00263: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2260 - acc: 0.9280 - val_loss: 0.1739 - val_acc: 0.9504\n",
            "Epoch 264/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2014 - acc: 0.9343\n",
            "Epoch 00264: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2029 - acc: 0.9341 - val_loss: 0.2137 - val_acc: 0.9298\n",
            "Epoch 265/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2320 - acc: 0.9236\n",
            "Epoch 00265: val_acc did not improve from 0.95301\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2315 - acc: 0.9238 - val_loss: 0.2108 - val_acc: 0.9385\n",
            "Epoch 266/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.2203 - acc: 0.9276\n",
            "Epoch 00266: val_acc improved from 0.95301 to 0.95764, saving model to best_model_1.hdf5\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2187 - acc: 0.9278 - val_loss: 0.1653 - val_acc: 0.9576\n",
            "Epoch 267/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2283 - acc: 0.9243\n",
            "Epoch 00267: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2271 - acc: 0.9248 - val_loss: 0.2045 - val_acc: 0.9371\n",
            "Epoch 268/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2147 - acc: 0.9304\n",
            "Epoch 00268: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2154 - acc: 0.9301 - val_loss: 0.2187 - val_acc: 0.9338\n",
            "Epoch 269/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.2293 - acc: 0.9206\n",
            "Epoch 00269: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2337 - acc: 0.9187 - val_loss: 0.1662 - val_acc: 0.9543\n",
            "Epoch 270/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2032 - acc: 0.9328\n",
            "Epoch 00270: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2025 - acc: 0.9331 - val_loss: 0.1759 - val_acc: 0.9497\n",
            "Epoch 271/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.2004 - acc: 0.9361\n",
            "Epoch 00271: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1983 - acc: 0.9368 - val_loss: 0.1823 - val_acc: 0.9451\n",
            "Epoch 272/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2071 - acc: 0.9301\n",
            "Epoch 00272: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2082 - acc: 0.9303 - val_loss: 0.1849 - val_acc: 0.9398\n",
            "Epoch 273/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2046 - acc: 0.9332\n",
            "Epoch 00273: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2049 - acc: 0.9331 - val_loss: 0.3067 - val_acc: 0.9080\n",
            "Epoch 274/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2626 - acc: 0.9121\n",
            "Epoch 00274: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2626 - acc: 0.9119 - val_loss: 0.2209 - val_acc: 0.9325\n",
            "Epoch 275/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.1970 - acc: 0.9373\n",
            "Epoch 00275: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1959 - acc: 0.9377 - val_loss: 0.1648 - val_acc: 0.9530\n",
            "Epoch 276/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2059 - acc: 0.9341\n",
            "Epoch 00276: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2055 - acc: 0.9341 - val_loss: 0.3572 - val_acc: 0.8723\n",
            "Epoch 277/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.1960 - acc: 0.9350\n",
            "Epoch 00277: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1973 - acc: 0.9346 - val_loss: 0.2526 - val_acc: 0.9206\n",
            "Epoch 278/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2331 - acc: 0.9213\n",
            "Epoch 00278: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2329 - acc: 0.9215 - val_loss: 0.2481 - val_acc: 0.9159\n",
            "Epoch 279/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2109 - acc: 0.9257\n",
            "Epoch 00279: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2109 - acc: 0.9257 - val_loss: 0.2421 - val_acc: 0.9212\n",
            "Epoch 280/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2383 - acc: 0.9219\n",
            "Epoch 00280: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2383 - acc: 0.9219 - val_loss: 0.4667 - val_acc: 0.8134\n",
            "Epoch 281/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2546 - acc: 0.9137\n",
            "Epoch 00281: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2543 - acc: 0.9139 - val_loss: 0.1982 - val_acc: 0.9411\n",
            "Epoch 282/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2038 - acc: 0.9319\n",
            "Epoch 00282: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2049 - acc: 0.9318 - val_loss: 0.2332 - val_acc: 0.9305\n",
            "Epoch 283/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9231\n",
            "Epoch 00283: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2308 - acc: 0.9228 - val_loss: 0.3611 - val_acc: 0.8795\n",
            "Epoch 284/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2217 - acc: 0.9266\n",
            "Epoch 00284: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2217 - acc: 0.9265 - val_loss: 0.2158 - val_acc: 0.9351\n",
            "Epoch 285/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2242 - acc: 0.9269\n",
            "Epoch 00285: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2245 - acc: 0.9267 - val_loss: 0.2094 - val_acc: 0.9385\n",
            "Epoch 286/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2127 - acc: 0.9279\n",
            "Epoch 00286: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2126 - acc: 0.9276 - val_loss: 0.2132 - val_acc: 0.9385\n",
            "Epoch 287/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2927 - acc: 0.9000\n",
            "Epoch 00287: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2941 - acc: 0.8997 - val_loss: 0.4234 - val_acc: 0.8551\n",
            "Epoch 288/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2112 - acc: 0.9315\n",
            "Epoch 00288: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2096 - acc: 0.9321 - val_loss: 0.3078 - val_acc: 0.9034\n",
            "Epoch 289/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.1973 - acc: 0.9359\n",
            "Epoch 00289: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1973 - acc: 0.9359 - val_loss: 0.1645 - val_acc: 0.9543\n",
            "Epoch 290/1000\n",
            "182/189 [===========================>..] - ETA: 0s - loss: 0.2072 - acc: 0.9313\n",
            "Epoch 00290: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2048 - acc: 0.9323 - val_loss: 0.1772 - val_acc: 0.9517\n",
            "Epoch 291/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.1906 - acc: 0.9380\n",
            "Epoch 00291: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1892 - acc: 0.9381 - val_loss: 0.2093 - val_acc: 0.9332\n",
            "Epoch 292/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2161 - acc: 0.9295\n",
            "Epoch 00292: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2161 - acc: 0.9295 - val_loss: 0.2092 - val_acc: 0.9404\n",
            "Epoch 293/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.1916 - acc: 0.9365\n",
            "Epoch 00293: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1913 - acc: 0.9366 - val_loss: 0.1787 - val_acc: 0.9510\n",
            "Epoch 294/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2039 - acc: 0.9337\n",
            "Epoch 00294: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2045 - acc: 0.9334 - val_loss: 0.1906 - val_acc: 0.9398\n",
            "Epoch 295/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2048 - acc: 0.9306\n",
            "Epoch 00295: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2056 - acc: 0.9303 - val_loss: 0.2587 - val_acc: 0.9133\n",
            "Epoch 296/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2260 - acc: 0.9241\n",
            "Epoch 00296: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2296 - acc: 0.9230 - val_loss: 0.3079 - val_acc: 0.8981\n",
            "Epoch 297/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2037 - acc: 0.9329\n",
            "Epoch 00297: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2032 - acc: 0.9329 - val_loss: 0.2001 - val_acc: 0.9378\n",
            "Epoch 298/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2282 - acc: 0.9251\n",
            "Epoch 00298: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2306 - acc: 0.9250 - val_loss: 0.2273 - val_acc: 0.9318\n",
            "Epoch 299/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2169 - acc: 0.9284\n",
            "Epoch 00299: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2160 - acc: 0.9288 - val_loss: 0.2264 - val_acc: 0.9292\n",
            "Epoch 300/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.2091 - acc: 0.9322\n",
            "Epoch 00300: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2086 - acc: 0.9320 - val_loss: 0.1803 - val_acc: 0.9510\n",
            "Epoch 301/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2632 - acc: 0.9101\n",
            "Epoch 00301: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2620 - acc: 0.9104 - val_loss: 0.1866 - val_acc: 0.9484\n",
            "Epoch 302/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.2300 - acc: 0.9226\n",
            "Epoch 00302: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2322 - acc: 0.9219 - val_loss: 0.4075 - val_acc: 0.8670\n",
            "Epoch 303/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.2118 - acc: 0.9273\n",
            "Epoch 00303: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2101 - acc: 0.9278 - val_loss: 0.1716 - val_acc: 0.9477\n",
            "Epoch 304/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2304 - acc: 0.9246\n",
            "Epoch 00304: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2282 - acc: 0.9252 - val_loss: 0.1610 - val_acc: 0.9557\n",
            "Epoch 305/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.1878 - acc: 0.9363\n",
            "Epoch 00305: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1857 - acc: 0.9371 - val_loss: 0.1744 - val_acc: 0.9523\n",
            "Epoch 306/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2126 - acc: 0.9263\n",
            "Epoch 00306: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2148 - acc: 0.9260 - val_loss: 0.2175 - val_acc: 0.9358\n",
            "Epoch 307/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2261 - acc: 0.9228\n",
            "Epoch 00307: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2259 - acc: 0.9232 - val_loss: 0.1865 - val_acc: 0.9471\n",
            "Epoch 308/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.2100 - acc: 0.9305\n",
            "Epoch 00308: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2090 - acc: 0.9313 - val_loss: 0.2636 - val_acc: 0.9239\n",
            "Epoch 309/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2086 - acc: 0.9309\n",
            "Epoch 00309: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2084 - acc: 0.9308 - val_loss: 0.2073 - val_acc: 0.9365\n",
            "Epoch 310/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2123 - acc: 0.9293\n",
            "Epoch 00310: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2114 - acc: 0.9298 - val_loss: 0.2217 - val_acc: 0.9318\n",
            "Epoch 311/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.2329 - acc: 0.9217\n",
            "Epoch 00311: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2340 - acc: 0.9210 - val_loss: 0.2284 - val_acc: 0.9232\n",
            "Epoch 312/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2007 - acc: 0.9352\n",
            "Epoch 00312: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1997 - acc: 0.9356 - val_loss: 0.1880 - val_acc: 0.9490\n",
            "Epoch 313/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.2147 - acc: 0.9276\n",
            "Epoch 00313: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2123 - acc: 0.9285 - val_loss: 0.2147 - val_acc: 0.9351\n",
            "Epoch 314/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.1981 - acc: 0.9366\n",
            "Epoch 00314: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1965 - acc: 0.9371 - val_loss: 0.1903 - val_acc: 0.9457\n",
            "Epoch 315/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.1948 - acc: 0.9363\n",
            "Epoch 00315: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1949 - acc: 0.9361 - val_loss: 0.2617 - val_acc: 0.9133\n",
            "Epoch 316/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2322 - acc: 0.9233\n",
            "Epoch 00316: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2322 - acc: 0.9233 - val_loss: 0.1626 - val_acc: 0.9477\n",
            "Epoch 317/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.3748 - acc: 0.8703\n",
            "Epoch 00317: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.3764 - acc: 0.8690 - val_loss: 0.7853 - val_acc: 0.7346\n",
            "Epoch 318/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.3599 - acc: 0.8807\n",
            "Epoch 00318: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.3578 - acc: 0.8813 - val_loss: 0.2937 - val_acc: 0.9186\n",
            "Epoch 319/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.3208 - acc: 0.8928\n",
            "Epoch 00319: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 11ms/step - loss: 0.3207 - acc: 0.8929 - val_loss: 0.3406 - val_acc: 0.8981\n",
            "Epoch 320/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.3212 - acc: 0.8912\n",
            "Epoch 00320: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.3202 - acc: 0.8919 - val_loss: 0.2810 - val_acc: 0.9186\n",
            "Epoch 321/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.3255 - acc: 0.8929\n",
            "Epoch 00321: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.3249 - acc: 0.8929 - val_loss: 0.3168 - val_acc: 0.9021\n",
            "Epoch 322/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2865 - acc: 0.9110\n",
            "Epoch 00322: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2873 - acc: 0.9104 - val_loss: 0.2838 - val_acc: 0.9140\n",
            "Epoch 323/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2246 - acc: 0.9311\n",
            "Epoch 00323: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2235 - acc: 0.9311 - val_loss: 0.1951 - val_acc: 0.9424\n",
            "Epoch 324/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.1918 - acc: 0.9348\n",
            "Epoch 00324: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1920 - acc: 0.9346 - val_loss: 0.1962 - val_acc: 0.9411\n",
            "Epoch 325/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9414\n",
            "Epoch 00325: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1825 - acc: 0.9414 - val_loss: 0.2958 - val_acc: 0.9034\n",
            "Epoch 326/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2141 - acc: 0.9296\n",
            "Epoch 00326: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2115 - acc: 0.9303 - val_loss: 0.2915 - val_acc: 0.9126\n",
            "Epoch 327/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.1918 - acc: 0.9385\n",
            "Epoch 00327: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1933 - acc: 0.9381 - val_loss: 0.1692 - val_acc: 0.9543\n",
            "Epoch 328/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2170 - acc: 0.9276\n",
            "Epoch 00328: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2170 - acc: 0.9276 - val_loss: 0.1756 - val_acc: 0.9490\n",
            "Epoch 329/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2012 - acc: 0.9327\n",
            "Epoch 00329: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2009 - acc: 0.9326 - val_loss: 0.2085 - val_acc: 0.9332\n",
            "Epoch 330/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2198 - acc: 0.9244\n",
            "Epoch 00330: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2195 - acc: 0.9245 - val_loss: 0.1726 - val_acc: 0.9504\n",
            "Epoch 331/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2108 - acc: 0.9315\n",
            "Epoch 00331: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2101 - acc: 0.9316 - val_loss: 0.1781 - val_acc: 0.9504\n",
            "Epoch 332/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2122 - acc: 0.9303\n",
            "Epoch 00332: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2120 - acc: 0.9301 - val_loss: 0.3177 - val_acc: 0.9007\n",
            "Epoch 333/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2074 - acc: 0.9338\n",
            "Epoch 00333: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2056 - acc: 0.9346 - val_loss: 0.1913 - val_acc: 0.9418\n",
            "Epoch 334/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.1935 - acc: 0.9373\n",
            "Epoch 00334: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1973 - acc: 0.9364 - val_loss: 0.2255 - val_acc: 0.9279\n",
            "Epoch 335/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2233 - acc: 0.9216\n",
            "Epoch 00335: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2231 - acc: 0.9219 - val_loss: 0.2187 - val_acc: 0.9259\n",
            "Epoch 336/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.1955 - acc: 0.9343\n",
            "Epoch 00336: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1965 - acc: 0.9339 - val_loss: 0.2092 - val_acc: 0.9418\n",
            "Epoch 337/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2069 - acc: 0.9306\n",
            "Epoch 00337: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2069 - acc: 0.9306 - val_loss: 0.2009 - val_acc: 0.9365\n",
            "Epoch 338/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.1924 - acc: 0.9362\n",
            "Epoch 00338: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1923 - acc: 0.9363 - val_loss: 0.1730 - val_acc: 0.9471\n",
            "Epoch 339/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2198 - acc: 0.9264\n",
            "Epoch 00339: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2198 - acc: 0.9265 - val_loss: 0.1763 - val_acc: 0.9484\n",
            "Epoch 340/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2184 - acc: 0.9261\n",
            "Epoch 00340: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2172 - acc: 0.9263 - val_loss: 0.2230 - val_acc: 0.9305\n",
            "Epoch 341/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2117 - acc: 0.9292\n",
            "Epoch 00341: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2110 - acc: 0.9295 - val_loss: 0.1682 - val_acc: 0.9523\n",
            "Epoch 342/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.1821 - acc: 0.9387\n",
            "Epoch 00342: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1821 - acc: 0.9387 - val_loss: 0.2099 - val_acc: 0.9398\n",
            "Epoch 343/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.2036 - acc: 0.9310\n",
            "Epoch 00343: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 11ms/step - loss: 0.2059 - acc: 0.9301 - val_loss: 0.2722 - val_acc: 0.9120\n",
            "Epoch 344/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2221 - acc: 0.9288\n",
            "Epoch 00344: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2216 - acc: 0.9290 - val_loss: 0.1902 - val_acc: 0.9471\n",
            "Epoch 345/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.1874 - acc: 0.9411\n",
            "Epoch 00345: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1880 - acc: 0.9407 - val_loss: 0.2111 - val_acc: 0.9371\n",
            "Epoch 346/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2019 - acc: 0.9368\n",
            "Epoch 00346: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2017 - acc: 0.9368 - val_loss: 0.2625 - val_acc: 0.9087\n",
            "Epoch 347/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2116 - acc: 0.9267\n",
            "Epoch 00347: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2130 - acc: 0.9257 - val_loss: 0.1758 - val_acc: 0.9477\n",
            "Epoch 348/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2026 - acc: 0.9355\n",
            "Epoch 00348: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2045 - acc: 0.9351 - val_loss: 0.1709 - val_acc: 0.9490\n",
            "Epoch 349/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2180 - acc: 0.9271\n",
            "Epoch 00349: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2189 - acc: 0.9270 - val_loss: 0.1866 - val_acc: 0.9464\n",
            "Epoch 350/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2144 - acc: 0.9296\n",
            "Epoch 00350: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2144 - acc: 0.9296 - val_loss: 0.2052 - val_acc: 0.9358\n",
            "Epoch 351/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.1906 - acc: 0.9383\n",
            "Epoch 00351: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1901 - acc: 0.9382 - val_loss: 0.1759 - val_acc: 0.9464\n",
            "Epoch 352/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2055 - acc: 0.9316\n",
            "Epoch 00352: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2055 - acc: 0.9316 - val_loss: 0.2652 - val_acc: 0.9140\n",
            "Epoch 353/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.1979 - acc: 0.9323\n",
            "Epoch 00353: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1979 - acc: 0.9323 - val_loss: 0.1891 - val_acc: 0.9437\n",
            "Epoch 354/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.1960 - acc: 0.9377\n",
            "Epoch 00354: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1963 - acc: 0.9379 - val_loss: 0.1970 - val_acc: 0.9391\n",
            "Epoch 355/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.1982 - acc: 0.9375\n",
            "Epoch 00355: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2025 - acc: 0.9359 - val_loss: 0.3624 - val_acc: 0.8842\n",
            "Epoch 356/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2039 - acc: 0.9377\n",
            "Epoch 00356: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2059 - acc: 0.9369 - val_loss: 0.2677 - val_acc: 0.9186\n",
            "Epoch 357/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2047 - acc: 0.9309\n",
            "Epoch 00357: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2047 - acc: 0.9310 - val_loss: 0.1883 - val_acc: 0.9444\n",
            "Epoch 358/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2313 - acc: 0.9242\n",
            "Epoch 00358: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2299 - acc: 0.9248 - val_loss: 0.1770 - val_acc: 0.9457\n",
            "Epoch 359/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.1922 - acc: 0.9365\n",
            "Epoch 00359: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1921 - acc: 0.9363 - val_loss: 0.2255 - val_acc: 0.9305\n",
            "Epoch 360/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2163 - acc: 0.9309\n",
            "Epoch 00360: val_acc did not improve from 0.95764\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2162 - acc: 0.9313 - val_loss: 0.3562 - val_acc: 0.8895\n",
            "Epoch 361/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.1935 - acc: 0.9356\n",
            "Epoch 00361: val_acc improved from 0.95764 to 0.95831, saving model to best_model_1.hdf5\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1935 - acc: 0.9356 - val_loss: 0.1590 - val_acc: 0.9583\n",
            "Epoch 362/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.1814 - acc: 0.9417\n",
            "Epoch 00362: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1814 - acc: 0.9417 - val_loss: 0.1599 - val_acc: 0.9510\n",
            "Epoch 363/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.1870 - acc: 0.9411\n",
            "Epoch 00363: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1873 - acc: 0.9409 - val_loss: 0.2480 - val_acc: 0.9219\n",
            "Epoch 364/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.1840 - acc: 0.9411\n",
            "Epoch 00364: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1824 - acc: 0.9416 - val_loss: 0.1924 - val_acc: 0.9424\n",
            "Epoch 365/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.1960 - acc: 0.9346\n",
            "Epoch 00365: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1960 - acc: 0.9346 - val_loss: 0.2784 - val_acc: 0.9087\n",
            "Epoch 366/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.1908 - acc: 0.9352\n",
            "Epoch 00366: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1903 - acc: 0.9353 - val_loss: 0.2324 - val_acc: 0.9292\n",
            "Epoch 367/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.2181 - acc: 0.9231\n",
            "Epoch 00367: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2173 - acc: 0.9228 - val_loss: 0.2900 - val_acc: 0.9047\n",
            "Epoch 368/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2053 - acc: 0.9341\n",
            "Epoch 00368: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2055 - acc: 0.9341 - val_loss: 0.2497 - val_acc: 0.9206\n",
            "Epoch 369/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.1958 - acc: 0.9362\n",
            "Epoch 00369: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1953 - acc: 0.9363 - val_loss: 0.2312 - val_acc: 0.9318\n",
            "Epoch 370/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2393 - acc: 0.9210\n",
            "Epoch 00370: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2387 - acc: 0.9212 - val_loss: 0.3878 - val_acc: 0.8756\n",
            "Epoch 371/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.1977 - acc: 0.9358\n",
            "Epoch 00371: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1976 - acc: 0.9353 - val_loss: 0.1893 - val_acc: 0.9391\n",
            "Epoch 372/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.1940 - acc: 0.9360\n",
            "Epoch 00372: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1955 - acc: 0.9353 - val_loss: 0.2341 - val_acc: 0.9312\n",
            "Epoch 373/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2023 - acc: 0.9345\n",
            "Epoch 00373: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2028 - acc: 0.9344 - val_loss: 0.4856 - val_acc: 0.8504\n",
            "Epoch 374/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2535 - acc: 0.9131\n",
            "Epoch 00374: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2535 - acc: 0.9131 - val_loss: 0.2875 - val_acc: 0.9073\n",
            "Epoch 375/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.1948 - acc: 0.9355\n",
            "Epoch 00375: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1938 - acc: 0.9358 - val_loss: 0.2363 - val_acc: 0.9259\n",
            "Epoch 376/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2086 - acc: 0.9323\n",
            "Epoch 00376: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2081 - acc: 0.9321 - val_loss: 0.2693 - val_acc: 0.9153\n",
            "Epoch 377/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.1910 - acc: 0.9353\n",
            "Epoch 00377: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1905 - acc: 0.9354 - val_loss: 0.1990 - val_acc: 0.9404\n",
            "Epoch 378/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.1719 - acc: 0.9410\n",
            "Epoch 00378: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1729 - acc: 0.9407 - val_loss: 0.1822 - val_acc: 0.9451\n",
            "Epoch 379/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2119 - acc: 0.9320\n",
            "Epoch 00379: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2119 - acc: 0.9320 - val_loss: 0.1906 - val_acc: 0.9444\n",
            "Epoch 380/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2018 - acc: 0.9339\n",
            "Epoch 00380: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2018 - acc: 0.9339 - val_loss: 0.1988 - val_acc: 0.9404\n",
            "Epoch 381/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.1894 - acc: 0.9372\n",
            "Epoch 00381: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1893 - acc: 0.9373 - val_loss: 0.3256 - val_acc: 0.8928\n",
            "Epoch 382/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2050 - acc: 0.9318\n",
            "Epoch 00382: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2050 - acc: 0.9318 - val_loss: 0.1743 - val_acc: 0.9490\n",
            "Epoch 383/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2232 - acc: 0.9260\n",
            "Epoch 00383: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2247 - acc: 0.9262 - val_loss: 0.2393 - val_acc: 0.9252\n",
            "Epoch 384/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.1993 - acc: 0.9348\n",
            "Epoch 00384: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1992 - acc: 0.9348 - val_loss: 0.1839 - val_acc: 0.9471\n",
            "Epoch 385/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2092 - acc: 0.9310\n",
            "Epoch 00385: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2085 - acc: 0.9313 - val_loss: 0.2292 - val_acc: 0.9312\n",
            "Epoch 386/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.1923 - acc: 0.9360\n",
            "Epoch 00386: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1933 - acc: 0.9361 - val_loss: 0.1540 - val_acc: 0.9517\n",
            "Epoch 387/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.1861 - acc: 0.9384\n",
            "Epoch 00387: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 11ms/step - loss: 0.1861 - acc: 0.9384 - val_loss: 0.2318 - val_acc: 0.9232\n",
            "Epoch 388/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2189 - acc: 0.9311\n",
            "Epoch 00388: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2183 - acc: 0.9315 - val_loss: 0.1824 - val_acc: 0.9471\n",
            "Epoch 389/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9337\n",
            "Epoch 00389: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1924 - acc: 0.9336 - val_loss: 0.1892 - val_acc: 0.9385\n",
            "Epoch 390/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2035 - acc: 0.9339\n",
            "Epoch 00390: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2035 - acc: 0.9339 - val_loss: 0.2132 - val_acc: 0.9371\n",
            "Epoch 391/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.1855 - acc: 0.9409\n",
            "Epoch 00391: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 12ms/step - loss: 0.1855 - acc: 0.9409 - val_loss: 0.1823 - val_acc: 0.9471\n",
            "Epoch 392/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.1955 - acc: 0.9375\n",
            "Epoch 00392: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1947 - acc: 0.9376 - val_loss: 0.1710 - val_acc: 0.9490\n",
            "Epoch 393/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2226 - acc: 0.9235\n",
            "Epoch 00393: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2226 - acc: 0.9235 - val_loss: 0.4394 - val_acc: 0.8537\n",
            "Epoch 394/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.1895 - acc: 0.9377\n",
            "Epoch 00394: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1895 - acc: 0.9377 - val_loss: 0.1834 - val_acc: 0.9431\n",
            "Epoch 395/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.1927 - acc: 0.9373\n",
            "Epoch 00395: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1927 - acc: 0.9376 - val_loss: 0.1797 - val_acc: 0.9504\n",
            "Epoch 396/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.1865 - acc: 0.9389\n",
            "Epoch 00396: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1865 - acc: 0.9389 - val_loss: 0.1768 - val_acc: 0.9471\n",
            "Epoch 397/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.2055 - acc: 0.9315\n",
            "Epoch 00397: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2052 - acc: 0.9316 - val_loss: 0.1578 - val_acc: 0.9530\n",
            "Epoch 398/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.1878 - acc: 0.9382\n",
            "Epoch 00398: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1878 - acc: 0.9382 - val_loss: 0.1806 - val_acc: 0.9471\n",
            "Epoch 399/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2005 - acc: 0.9351\n",
            "Epoch 00399: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2005 - acc: 0.9351 - val_loss: 0.1965 - val_acc: 0.9385\n",
            "Epoch 400/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.1837 - acc: 0.9410\n",
            "Epoch 00400: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1835 - acc: 0.9411 - val_loss: 0.1981 - val_acc: 0.9351\n",
            "Epoch 401/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.1836 - acc: 0.9400\n",
            "Epoch 00401: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1845 - acc: 0.9394 - val_loss: 0.1663 - val_acc: 0.9510\n",
            "Epoch 402/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.2166 - acc: 0.9309\n",
            "Epoch 00402: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2165 - acc: 0.9310 - val_loss: 0.1920 - val_acc: 0.9378\n",
            "Epoch 403/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2084 - acc: 0.9300\n",
            "Epoch 00403: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2084 - acc: 0.9300 - val_loss: 0.1876 - val_acc: 0.9365\n",
            "Epoch 404/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.2353 - acc: 0.9248\n",
            "Epoch 00404: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2393 - acc: 0.9228 - val_loss: 0.4046 - val_acc: 0.8577\n",
            "Epoch 405/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.1743 - acc: 0.9410\n",
            "Epoch 00405: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 11ms/step - loss: 0.1739 - acc: 0.9412 - val_loss: 0.1689 - val_acc: 0.9490\n",
            "Epoch 406/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2117 - acc: 0.9305\n",
            "Epoch 00406: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2129 - acc: 0.9300 - val_loss: 0.2089 - val_acc: 0.9312\n",
            "Epoch 407/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.1731 - acc: 0.9440\n",
            "Epoch 00407: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1723 - acc: 0.9445 - val_loss: 0.1585 - val_acc: 0.9576\n",
            "Epoch 408/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.1866 - acc: 0.9382\n",
            "Epoch 00408: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1869 - acc: 0.9382 - val_loss: 0.1802 - val_acc: 0.9437\n",
            "Epoch 409/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.1967 - acc: 0.9348\n",
            "Epoch 00409: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1967 - acc: 0.9348 - val_loss: 0.1746 - val_acc: 0.9510\n",
            "Epoch 410/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.1791 - acc: 0.9419\n",
            "Epoch 00410: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1791 - acc: 0.9419 - val_loss: 0.2527 - val_acc: 0.9226\n",
            "Epoch 411/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.2068 - acc: 0.9275\n",
            "Epoch 00411: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2070 - acc: 0.9278 - val_loss: 0.1632 - val_acc: 0.9563\n",
            "Epoch 412/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2208 - acc: 0.9233\n",
            "Epoch 00412: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2208 - acc: 0.9233 - val_loss: 0.1961 - val_acc: 0.9404\n",
            "Epoch 413/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2129 - acc: 0.9325\n",
            "Epoch 00413: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2115 - acc: 0.9328 - val_loss: 0.2463 - val_acc: 0.9179\n",
            "Epoch 414/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.1930 - acc: 0.9365\n",
            "Epoch 00414: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1918 - acc: 0.9368 - val_loss: 0.1960 - val_acc: 0.9371\n",
            "Epoch 415/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2065 - acc: 0.9315\n",
            "Epoch 00415: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 11ms/step - loss: 0.2065 - acc: 0.9315 - val_loss: 0.3465 - val_acc: 0.8868\n",
            "Epoch 416/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2002 - acc: 0.9373\n",
            "Epoch 00416: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1991 - acc: 0.9377 - val_loss: 0.1600 - val_acc: 0.9510\n",
            "Epoch 417/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2142 - acc: 0.9276\n",
            "Epoch 00417: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2142 - acc: 0.9276 - val_loss: 0.2631 - val_acc: 0.9087\n",
            "Epoch 418/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.1974 - acc: 0.9348\n",
            "Epoch 00418: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1977 - acc: 0.9348 - val_loss: 0.1931 - val_acc: 0.9404\n",
            "Epoch 419/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.1768 - acc: 0.9400\n",
            "Epoch 00419: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1783 - acc: 0.9392 - val_loss: 0.1828 - val_acc: 0.9477\n",
            "Epoch 420/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2045 - acc: 0.9289\n",
            "Epoch 00420: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2042 - acc: 0.9291 - val_loss: 0.1967 - val_acc: 0.9398\n",
            "Epoch 421/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.1921 - acc: 0.9407\n",
            "Epoch 00421: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 11ms/step - loss: 0.1975 - acc: 0.9397 - val_loss: 0.4017 - val_acc: 0.8683\n",
            "Epoch 422/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.2084 - acc: 0.9263\n",
            "Epoch 00422: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2061 - acc: 0.9273 - val_loss: 0.1876 - val_acc: 0.9411\n",
            "Epoch 423/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2104 - acc: 0.9323\n",
            "Epoch 00423: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2082 - acc: 0.9333 - val_loss: 0.2653 - val_acc: 0.9212\n",
            "Epoch 424/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2202 - acc: 0.9240\n",
            "Epoch 00424: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 11ms/step - loss: 0.2192 - acc: 0.9243 - val_loss: 0.3004 - val_acc: 0.8974\n",
            "Epoch 425/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2158 - acc: 0.9288\n",
            "Epoch 00425: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2158 - acc: 0.9288 - val_loss: 0.2381 - val_acc: 0.9252\n",
            "Epoch 426/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.1905 - acc: 0.9377\n",
            "Epoch 00426: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1911 - acc: 0.9374 - val_loss: 0.2637 - val_acc: 0.9232\n",
            "Epoch 427/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.2040 - acc: 0.9300\n",
            "Epoch 00427: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2048 - acc: 0.9295 - val_loss: 0.2551 - val_acc: 0.9239\n",
            "Epoch 428/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.1890 - acc: 0.9393\n",
            "Epoch 00428: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 11ms/step - loss: 0.1882 - acc: 0.9396 - val_loss: 0.1931 - val_acc: 0.9378\n",
            "Epoch 429/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.1861 - acc: 0.9373\n",
            "Epoch 00429: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1863 - acc: 0.9371 - val_loss: 0.1972 - val_acc: 0.9444\n",
            "Epoch 430/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.1714 - acc: 0.9462\n",
            "Epoch 00430: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1717 - acc: 0.9460 - val_loss: 0.2340 - val_acc: 0.9298\n",
            "Epoch 431/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.1932 - acc: 0.9355\n",
            "Epoch 00431: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1956 - acc: 0.9346 - val_loss: 0.1846 - val_acc: 0.9451\n",
            "Epoch 432/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.2090 - acc: 0.9307\n",
            "Epoch 00432: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2100 - acc: 0.9308 - val_loss: 0.3124 - val_acc: 0.8908\n",
            "Epoch 433/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.2090 - acc: 0.9296\n",
            "Epoch 00433: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2090 - acc: 0.9296 - val_loss: 0.3377 - val_acc: 0.8815\n",
            "Epoch 434/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.1969 - acc: 0.9405\n",
            "Epoch 00434: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1969 - acc: 0.9402 - val_loss: 0.1910 - val_acc: 0.9431\n",
            "Epoch 435/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.1898 - acc: 0.9389\n",
            "Epoch 00435: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 11ms/step - loss: 0.1885 - acc: 0.9387 - val_loss: 0.1768 - val_acc: 0.9504\n",
            "Epoch 436/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.1794 - acc: 0.9446\n",
            "Epoch 00436: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1788 - acc: 0.9444 - val_loss: 0.1458 - val_acc: 0.9576\n",
            "Epoch 437/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2000 - acc: 0.9343\n",
            "Epoch 00437: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2006 - acc: 0.9341 - val_loss: 0.1760 - val_acc: 0.9404\n",
            "Epoch 438/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2206 - acc: 0.9275\n",
            "Epoch 00438: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2208 - acc: 0.9275 - val_loss: 0.3070 - val_acc: 0.9067\n",
            "Epoch 439/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.1822 - acc: 0.9390\n",
            "Epoch 00439: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 11ms/step - loss: 0.1821 - acc: 0.9389 - val_loss: 0.1901 - val_acc: 0.9431\n",
            "Epoch 440/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.1913 - acc: 0.9353\n",
            "Epoch 00440: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1921 - acc: 0.9354 - val_loss: 0.1914 - val_acc: 0.9510\n",
            "Epoch 441/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.1916 - acc: 0.9339\n",
            "Epoch 00441: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1916 - acc: 0.9339 - val_loss: 0.2424 - val_acc: 0.9166\n",
            "Epoch 442/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.1935 - acc: 0.9367\n",
            "Epoch 00442: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1941 - acc: 0.9366 - val_loss: 0.1944 - val_acc: 0.9418\n",
            "Epoch 443/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.1968 - acc: 0.9380\n",
            "Epoch 00443: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1963 - acc: 0.9382 - val_loss: 0.2557 - val_acc: 0.9186\n",
            "Epoch 444/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.2005 - acc: 0.9368\n",
            "Epoch 00444: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2000 - acc: 0.9371 - val_loss: 0.1785 - val_acc: 0.9457\n",
            "Epoch 445/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.1859 - acc: 0.9370\n",
            "Epoch 00445: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1849 - acc: 0.9374 - val_loss: 0.2030 - val_acc: 0.9404\n",
            "Epoch 446/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.2032 - acc: 0.9329\n",
            "Epoch 00446: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.2016 - acc: 0.9334 - val_loss: 0.2143 - val_acc: 0.9325\n",
            "Epoch 447/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.1900 - acc: 0.9380\n",
            "Epoch 00447: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1898 - acc: 0.9381 - val_loss: 0.1700 - val_acc: 0.9457\n",
            "Epoch 448/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.1798 - acc: 0.9398\n",
            "Epoch 00448: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1797 - acc: 0.9399 - val_loss: 0.1893 - val_acc: 0.9431\n",
            "Epoch 449/1000\n",
            "187/189 [============================>.] - ETA: 0s - loss: 0.1768 - acc: 0.9452\n",
            "Epoch 00449: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1759 - acc: 0.9454 - val_loss: 0.2721 - val_acc: 0.9133\n",
            "Epoch 450/1000\n",
            "189/189 [==============================] - ETA: 0s - loss: 0.1953 - acc: 0.9343\n",
            "Epoch 00450: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1953 - acc: 0.9343 - val_loss: 0.1826 - val_acc: 0.9484\n",
            "Epoch 451/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.1727 - acc: 0.9434\n",
            "Epoch 00451: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1720 - acc: 0.9432 - val_loss: 0.1902 - val_acc: 0.9484\n",
            "Epoch 452/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2083 - acc: 0.9303\n",
            "Epoch 00452: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2068 - acc: 0.9306 - val_loss: 0.2216 - val_acc: 0.9378\n",
            "Epoch 453/1000\n",
            "184/189 [============================>.] - ETA: 0s - loss: 0.2036 - acc: 0.9343\n",
            "Epoch 00453: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2022 - acc: 0.9349 - val_loss: 0.2162 - val_acc: 0.9345\n",
            "Epoch 454/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.1940 - acc: 0.9363\n",
            "Epoch 00454: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1952 - acc: 0.9363 - val_loss: 0.1660 - val_acc: 0.9497\n",
            "Epoch 455/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.1739 - acc: 0.9421\n",
            "Epoch 00455: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1736 - acc: 0.9417 - val_loss: 0.1908 - val_acc: 0.9378\n",
            "Epoch 456/1000\n",
            "185/189 [============================>.] - ETA: 0s - loss: 0.1979 - acc: 0.9378\n",
            "Epoch 00456: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1964 - acc: 0.9387 - val_loss: 0.1803 - val_acc: 0.9437\n",
            "Epoch 457/1000\n",
            "183/189 [============================>.] - ETA: 0s - loss: 0.1972 - acc: 0.9358\n",
            "Epoch 00457: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1982 - acc: 0.9358 - val_loss: 0.2189 - val_acc: 0.9312\n",
            "Epoch 458/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.1956 - acc: 0.9348\n",
            "Epoch 00458: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.1945 - acc: 0.9351 - val_loss: 0.1716 - val_acc: 0.9510\n",
            "Epoch 459/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.1836 - acc: 0.9417\n",
            "Epoch 00459: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1848 - acc: 0.9417 - val_loss: 0.1998 - val_acc: 0.9391\n",
            "Epoch 460/1000\n",
            "188/189 [============================>.] - ETA: 0s - loss: 0.1987 - acc: 0.9343\n",
            "Epoch 00460: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 9ms/step - loss: 0.1993 - acc: 0.9343 - val_loss: 0.1699 - val_acc: 0.9530\n",
            "Epoch 461/1000\n",
            "186/189 [============================>.] - ETA: 0s - loss: 0.2048 - acc: 0.9320\n",
            "Epoch 00461: val_acc did not improve from 0.95831\n",
            "189/189 [==============================] - 2s 10ms/step - loss: 0.2057 - acc: 0.9316 - val_loss: 0.1868 - val_acc: 0.9451\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1e53c79588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2puCundnaBpz",
        "outputId": "ac46fd15-32b6-4eb2-8ae5-d38a35aa64e5"
      },
      "source": [
        "x_train.shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6040"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bElNWHUcYEhY"
      },
      "source": [
        "# Compile the network :\n",
        "model.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
        "model.fit(X_pre.reshape((X.shape[0], 1, X.shape[1])),Y,epochs=100,callbacks=[callbacks_list_2])\n",
        "#model.fit(X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1])),Y_train,validation_data=(X_cv.values.reshape((X_cv.shape[0], 1, X_cv.shape[1])),Y_cv),verbose=1,epochs=50,callbacks=callbacks_list_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvg-bCWg9KSX"
      },
      "source": [
        "## BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mytjn5mU9Ljy"
      },
      "source": [
        "## Loading the Pretrained Model from tensorflow HUB\n",
        "#all imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# maximum length of a seq in the data we have, for now i am making it as 55. You can change this\n",
        "max_seq_length = 650\n",
        "\n",
        "#BERT takes 3 inputs\n",
        "\n",
        "#this is input words. Sequence of words represented as integers\n",
        "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "\n",
        "#mask vector if you are padding anything\n",
        "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_mask\")\n",
        "\n",
        "#segment vectors. If you are giving only one sentence for the classification, total seg vector is 0. \n",
        "#If you are giving two sentenced with [sep] token separated, first seq segment vectors are zeros and \n",
        "#second seq segment vector are 1's\n",
        "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"segment_ids\")\n",
        "\n",
        "#bert layer \n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\", trainable=False)\n",
        "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "\n",
        "#Bert model\n",
        "#We are using only pooled output not sequence out. \n",
        "#If you want to know about those, please read https://www.kaggle.com/questions-and-answers/86510\n",
        "bert_model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=pooled_output)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqwOlRgY9Sfz",
        "outputId": "e215ba9c-6874-4401-c352-07a55c5f8a64"
      },
      "source": [
        "bert_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 650)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, 650)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 650)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 segment_ids[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 109,482,241\n",
            "Trainable params: 0\n",
            "Non-trainable params: 109,482,241\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5VSAXJR9V1h"
      },
      "source": [
        "#getting Vocab file\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duKgvFu89aJy",
        "outputId": "ca6254d7-0809-4835-da52-f72745165625"
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.94)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PyVR37_9dVt"
      },
      "source": [
        "# Create tokenizer \" Instantiate FullTokenizer\" \n",
        "# name must be \"tokenizer\"\n",
        "# the FullTokenizer takes two parameters 1. vocab_file and 2. do_lower_case \n",
        "# we have created these in the above cell ex: FullTokenizer(vocab_file, do_lower_case )\n",
        "# please check the \"tokenization.py\" file the complete implementation\n",
        "\n",
        "import tokenization\n",
        "tokenizer=tokenization.FullTokenizer(vocab_file,do_lower_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFfTRi849h-w",
        "outputId": "db0bfad5-9b7f-456f-d2c2-d9fcdce27052"
      },
      "source": [
        "#it has to give no error \n",
        "def grader_tokenize(tokenizer):\n",
        "    out = False\n",
        "    try:\n",
        "        out=('[CLS]' in tokenizer.vocab) and ('[SEP]' in tokenizer.vocab)\n",
        "    except:\n",
        "        out = False\n",
        "    assert(out==True)\n",
        "    return out\n",
        "grader_tokenize(tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrRwuYUo9kJV"
      },
      "source": [
        "# Create train and test tokens (X_train_tokens, X_test_tokens) from (X_train, X_test) using Tokenizer and \n",
        "\n",
        "# add '[CLS]' at start of the Tokens and '[SEP]' at the end of the tokens. \n",
        "\n",
        "# maximum number of tokens is act(We already given this to BERT layer above) so shape is (None, act)\n",
        "\n",
        "# if it is less than act, add '[PAD]' token else truncate the tokens length.(similar to padding)\n",
        "\n",
        "# Based on padding, create the mask for Train and Test ( 1 for real token, 0 for '[PAD]'), \n",
        "# it will also same shape as input tokens (None, act) save those in X_train_mask, X_test_mask\n",
        "\n",
        "# Create a segment input for train and test. We are using only one sentence so all zeros. This shape will also (None, act)\n",
        "\n",
        "# type of all the above arrays should be numpy arrays\n",
        "\n",
        "# after execution of this cell, you have to get \n",
        "# X_train_tokens, X_train_mask, X_train_segment\n",
        "# X_test_tokens, X_test_mask, X_test_segment\n",
        "\n",
        "act=max_seq_length\n",
        "act_sub=max_seq_length-2\n",
        "\n",
        "def train_test_tokens(X):\n",
        "  X_tokens=np.zeros((len(X),act))\n",
        "  X_mask=np.zeros((len(X),act))\n",
        "  X_segment=np.zeros((len(X),act))\n",
        "  for i,val1 in enumerate(X.values):\n",
        "    tokens=tokenizer.tokenize(val1)\n",
        "    if len(tokens)>max_seq_length-2:\n",
        "      tokens=tokens[0:(max_seq_length-2)]\n",
        "      actual_len=len(tokens)+2\n",
        "    elif len(tokens)<act_sub:\n",
        "      actual_len=len(tokens)+2\n",
        "      for j in range((max_seq_length-2)-len(tokens)):\n",
        "        tokens.append('[PAD]')\n",
        "    else:\n",
        "      actual_len=act\n",
        "    tokens=['[CLS]',*tokens,'[SEP]']\n",
        "    xt=np.array(tokenizer.convert_tokens_to_ids(tokens))\n",
        "    for t,val2 in enumerate(xt):\n",
        "      X_tokens[i][t]=val2\n",
        "    #Mask array\n",
        "    xm=np.array([1]*actual_len+[0]*(max_seq_length-actual_len))\n",
        "    for m,val3 in enumerate(xm):\n",
        "      X_mask[i][m]=val3\n",
        "    #Segment array\n",
        "    xs=np.array([0]*max_seq_length)\n",
        "    for s,val4 in enumerate(xs):\n",
        "      X_segment[i][s]=val4\n",
        "\n",
        "  return X_tokens,X_mask,X_segment\n",
        "\n",
        "\n",
        "X_train_tokens, X_train_mask, X_train_segment=train_test_tokens(x_train1['STORY'])\n",
        "X_test_tokens, X_test_mask, X_test_segment=train_test_tokens(x_val1['STORY'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XThMPSee-7ii",
        "outputId": "d6cefcb4-0b51-4d2e-d099-891278c56e42"
      },
      "source": [
        "def grader_alltokens_train():\n",
        "    out = False\n",
        "    \n",
        "    if type(X_train_tokens) == np.ndarray:\n",
        "        \n",
        "        temp_shapes = (X_train_tokens.shape[1]==max_seq_length) and (X_train_mask.shape[1]==max_seq_length) and \\\n",
        "        (X_train_segment.shape[1]==max_seq_length)\n",
        "        \n",
        "        segment_temp = not np.any(X_train_segment)\n",
        "        \n",
        "        mask_temp = np.sum(X_train_mask==0) == np.sum(X_train_tokens==0)\n",
        "        \n",
        "        no_cls = np.sum(X_train_tokens==tokenizer.vocab['[CLS]'])==X_train_tokens.shape[0]\n",
        "        \n",
        "        no_sep = np.sum(X_train_tokens==tokenizer.vocab['[SEP]'])==X_train_tokens.shape[0]\n",
        "        \n",
        "        out = temp_shapes and segment_temp and mask_temp and no_cls and no_sep\n",
        "      \n",
        "    else:\n",
        "        print('Type of all above token arrays should be list not numpy array')\n",
        "        out = False\n",
        "    assert(out==True)\n",
        "    return out\n",
        "\n",
        "grader_alltokens_train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G47s1ph5-xVd",
        "outputId": "2a674ec8-3964-41ef-9276-5687d83a546f"
      },
      "source": [
        "bert_model.input"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor 'input_word_ids:0' shape=(None, 650) dtype=int32>,\n",
              " <tf.Tensor 'input_mask:0' shape=(None, 650) dtype=int32>,\n",
              " <tf.Tensor 'segment_ids:0' shape=(None, 650) dtype=int32>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93cFnQgs_C46",
        "outputId": "3d174f5d-c1b4-4b8a-cbb5-561f494589e0"
      },
      "source": [
        "bert_model.output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'keras_layer/StatefulPartitionedCall:0' shape=(None, 768) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyvptCdf_F9P",
        "outputId": "ac1fb8a0-15ee-4608-8565-4145109f6eea"
      },
      "source": [
        "print(X_train_tokens.shape)\n",
        "print(X_train_mask.shape)\n",
        "print(X_train_segment.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6040, 650)\n",
            "(6040, 650)\n",
            "(6040, 650)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "w4p5B0XA_IpF",
        "outputId": "0a2859ba-a4f8-44f5-b735-e8a45a1dabf8"
      },
      "source": [
        "# get the train output, BERT model will give one output so save in\n",
        "from tqdm import tqdm\n",
        "for i in tqdm(range(0,61)):\n",
        "  if i==0:\n",
        "    X_train_pooled_output=bert_model([X_train_tokens[100*i:100*(i+1)], X_train_mask[100*i:100*(i+1)], X_train_segment[100*i:100*(i+1)]])\n",
        "  else:\n",
        "    res=bert_model([X_train_tokens[100*i:100*(i+1)], X_train_mask[100*i:100*(i+1)], X_train_segment[100*i:100*(i+1)]])\n",
        "    X_train_pooled_output=np.concatenate((X_train_pooled_output,res),axis=0)\n",
        "\n",
        "print(X_train_pooled_output.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/61 [00:01<?, ?it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-ee297b7f3b77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m61\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mX_train_pooled_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_segment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_segment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    384\u001b[0m     \"\"\"\n\u001b[1;32m    385\u001b[0m     return self._run_internal_graph(\n\u001b[0;32m--> 386\u001b[0;31m         inputs, training=training, mask=mask)\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    235\u001b[0m       result = smart_cond.smart_cond(training,\n\u001b[1;32m    236\u001b[0m                                      \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m                                      lambda: f(training=False))\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;31m# Unwrap dicts returned by signatures.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/smart_cond.py\u001b[0m in \u001b[0;36msmart_cond\u001b[0;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[1;32m     54\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtrue_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfalse_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     return control_flow_ops.cond(pred, true_fn=true_fn, false_fn=false_fn,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    235\u001b[0m       result = smart_cond.smart_cond(training,\n\u001b[1;32m    236\u001b[0m                                      \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m                                      lambda: f(training=False))\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;31m# Unwrap dicts returned by signatures.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m_call_attribute\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_call_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m:  Expected size[0] in [0, 512], but got 650\n\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/bert_model/StatefulPartitionedCall/embedding_postprocessor/Slice}}]] [Op:__inference_restored_function_body_132206]\n\nFunction call stack:\nrestored_function_body\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y015QzcdGkaP"
      },
      "source": [
        "from tqdm import tqdm\n",
        "for i in tqdm(range(0,16)):\n",
        "  if i==0:\n",
        "    X_test_pooled_output=bert_model([X_test_tokens[100*i:100*(i+1)], X_test_mask[100*i:100*(i+1)], X_test_segment[100*i:100*(i+1)]])\n",
        "  else:\n",
        "    res=bert_model([X_test_tokens[100*i:100*(i+1)], X_test_mask[100*i:100*(i+1)], X_test_segment[100*i:100*(i+1)]])\n",
        "    X_test_pooled_output=np.concatenate((X_test_pooled_output,res),axis=0)\n",
        "\n",
        "print(X_test_pooled_output.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nThzTMPA_fuj"
      },
      "source": [
        "print(X_train_pooled_output.shape)\n",
        "print(X_test_pooled_output.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muy6MLtN_jq3"
      },
      "source": [
        "# get the test output, BERT model will give one output so save in\n",
        "#from tqdm import tqdm\n",
        "#X_test_pooled_output=bert_model([X_test_tokens, X_test_mask, X_test_segment])\n",
        "#for i in tqdm(range(1,5)):\n",
        "  #res=bert_model([X_test_tokens[1584*i:1584*(i+1)], X_test_mask[1584*i:1584*(i+1)], X_test_segment[1584*i:1584*(i+1)]])\n",
        "  #X_test_pooled_output=np.concatenate((X_test_pooled_output,res),axis=0)\n",
        "\n",
        "#print(X_test_pooled_output.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DW0NRC8_qoa"
      },
      "source": [
        "#AUC Function\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "def AUC(y_true, y_pred):\n",
        "  try:\n",
        "    return tf.py_function(metrics.roc_auc_score, (y_true, y_pred), tf.double)\n",
        "  except ValueError:\n",
        "    pass\n",
        "\n",
        "def F1_Score(y_true, y_pred):\n",
        "  try:\n",
        "    return tf.py_function(metrics.f1_score, (y_true, y_pred), tf.double)\n",
        "  except ValueError:\n",
        "    pass\n",
        "\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import os\n",
        "if not os.path.exists('my_folder'):\n",
        "  os.makedirs('my_folder')\n",
        "filepath=\"best_model_1.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "################ Earlystopping callback ######################\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "earlystop = EarlyStopping(monitor='val_acc', patience=50,verbose=1,mode='max')\n",
        "\n",
        "\n",
        "#Tensorboard Callback\n",
        "import tensorflow as tf\n",
        "%load_ext tensorboard\n",
        "!rm -rf ./logs/\n",
        "import datetime\n",
        "log_dir=\"logs\\\\fit\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=0, write_graph=True,write_grads=True)\n",
        "\n",
        "#final_callbacks=[checkpoint,earlystop,tensorboard_callback]\n",
        "final_callbacks=[checkpoint,earlystop]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPym9aLo_wUB"
      },
      "source": [
        "##create an NN and \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout, BatchNormalization\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(2048, input_dim=768))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(1024))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(BatchNormalization())\n",
        "#model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(256))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(32))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(4))\n",
        "model.add(Activation('softmax'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1mHzPo9Aom5"
      },
      "source": [
        "#to convert class labels to one hot encoded vectors.\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ohe=OneHotEncoder()\n",
        "y_ohe_train=ohe.fit_transform(np.array(y_train).reshape(-1,1)).todense()\n",
        "y_ohe_test=ohe.transform(np.array(y_val).reshape(-1,1)).todense()\t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17zwnuDR__w0"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['acc'])\n",
        "model.fit(X_train_pooled_output,y_ohe_train,epochs=500,batch_size=32,\n",
        "          validation_data=(X_test_pooled_output,y_ohe_test),callbacks=[final_callbacks])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8emhQSNDV62"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "model = load_model('/content/best_model_1.h5')\n",
        "model.evaluate(X_train_pooled_output,y_ohe_train)\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "y_train_pred=model.predict_classes(X_train_pooled_output)\n",
        "y_test_pred=model.predict_classes(X_test_pooled_output)\n",
        "print(\"Train F1_score is :\",accuracy_score(y_train,y_train_pred))\n",
        "print(\"Test F1_score is :\",accuracy_score(y_val,y_test_pred))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}